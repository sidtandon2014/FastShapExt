{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "straight-behalf",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "disturbed-attitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidtandon/Sid/GitRepo/FastShapExt/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "immune-sigma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load train set\n",
    "train_set = dsets.CIFAR10('../', train=True, download=True, transform=transform_train)\n",
    "\n",
    "# Load test set (using as validation)\n",
    "val_set = dsets.CIFAR10('../', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561a6efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-george",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subjective-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os.path\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "from resnet import ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compliant-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24e87f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "clear-homework",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Loading saved model')\n",
    "model = ResNet18(num_classes=10)\n",
    "model.load_state_dict(torch.load('cifar resnet.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-education",
   "metadata": {},
   "source": [
    "# Train surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d985b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/sidtandon/Sid/GitRepo/fastshap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alternate-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastshap import ImageSurrogate\n",
    "from fastshap.utils import MaskLayer2d, MaskLayer2dSCL, KLDivLoss, DatasetInputOnly\n",
    "from scl.networks.resnet_big import SupConResNet\n",
    "from scl.losses import SupConLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e88ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoCropTransform:\n",
    "    \"\"\"Create two crops of the same image\"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.transform(x), self.transform(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a1dd03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "def parse_option():\n",
    "    parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "    parser.add_argument('--print_freq', type=int, default=10,\n",
    "                        help='print frequency')\n",
    "    parser.add_argument('--save_freq', type=int, default=50,\n",
    "                        help='save frequency')\n",
    "    parser.add_argument('--batch_size', type=int, default=256,\n",
    "                        help='batch_size')\n",
    "    parser.add_argument('--num_workers', type=int, default=16,\n",
    "                        help='num of workers to use')\n",
    "    parser.add_argument('--epochs', type=int, default=1000,\n",
    "                        help='number of training epochs')\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.05,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--lr_decay_epochs', type=str, default='700,800,900',\n",
    "                        help='where to decay lr, can be a list')\n",
    "    parser.add_argument('--lr_decay_rate', type=float, default=0.1,\n",
    "                        help='decay rate for learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-4,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                        help='momentum')\n",
    "\n",
    "    # model dataset\n",
    "    parser.add_argument('--model', type=str, default='resnet18')\n",
    "    parser.add_argument('--dataset', type=str, default='cifar10',\n",
    "                        choices=['cifar10', 'cifar100', 'path'], help='dataset')\n",
    "    parser.add_argument('--mean', type=str, help='mean of dataset in path in form of str tuple')\n",
    "    parser.add_argument('--std', type=str, help='std of dataset in path in form of str tuple')\n",
    "    parser.add_argument('--data_folder', type=str, default=None, help='path to custom dataset')\n",
    "    parser.add_argument('--size', type=int, default=32, help='parameter for RandomResizedCrop')\n",
    "\n",
    "    # method\n",
    "    parser.add_argument('--method', type=str, default='SupCon',\n",
    "                        choices=['SupCon', 'SimCLR'], help='choose method')\n",
    "\n",
    "    # temperature\n",
    "    parser.add_argument('--temp', type=float, default=0.07,\n",
    "                        help='temperature for loss function')\n",
    "\n",
    "    # other setting\n",
    "    parser.add_argument('--cosine', action='store_true',\n",
    "                        help='using cosine annealing')\n",
    "    parser.add_argument('--syncBN', action='store_true',\n",
    "                        help='using synchronized batch normalization')\n",
    "    parser.add_argument('--warm', action='store_true',\n",
    "                        help='warm-up for large batch training')\n",
    "    parser.add_argument('--trial', type=str, default='0',\n",
    "                        help='id for recording multiple runs')\n",
    "\n",
    "    opt = parser.parse_args(\"\")\n",
    "\n",
    "    # check if dataset is path that passed required arguments\n",
    "    if opt.dataset == 'path':\n",
    "        assert opt.data_folder is not None \\\n",
    "            and opt.mean is not None \\\n",
    "            and opt.std is not None\n",
    "\n",
    "    # set the path according to the environment\n",
    "    if opt.data_folder is None:\n",
    "        opt.data_folder = './datasets/'\n",
    "    opt.model_path = './scl_models/SupCon/{}_models'.format(opt.dataset)\n",
    "    opt.tb_path = './scl_models/SupCon/{}_tensorboard'.format(opt.dataset)\n",
    "\n",
    "    iterations = opt.lr_decay_epochs.split(',')\n",
    "    opt.lr_decay_epochs = list([])\n",
    "    for it in iterations:\n",
    "        opt.lr_decay_epochs.append(int(it))\n",
    "\n",
    "    opt.model_name = '{}_{}_{}_lr_{}_decay_{}_bsz_{}_temp_{}_trial_{}'.\\\n",
    "        format(opt.method, opt.dataset, opt.model, opt.learning_rate,\n",
    "               opt.weight_decay, opt.batch_size, opt.temp, opt.trial)\n",
    "\n",
    "    if opt.cosine:\n",
    "        opt.model_name = '{}_cosine'.format(opt.model_name)\n",
    "\n",
    "    # warm-up for large-batch training,\n",
    "    if opt.batch_size > 256:\n",
    "        opt.warm = True\n",
    "    if opt.warm:\n",
    "        opt.model_name = '{}_warm'.format(opt.model_name)\n",
    "        opt.warmup_from = 0.01\n",
    "        opt.warm_epochs = 10\n",
    "        if opt.cosine:\n",
    "            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
    "                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
    "        else:\n",
    "            opt.warmup_to = opt.learning_rate\n",
    "\n",
    "    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n",
    "    if not os.path.isdir(opt.tb_folder):\n",
    "        os.makedirs(opt.tb_folder)\n",
    "\n",
    "    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n",
    "    if not os.path.isdir(opt.save_folder):\n",
    "        os.makedirs(opt.save_folder)\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48df7c83",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_original_model_scl() got an unexpected keyword argument 'opt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m original_model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(model, nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m surrogate\u001b[38;5;241m.\u001b[39mtrain_original_model_scl(\n\u001b[1;32m     58\u001b[0m     train_surr,\n\u001b[1;32m     59\u001b[0m     val_surr,\n\u001b[1;32m     60\u001b[0m     original_model,\n\u001b[1;32m     61\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     62\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mepochs,\n\u001b[1;32m     63\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[1;32m     64\u001b[0m     opt\u001b[38;5;241m=\u001b[39mopt,\n\u001b[1;32m     65\u001b[0m     lookback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     66\u001b[0m     bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     67\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Save surrogate\u001b[39;00m\n\u001b[1;32m     70\u001b[0m surr\u001b[38;5;241m.\u001b[39mcpu()\n",
      "\u001b[0;31mTypeError\u001b[0m: train_original_model_scl() got an unexpected keyword argument 'opt'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Check for model\n",
    "if os.path.isfile('cifar surrogate.pt'):\n",
    "    print('Loading saved surrogate model')\n",
    "    surr = torch.load('cifar surrogate.pt').to(device)\n",
    "    surrogate = ImageSurrogate(surr, width=32, height=32, superpixel_size=2)\n",
    "\n",
    "else:\n",
    "    # Create model\n",
    "    opt = parse_option()\n",
    "    surr = nn.Sequential(\n",
    "        MaskLayer2dSCL(value=0, append=True),\n",
    "        SupConResNet(name=opt.model).to(device)\n",
    "    )\n",
    "\n",
    "    criterion = SupConLoss(temperature=opt.temp)\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2023, 0.1994, 0.2010)\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=(32,32), scale=(0.2, 1.)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([\n",
    "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "        ], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    '''\n",
    "    # Copy resent weights from original model\n",
    "    for name, param in model.state_dict().items():\n",
    "        _name = \"1.\" + name \n",
    "        #print(name,param.shape)\n",
    "        surr_state_dict = surr.state_dict()\n",
    "        if _name != \"1.conv1.weight\":\n",
    "            surr_state_dict[_name].copy_(param.data)\n",
    "\n",
    "    for name, param in surr.named_parameters():\n",
    "        if param.requires_grad and '1.conv1.weight' != name:\n",
    "            param.requires_grad = False\n",
    "    '''\n",
    "    # Set up surrogate object\n",
    "    surrogate = ImageSurrogate(surr, width=32, height=32, superpixel_size=2)\n",
    "    \n",
    "    # Set up datasets\n",
    "    train_surr = DatasetInputOnly(train_set)\n",
    "    val_surr = DatasetInputOnly(val_set)\n",
    "    original_model = nn.Sequential(model, nn.Softmax(dim=1))\n",
    "\n",
    "    # Train\n",
    "    surrogate.train_original_model_scl(\n",
    "        train_surr,\n",
    "        val_surr,\n",
    "        original_model,\n",
    "        batch_size=256,\n",
    "        max_epochs=opt.epochs,\n",
    "        loss_fn=criterion,\n",
    "        opt=opt,\n",
    "        lookback=10,\n",
    "        bar=True,\n",
    "        verbose=True)\n",
    "    \n",
    "    # Save surrogate\n",
    "    surr.cpu()\n",
    "    torch.save(surr.state_dict(), 'cifar surrogate scl.pt')\n",
    "    surr.to(device)\n",
    "\n",
    "end  = time.time() - start\n",
    "print(f\"Time Taken: {end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "uniform-property",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_original_model_scl() got an unexpected keyword argument 'opt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m original_model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(model, nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m surrogate\u001b[38;5;241m.\u001b[39mtrain_original_model_scl(\n\u001b[1;32m     58\u001b[0m     train_surr,\n\u001b[1;32m     59\u001b[0m     val_surr,\n\u001b[1;32m     60\u001b[0m     original_model,\n\u001b[1;32m     61\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     62\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mepochs,\n\u001b[1;32m     63\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[1;32m     64\u001b[0m     lookback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     65\u001b[0m     opt\u001b[38;5;241m=\u001b[39mopt,\n\u001b[1;32m     66\u001b[0m     bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     67\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Save surrogate\u001b[39;00m\n\u001b[1;32m     70\u001b[0m surr\u001b[38;5;241m.\u001b[39mcpu()\n",
      "\u001b[0;31mTypeError\u001b[0m: train_original_model_scl() got an unexpected keyword argument 'opt'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db524b4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cifar surrogate.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m surr \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      2\u001b[0m         MaskLayer2d(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      3\u001b[0m         ResNet18(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m surr\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcifar surrogate.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m surr\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Set up surrogate object\u001b[39;00m\n",
      "File \u001b[0;32m~/Sid/GitRepo/FastShapExt/.venv/lib/python3.9/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Sid/GitRepo/FastShapExt/.venv/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Sid/GitRepo/FastShapExt/.venv/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cifar surrogate.pt'"
     ]
    }
   ],
   "source": [
    "surr = nn.Sequential(\n",
    "        MaskLayer2d(value=0, append=True),\n",
    "        ResNet18(in_channels=4, num_classes=10)).to(device)\n",
    "\n",
    "surr.load_state_dict(torch.load('cifar surrogate.pt'))\n",
    "surr.to(device)\n",
    "\n",
    "# Set up surrogate object\n",
    "surrogate = ImageSurrogate(surr, width=32, height=32, superpixel_size=2)\n",
    "\n",
    "# Set up datasets\n",
    "train_surr = DatasetInputOnly(train_set)\n",
    "val_surr = DatasetInputOnly(val_set)\n",
    "original_model = nn.Sequential(model, nn.Softmax(dim=1))\n",
    "\n",
    "# Train\n",
    "surrogate.train_original_model(\n",
    "    train_surr,\n",
    "    val_surr,\n",
    "    original_model,\n",
    "    batch_size=256,\n",
    "    max_epochs=100,\n",
    "    loss_fn=KLDivLoss(),\n",
    "    lookback=10,\n",
    "    bar=True,\n",
    "    verbose=True)\n",
    "\n",
    "# Save surrogate\n",
    "surr.cpu()\n",
    "torch.save(surr.state_dict(), 'cifar surrogate.pt')\n",
    "surr.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-partition",
   "metadata": {},
   "source": [
    "# Train FastSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import UNet\n",
    "from fastshap import FastSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for model\n",
    "start = time.time()\n",
    "\n",
    "if os.path.isfile('cifar explainer.pt'):\n",
    "    print('Loading saved explainer model')\n",
    "    explainer = torch.load('cifar explainer.pt').to(device)\n",
    "    fastshap = FastSHAP(explainer, surrogate, link=nn.LogSoftmax(dim=1))\n",
    "\n",
    "else:\n",
    "    # Set up explainer model\n",
    "    explainer = UNet(n_classes=10, num_down=2, num_up=1, num_convs=3).to(device)\n",
    "\n",
    "    # Set up FastSHAP object\n",
    "    fastshap = FastSHAP(explainer, surrogate, link=nn.LogSoftmax(dim=1))\n",
    "\n",
    "    # Set up datasets\n",
    "    fastshap_train = DatasetInputOnly(train_set)\n",
    "    fastshap_val = DatasetInputOnly(val_set)\n",
    "\n",
    "    # Train\n",
    "    fastshap.train(\n",
    "        fastshap_train,\n",
    "        fastshap_val,\n",
    "        batch_size=128,\n",
    "        num_samples=2,\n",
    "        max_epochs=200,\n",
    "        eff_lambda=1e-2,\n",
    "        validation_samples=1,\n",
    "        lookback=10,\n",
    "        bar=True,\n",
    "        verbose=True)\n",
    "    \n",
    "    # Save explainer\n",
    "    explainer.cpu()\n",
    "    torch.save(explainer, 'cifar explainer.pt')\n",
    "    explainer.to(device)\n",
    "\n",
    "end = time.time() - start\n",
    "print(f\"Time taken: {end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-thirty",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one image from each class\n",
    "dset = val_set\n",
    "targets = np.array(dset.targets)\n",
    "num_classes = targets.max() + 1\n",
    "inds_lists = [np.where(targets == cat)[0] for cat in range(num_classes)]\n",
    "inds = [np.random.choice(cat_inds) for cat_inds in inds_lists]\n",
    "x, y = zip(*[dset[ind] for ind in inds])\n",
    "x = torch.stack(x)\n",
    "\n",
    "# Get explanations\n",
    "values = fastshap.shap_values(x.to(device))\n",
    "\n",
    "# Get predictions\n",
    "pred = surrogate(\n",
    "    x.to(device),\n",
    "    torch.ones(num_classes, surrogate.num_players, device=device)\n",
    ").softmax(dim=1).cpu().data.numpy()\n",
    "\n",
    "fig, axarr = plt.subplots(num_classes, num_classes + 1, figsize=(22, 20))\n",
    "\n",
    "for row in range(num_classes):\n",
    "    # Image\n",
    "    classes = ['Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])[:, np.newaxis, np.newaxis]\n",
    "    std = np.array([0.2023, 0.1994, 0.2010])[:, np.newaxis, np.newaxis]\n",
    "    im = x[row].numpy() * std + mean\n",
    "    im = im.transpose(1, 2, 0).astype(float)\n",
    "    im = np.clip(im, a_min=0, a_max=1)\n",
    "    axarr[row, 0].imshow(im, vmin=0, vmax=1)\n",
    "    axarr[row, 0].set_xticks([])\n",
    "    axarr[row, 0].set_yticks([])\n",
    "    axarr[row, 0].set_ylabel('{}'.format(classes[y[row]]), fontsize=14)\n",
    "    \n",
    "    # Explanations\n",
    "    m = np.abs(values[row]).max()\n",
    "    for col in range(num_classes):\n",
    "        axarr[row, col + 1].imshow(values[row, col], cmap='seismic', vmin=-m, vmax=m)\n",
    "        axarr[row, col + 1].set_xticks([])\n",
    "        axarr[row, col + 1].set_yticks([])\n",
    "        if col == y[row]:\n",
    "            axarr[row, col + 1].set_xlabel('{:.2f}'.format(pred[row, col]), fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            axarr[row, col + 1].set_xlabel('{:.2f}'.format(pred[row, col]), fontsize=12)\n",
    "        \n",
    "        # Class labels\n",
    "        if row == 0:\n",
    "            axarr[row, col + 1].set_title('{}'.format(classes[y[col]]), fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-attendance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1fc9b8638362cbdc7141d1e74cba8a8c09a8baa0cc5883fc38fa28281f96844d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
