{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "straight-behalf",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c2e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/sidtandon/Sid/GitRepo/FastShapExt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "disturbed-attitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidtandon/Sid/GitRepo/FastShapExt/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os.path\n",
    "from copy import deepcopy\n",
    "from resnet import ResNet18\n",
    "\n",
    "from fastshap import ImageSurrogate\n",
    "from fastshap.image_surrogate  import generate_labels\n",
    "from fastshap.image_imputers import ImageImputer\n",
    "from fastshap.utils import MaskLayer2d, MaskLayer2dSCL, KLDivLoss, DatasetInputOnly\n",
    "from scl.networks.resnet_big import SupConResNet, LinearClassifier\n",
    "from scl.util import AverageMeter\n",
    "import torch.backends.cudnn as cudnn\n",
    "import time\n",
    "from torch.utils.data import RandomSampler, BatchSampler, DataLoader, TensorDataset\n",
    "from fastshap.utils import UniformSampler, DatasetRepeat\n",
    "import sys\n",
    "from scl.util import set_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "compliant-craps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select device\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1dd03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "def parse_option():\n",
    "    parser = argparse.ArgumentParser('argument for training')\n",
    "\n",
    "    parser.add_argument('--print_freq', type=int, default=10,\n",
    "                        help='print frequency')\n",
    "    parser.add_argument('--save_freq', type=int, default=50,\n",
    "                        help='save frequency')\n",
    "    parser.add_argument('--batch_size', type=int, default=512,\n",
    "                        help='batch_size')\n",
    "    parser.add_argument('--num_workers', type=int, default=0,\n",
    "                        help='num of workers to use')\n",
    "    parser.add_argument('--epochs', type=int, default=100,\n",
    "                        help='number of training epochs')\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',\n",
    "                        help='where to decay lr, can be a list')\n",
    "    parser.add_argument('--lr_decay_rate', type=float, default=0.2,\n",
    "                        help='decay rate for learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                        help='momentum')\n",
    "\n",
    "    # model dataset\n",
    "    parser.add_argument('--model', type=str, default='resnet18')\n",
    "    parser.add_argument('--dataset', type=str, default='cifar10',\n",
    "                        choices=['cifar10', 'cifar100'], help='dataset')\n",
    "\n",
    "    # other setting\n",
    "    parser.add_argument('--cosine', action='store_true',\n",
    "                        help='using cosine annealing')\n",
    "    parser.add_argument('--warm', action='store_true',\n",
    "                        help='warm-up for large batch training')\n",
    "\n",
    "    parser.add_argument('--ckpt', type=str, default='./scl_models/SupCon/cifar10_models/SupCon_cifar10_resnet18_lr_0.05_decay_0.0001_bsz_256_temp_0.07_trial_0_cosine/ckpt_epoch_500.pth',\n",
    "                        help='path to pre-trained model')\n",
    "\n",
    "    opt = parser.parse_args(\"\")\n",
    "\n",
    "    # set the path according to the environment\n",
    "    opt.data_folder = './datasets/'\n",
    "\n",
    "    iterations = opt.lr_decay_epochs.split(',')\n",
    "    opt.lr_decay_epochs = list([])\n",
    "    for it in iterations:\n",
    "        opt.lr_decay_epochs.append(int(it))\n",
    "\n",
    "    opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\\\n",
    "        format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,\n",
    "               opt.batch_size)\n",
    "\n",
    "    if opt.cosine:\n",
    "        opt.model_name = '{}_cosine'.format(opt.model_name)\n",
    "\n",
    "    # warm-up for large-batch training,\n",
    "    if opt.warm:\n",
    "        opt.model_name = '{}_warm'.format(opt.model_name)\n",
    "        opt.warmup_from = 0.01\n",
    "        opt.warm_epochs = 10\n",
    "        if opt.cosine:\n",
    "            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n",
    "            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n",
    "                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n",
    "        else:\n",
    "            opt.warmup_to = opt.learning_rate\n",
    "\n",
    "    if opt.dataset == 'cifar10':\n",
    "        opt.n_cls = 10\n",
    "    elif opt.dataset == 'cifar100':\n",
    "        opt.n_cls = 100\n",
    "    else:\n",
    "        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c48e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_learning_rate(args, epoch, batch_id, total_batches, optimizer):\n",
    "    if args.warm and epoch <= args.warm_epochs:\n",
    "        p = (batch_id + (epoch - 1) * total_batches) / \\\n",
    "            (args.warm_epochs * total_batches)\n",
    "        lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "def adjust_learning_rate(args, optimizer, epoch):\n",
    "    lr = args.learning_rate\n",
    "    if args.cosine:\n",
    "        eta_min = lr * (args.lr_decay_rate ** 3)\n",
    "        lr = eta_min + (lr - eta_min) * (\n",
    "                1 + math.cos(math.pi * epoch / args.epochs)) / 2\n",
    "    else:\n",
    "        steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n",
    "        if steps > 0:\n",
    "            lr = lr * (args.lr_decay_rate ** steps)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f2e19e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_loader(opt, original_model, num_players):\n",
    "    # construct data loader\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    # Load train set\n",
    "    train_set = dsets.CIFAR10('../', train=True, download=True, transform=transform_train)\n",
    "\n",
    "    # Load test set (using as validation)\n",
    "    val_set = dsets.CIFAR10('../', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    train_set = DatasetInputOnly(train_set)\n",
    "    val_set = DatasetInputOnly(val_set)\n",
    "    \n",
    "    random_sampler = RandomSampler(\n",
    "        train_set, replacement=True,\n",
    "        num_samples=int(np.ceil(len(train_set) / opt.batch_size))*opt.batch_size)\n",
    "    batch_sampler = BatchSampler(\n",
    "        random_sampler, batch_size=opt.batch_size, drop_last=True)\n",
    "    train_loader = DataLoader(train_set, batch_sampler=batch_sampler,\n",
    "                                pin_memory=True, num_workers=opt.num_workers)\n",
    "\n",
    "\n",
    "    sampler = UniformSampler(num_players)\n",
    "    torch.manual_seed(1)\n",
    "    S_val = sampler.sample(len(val_set))\n",
    "    validation_batch_size = opt.batch_size\n",
    "\n",
    "\n",
    "        # Generate validation labels.\n",
    "    y_val = generate_labels(val_set, original_model,\n",
    "                            validation_batch_size, opt.num_workers)\n",
    "    \n",
    "    # Create dataset.\n",
    "    val_set = DatasetRepeat(\n",
    "        [val_set, TensorDataset(y_val, S_val)])\n",
    "   \n",
    "    val_loader = DataLoader(val_set, batch_size=validation_batch_size,\n",
    "                            pin_memory=True, num_workers=opt.num_workers)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4148600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(opt):\n",
    "\n",
    "    mask_layer_model = MaskLayer2dSCL(value=0, append=True, include_second_coalition=False)\n",
    "    supcon_resnet_model = SupConResNet(name=opt.model)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)\n",
    "\n",
    "    ckpt = torch.load(opt.ckpt, map_location='cpu')\n",
    "    state_dict = ckpt['model']\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            supcon_resnet_model.encoder = torch.nn.DataParallel(supcon_resnet_model.encoder)\n",
    "        else:\n",
    "            new_state_dict = {}\n",
    "            for k, v in state_dict.items():\n",
    "                k = k.replace(\"1.encoder\", \"encoder\")\n",
    "                k = k.replace(\"1.head\", \"head\")\n",
    "                new_state_dict[k] = v\n",
    "            state_dict = new_state_dict\n",
    "        supcon_resnet_model = supcon_resnet_model.cuda()\n",
    "        classifier = classifier.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        mask_layer_model = mask_layer_model.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "        supcon_resnet_model.load_state_dict(state_dict)\n",
    "\n",
    "    return mask_layer_model, supcon_resnet_model, classifier, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f642017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "class KLDivLoss(nn.Module):\n",
    "    '''\n",
    "    KL divergence loss that applies log softmax operation to predictions.\n",
    "\n",
    "    Args:\n",
    "      reduction: how to reduce loss value (e.g., 'batchmean').\n",
    "      log_target: whether the target is expected as a log probabilities (or as\n",
    "        probabilities).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, reduction='batchmean', log_target=False):\n",
    "        super().__init__()\n",
    "        self.kld = nn.KLDivLoss(reduction=reduction, log_target=log_target)\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        '''\n",
    "        Evaluate loss.\n",
    "\n",
    "        Args:\n",
    "          pred:\n",
    "          target:\n",
    "        '''\n",
    "        return self.kld(pred.log_softmax(dim=1), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ae81ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, mask_layer_model, supcon_resnet_model, original_model, classifier, criterion, optimizer, epoch, opt\n",
    "            , image_imputer: ImageImputer):\n",
    "    \"\"\"one epoch training\"\"\"\n",
    "    mask_layer_model.eval()\n",
    "    supcon_resnet_model.eval()\n",
    "    classifier.train()\n",
    "\n",
    "    kldiv_criterion = KLDivLoss()\n",
    "    sampler = UniformSampler(image_imputer.num_players)\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    kl_top1= AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for idx, (images,) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        bsz = images.shape[0]\n",
    "\n",
    "        # warm-up learning rate\n",
    "        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n",
    "\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            S = sampler.sample(opt.batch_size).to(device=device)\n",
    "            S = image_imputer.resize(S)\n",
    "            mod_images =  mask_layer_model((images, S))\n",
    "            features = supcon_resnet_model.encoder(mod_images)\n",
    "            y_for_klloss = original_model(images)\n",
    "            labels = torch.argmax(original_model(images), dim = 1)\n",
    "            \n",
    "        output = classifier(features.detach())\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            klloss = kldiv_criterion(output, y_for_klloss)\n",
    "            \n",
    "        # update metric\n",
    "        losses.update(loss.item(), bsz)\n",
    "        acc1 = accuracy(output, labels, topk=[1]) # Mod by Sid: topk=(1,5). Similarly in validate function\n",
    "        top1.update(acc1[0], bsz)\n",
    "        kl_top1.update(klloss, bsz)\n",
    "\n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # print info\n",
    "        if (idx + 1) % opt.print_freq == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n",
    "                  'KL loss {klloss.val:.3f} ({klloss.avg:.3f})\\t'\n",
    "                  'Acc@1 {top1.val[0]:.3f} ({top1.avg[0]:.3f})'.format(\n",
    "                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, klloss = kl_top1,top1=top1))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return losses.avg, top1.avg[0]\n",
    "\n",
    "\n",
    "def validate(val_loader, mask_layer_model, supcon_resnet_model, classifier, criterion, opt, image_imputer: ImageImputer):\n",
    "    \"\"\"validation\"\"\"\n",
    "    supcon_resnet_model.eval()\n",
    "    mask_layer_model.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    kldiv_criterion = KLDivLoss()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    kl_top1= AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (images, labels, S) in enumerate(val_loader):\n",
    "            images = images.float().cuda()\n",
    "            labels = labels.cuda()\n",
    "            S = S.cuda()\n",
    "            bsz = labels.shape[0]\n",
    "\n",
    "            # forward\n",
    "            S = image_imputer.resize(S)\n",
    "            mod_images =  mask_layer_model((images, S))\n",
    "            output = classifier(supcon_resnet_model.encoder(mod_images))\n",
    "            labels_index = torch.argmax(labels, dim = 1)\n",
    "            loss = criterion(output, labels_index)\n",
    "            klloss = kldiv_criterion(output, labels)\n",
    "\n",
    "            # update metric\n",
    "            losses.update(loss.item(), bsz)\n",
    "            acc1 = accuracy(output, labels_index, topk=[1])\n",
    "            top1.update(acc1[0], bsz)\n",
    "            kl_top1.update(klloss, bsz)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if idx % opt.print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'KL loss {klloss.val:.3f} ({klloss.avg:.3f})\\t'\n",
    "                      'Acc@1 {top1.val[0]:.3f} ({top1.avg[0]:.3f})'.format(\n",
    "                       idx, len(val_loader), batch_time=batch_time,\n",
    "                       loss=losses, klloss = kl_top1,top1=top1))\n",
    "\n",
    "    print(' * Acc@1 {top1.avg[0]:.3f}'.format(top1=top1))\n",
    "    return losses.avg, top1.avg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62e77924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train: [1][10/98]\tBT 0.668 (0.666)\tDT 0.278 (0.276)\tloss 1.642 (3.535)\tKL loss 1.682 (3.563)\tAcc@1 90.820 (73.867)\n",
      "Train: [1][20/98]\tBT 0.668 (0.668)\tDT 0.275 (0.277)\tloss 1.279 (2.521)\tKL loss 1.252 (2.580)\tAcc@1 94.336 (83.418)\n",
      "Train: [1][30/98]\tBT 0.662 (0.668)\tDT 0.269 (0.276)\tloss 1.488 (2.200)\tKL loss 1.541 (2.266)\tAcc@1 93.750 (86.934)\n",
      "Train: [1][40/98]\tBT 0.667 (0.670)\tDT 0.272 (0.278)\tloss 1.196 (2.026)\tKL loss 1.317 (2.117)\tAcc@1 92.773 (88.638)\n",
      "Train: [1][50/98]\tBT 0.672 (0.670)\tDT 0.278 (0.278)\tloss 1.745 (1.878)\tKL loss 1.869 (1.981)\tAcc@1 93.945 (89.816)\n",
      "Train: [1][60/98]\tBT 0.666 (0.670)\tDT 0.271 (0.277)\tloss 1.327 (1.753)\tKL loss 1.560 (1.875)\tAcc@1 94.727 (90.602)\n",
      "Train: [1][70/98]\tBT 0.672 (0.675)\tDT 0.277 (0.282)\tloss 1.270 (1.696)\tKL loss 1.394 (1.814)\tAcc@1 94.922 (91.152)\n",
      "Train: [1][80/98]\tBT 0.669 (0.676)\tDT 0.273 (0.282)\tloss 1.149 (1.619)\tKL loss 1.250 (1.735)\tAcc@1 94.141 (91.626)\n",
      "Train: [1][90/98]\tBT 0.700 (0.677)\tDT 0.300 (0.282)\tloss 1.156 (1.548)\tKL loss 1.199 (1.665)\tAcc@1 93.945 (91.999)\n",
      "Train epoch 1, total time 66.31, accuracy:92.23\n",
      "Test: [0/20]\tTime 0.297 (0.297)\tLoss 9.0990 (9.0990)\tKL loss 9.199 (9.199)\tAcc@1 81.055 (81.055)\n",
      "Test: [10/20]\tTime 0.282 (0.291)\tLoss 9.7604 (9.9819)\tKL loss 10.242 (10.316)\tAcc@1 79.883 (80.859)\n",
      " * Acc@1 81.020\n",
      "Train: [2][10/98]\tBT 0.667 (0.674)\tDT 0.266 (0.275)\tloss 0.927 (0.954)\tKL loss 1.051 (1.041)\tAcc@1 94.141 (94.492)\n",
      "Train: [2][20/98]\tBT 0.687 (0.675)\tDT 0.287 (0.276)\tloss 0.632 (0.979)\tKL loss 0.768 (1.072)\tAcc@1 96.680 (94.541)\n",
      "Train: [2][30/98]\tBT 0.688 (0.677)\tDT 0.285 (0.278)\tloss 0.636 (0.944)\tKL loss 0.713 (1.051)\tAcc@1 94.531 (94.577)\n",
      "Train: [2][40/98]\tBT 0.677 (0.677)\tDT 0.275 (0.277)\tloss 0.625 (0.904)\tKL loss 0.690 (1.017)\tAcc@1 93.750 (94.609)\n",
      "Train: [2][50/98]\tBT 0.664 (0.683)\tDT 0.263 (0.283)\tloss 0.452 (0.883)\tKL loss 0.522 (0.993)\tAcc@1 95.508 (94.594)\n",
      "Train: [2][60/98]\tBT 0.698 (0.683)\tDT 0.295 (0.282)\tloss 0.942 (0.866)\tKL loss 1.032 (0.976)\tAcc@1 92.773 (94.564)\n",
      "Train: [2][70/98]\tBT 0.683 (0.682)\tDT 0.278 (0.281)\tloss 0.859 (0.866)\tKL loss 1.171 (0.977)\tAcc@1 92.969 (94.523)\n",
      "Train: [2][80/98]\tBT 0.685 (0.683)\tDT 0.281 (0.281)\tloss 0.779 (0.843)\tKL loss 0.799 (0.951)\tAcc@1 93.945 (94.558)\n",
      "Train: [2][90/98]\tBT 0.677 (0.683)\tDT 0.273 (0.281)\tloss 1.079 (0.842)\tKL loss 1.103 (0.944)\tAcc@1 93.750 (94.533)\n",
      "Train epoch 2, total time 66.90, accuracy:94.56\n",
      "Test: [0/20]\tTime 0.285 (0.285)\tLoss 7.0388 (7.0388)\tKL loss 7.145 (7.145)\tAcc@1 80.859 (80.859)\n",
      "Test: [10/20]\tTime 0.297 (0.317)\tLoss 7.2671 (7.6159)\tKL loss 7.640 (7.897)\tAcc@1 81.250 (81.090)\n",
      " * Acc@1 81.210\n",
      "Train: [3][10/98]\tBT 0.684 (0.691)\tDT 0.277 (0.285)\tloss 0.660 (0.659)\tKL loss 0.836 (0.753)\tAcc@1 95.117 (95.137)\n",
      "Train: [3][20/98]\tBT 0.661 (0.683)\tDT 0.259 (0.278)\tloss 0.566 (0.680)\tKL loss 0.850 (0.787)\tAcc@1 94.336 (94.756)\n",
      "Train: [3][30/98]\tBT 0.676 (0.690)\tDT 0.269 (0.285)\tloss 0.647 (0.626)\tKL loss 0.642 (0.729)\tAcc@1 94.922 (94.831)\n",
      "Train: [3][40/98]\tBT 0.681 (0.691)\tDT 0.275 (0.285)\tloss 0.734 (0.617)\tKL loss 0.810 (0.712)\tAcc@1 94.727 (94.810)\n",
      "Train: [3][50/98]\tBT 0.672 (0.689)\tDT 0.264 (0.282)\tloss 0.691 (0.625)\tKL loss 0.731 (0.717)\tAcc@1 94.141 (94.695)\n",
      "Train: [3][60/98]\tBT 0.689 (0.688)\tDT 0.282 (0.281)\tloss 0.224 (0.625)\tKL loss 0.282 (0.716)\tAcc@1 95.508 (94.596)\n",
      "Train: [3][70/98]\tBT 0.687 (0.687)\tDT 0.281 (0.280)\tloss 0.552 (0.625)\tKL loss 0.618 (0.717)\tAcc@1 93.164 (94.590)\n",
      "Train: [3][80/98]\tBT 0.683 (0.687)\tDT 0.275 (0.279)\tloss 0.660 (0.627)\tKL loss 0.638 (0.715)\tAcc@1 95.703 (94.563)\n",
      "Train: [3][90/98]\tBT 0.677 (0.686)\tDT 0.269 (0.278)\tloss 0.491 (0.616)\tKL loss 0.572 (0.705)\tAcc@1 94.141 (94.501)\n",
      "Train epoch 3, total time 67.19, accuracy:94.48\n",
      "Test: [0/20]\tTime 0.296 (0.296)\tLoss 6.1213 (6.1213)\tKL loss 6.205 (6.205)\tAcc@1 80.469 (80.469)\n",
      "Test: [10/20]\tTime 0.290 (0.291)\tLoss 5.7194 (6.1356)\tKL loss 6.020 (6.364)\tAcc@1 81.445 (80.877)\n",
      " * Acc@1 80.810\n",
      "Train: [4][10/98]\tBT 0.679 (0.697)\tDT 0.270 (0.286)\tloss 0.381 (0.476)\tKL loss 0.518 (0.553)\tAcc@1 94.922 (94.727)\n",
      "Train: [4][20/98]\tBT 0.683 (0.697)\tDT 0.274 (0.287)\tloss 0.629 (0.536)\tKL loss 0.680 (0.606)\tAcc@1 94.531 (94.609)\n",
      "Train: [4][30/98]\tBT 0.682 (0.692)\tDT 0.272 (0.281)\tloss 0.504 (0.551)\tKL loss 0.536 (0.619)\tAcc@1 95.508 (94.648)\n",
      "Train: [4][40/98]\tBT 0.676 (0.690)\tDT 0.267 (0.280)\tloss 0.843 (0.537)\tKL loss 0.820 (0.601)\tAcc@1 93.750 (94.688)\n",
      "Train: [4][50/98]\tBT 0.670 (0.689)\tDT 0.262 (0.279)\tloss 0.615 (0.536)\tKL loss 0.689 (0.602)\tAcc@1 93.164 (94.605)\n",
      "Train: [4][60/98]\tBT 0.677 (0.688)\tDT 0.265 (0.277)\tloss 0.513 (0.519)\tKL loss 0.565 (0.586)\tAcc@1 94.922 (94.639)\n",
      "Train: [4][70/98]\tBT 0.674 (0.687)\tDT 0.261 (0.276)\tloss 0.594 (0.530)\tKL loss 0.652 (0.592)\tAcc@1 94.141 (94.562)\n",
      "Train: [4][80/98]\tBT 0.696 (0.686)\tDT 0.283 (0.276)\tloss 0.640 (0.530)\tKL loss 0.681 (0.591)\tAcc@1 92.969 (94.524)\n",
      "Train: [4][90/98]\tBT 0.696 (0.686)\tDT 0.283 (0.275)\tloss 0.243 (0.528)\tKL loss 0.290 (0.587)\tAcc@1 95.508 (94.484)\n",
      "Train epoch 4, total time 67.47, accuracy:94.48\n",
      "Test: [0/20]\tTime 0.289 (0.289)\tLoss 4.8768 (4.8768)\tKL loss 4.948 (4.948)\tAcc@1 81.250 (81.250)\n",
      "Test: [10/20]\tTime 0.296 (0.294)\tLoss 4.5872 (4.8062)\tKL loss 4.791 (4.990)\tAcc@1 79.688 (80.806)\n",
      " * Acc@1 80.750\n",
      "Train: [5][10/98]\tBT 0.683 (0.682)\tDT 0.268 (0.268)\tloss 0.844 (0.470)\tKL loss 0.757 (0.505)\tAcc@1 92.383 (94.199)\n",
      "Train: [5][20/98]\tBT 0.680 (0.684)\tDT 0.265 (0.270)\tloss 0.427 (0.458)\tKL loss 0.456 (0.499)\tAcc@1 93.750 (94.170)\n",
      "Train: [5][30/98]\tBT 0.674 (0.684)\tDT 0.261 (0.270)\tloss 0.375 (0.434)\tKL loss 0.402 (0.480)\tAcc@1 93.555 (94.408)\n",
      "Train: [5][40/98]\tBT 0.669 (0.683)\tDT 0.257 (0.270)\tloss 0.312 (0.414)\tKL loss 0.328 (0.460)\tAcc@1 95.703 (94.468)\n",
      "Train: [5][50/98]\tBT 0.688 (0.683)\tDT 0.273 (0.270)\tloss 0.260 (0.417)\tKL loss 0.287 (0.463)\tAcc@1 94.922 (94.473)\n",
      "Train: [5][60/98]\tBT 0.677 (0.683)\tDT 0.265 (0.270)\tloss 0.397 (0.419)\tKL loss 0.436 (0.466)\tAcc@1 95.312 (94.456)\n",
      "Train: [5][70/98]\tBT 0.681 (0.683)\tDT 0.270 (0.270)\tloss 0.268 (0.408)\tKL loss 0.356 (0.457)\tAcc@1 95.508 (94.459)\n",
      "Train: [5][80/98]\tBT 0.684 (0.686)\tDT 0.272 (0.272)\tloss 0.247 (0.401)\tKL loss 0.294 (0.450)\tAcc@1 95.312 (94.434)\n",
      "Train: [5][90/98]\tBT 0.674 (0.686)\tDT 0.263 (0.272)\tloss 0.512 (0.399)\tKL loss 0.545 (0.450)\tAcc@1 92.188 (94.397)\n",
      "Train epoch 5, total time 67.18, accuracy:94.39\n",
      "Test: [0/20]\tTime 0.291 (0.291)\tLoss 4.1027 (4.1027)\tKL loss 4.196 (4.196)\tAcc@1 80.859 (80.859)\n",
      "Test: [10/20]\tTime 0.290 (0.291)\tLoss 3.5811 (4.1290)\tKL loss 3.788 (4.290)\tAcc@1 80.859 (80.895)\n",
      " * Acc@1 80.860\n",
      "Train: [6][10/98]\tBT 0.675 (0.683)\tDT 0.262 (0.269)\tloss 0.352 (0.380)\tKL loss 0.426 (0.410)\tAcc@1 93.750 (94.609)\n",
      "Train: [6][20/98]\tBT 0.681 (0.682)\tDT 0.268 (0.267)\tloss 0.239 (0.382)\tKL loss 0.304 (0.424)\tAcc@1 93.359 (94.580)\n",
      "Train: [6][30/98]\tBT 0.683 (0.683)\tDT 0.270 (0.269)\tloss 0.249 (0.377)\tKL loss 0.401 (0.422)\tAcc@1 95.898 (94.440)\n",
      "Train: [6][40/98]\tBT 0.679 (0.683)\tDT 0.261 (0.269)\tloss 0.248 (0.376)\tKL loss 0.321 (0.423)\tAcc@1 93.945 (94.414)\n",
      "Train: [6][50/98]\tBT 0.674 (0.684)\tDT 0.258 (0.269)\tloss 0.386 (0.382)\tKL loss 0.423 (0.429)\tAcc@1 93.164 (94.352)\n",
      "Train: [6][60/98]\tBT 0.675 (0.687)\tDT 0.260 (0.272)\tloss 0.339 (0.377)\tKL loss 0.507 (0.423)\tAcc@1 95.508 (94.427)\n",
      "Train: [6][70/98]\tBT 0.684 (0.686)\tDT 0.267 (0.272)\tloss 0.304 (0.371)\tKL loss 0.399 (0.416)\tAcc@1 94.531 (94.395)\n",
      "Train: [6][80/98]\tBT 0.678 (0.686)\tDT 0.264 (0.271)\tloss 0.345 (0.372)\tKL loss 0.397 (0.421)\tAcc@1 93.750 (94.377)\n",
      "Train: [6][90/98]\tBT 0.692 (0.686)\tDT 0.277 (0.271)\tloss 0.390 (0.368)\tKL loss 0.468 (0.416)\tAcc@1 93.945 (94.377)\n",
      "Train epoch 6, total time 67.15, accuracy:94.33\n",
      "Test: [0/20]\tTime 0.290 (0.290)\tLoss 3.6075 (3.6075)\tKL loss 3.671 (3.671)\tAcc@1 80.078 (80.078)\n",
      "Test: [10/20]\tTime 0.284 (0.289)\tLoss 3.2527 (3.5445)\tKL loss 3.388 (3.664)\tAcc@1 80.078 (80.913)\n",
      " * Acc@1 80.810\n",
      "Train: [7][10/98]\tBT 0.679 (0.681)\tDT 0.265 (0.268)\tloss 0.375 (0.351)\tKL loss 0.381 (0.372)\tAcc@1 94.922 (94.531)\n",
      "Train: [7][20/98]\tBT 0.667 (0.682)\tDT 0.255 (0.269)\tloss 0.359 (0.324)\tKL loss 0.397 (0.366)\tAcc@1 94.531 (94.639)\n",
      "Train: [7][30/98]\tBT 0.685 (0.683)\tDT 0.272 (0.270)\tloss 0.296 (0.328)\tKL loss 0.349 (0.369)\tAcc@1 94.922 (94.557)\n",
      "Train: [7][40/98]\tBT 0.679 (0.687)\tDT 0.267 (0.274)\tloss 0.436 (0.334)\tKL loss 0.429 (0.374)\tAcc@1 94.531 (94.375)\n",
      "Train: [7][50/98]\tBT 0.684 (0.687)\tDT 0.268 (0.274)\tloss 0.211 (0.334)\tKL loss 0.234 (0.373)\tAcc@1 94.922 (94.398)\n",
      "Train: [7][60/98]\tBT 0.671 (0.687)\tDT 0.257 (0.273)\tloss 0.412 (0.330)\tKL loss 0.443 (0.369)\tAcc@1 92.773 (94.437)\n",
      "Train: [7][70/98]\tBT 0.678 (0.686)\tDT 0.264 (0.273)\tloss 0.381 (0.333)\tKL loss 0.423 (0.371)\tAcc@1 94.336 (94.400)\n",
      "Train: [7][80/98]\tBT 0.692 (0.687)\tDT 0.276 (0.273)\tloss 0.304 (0.331)\tKL loss 0.312 (0.368)\tAcc@1 93.750 (94.346)\n",
      "Train: [7][90/98]\tBT 0.674 (0.687)\tDT 0.261 (0.273)\tloss 0.267 (0.326)\tKL loss 0.276 (0.363)\tAcc@1 95.508 (94.353)\n",
      "Train epoch 7, total time 67.36, accuracy:94.33\n",
      "Test: [0/20]\tTime 0.287 (0.287)\tLoss 3.0977 (3.0977)\tKL loss 3.141 (3.141)\tAcc@1 80.859 (80.859)\n",
      "Test: [10/20]\tTime 0.285 (0.291)\tLoss 2.7655 (3.1370)\tKL loss 2.912 (3.238)\tAcc@1 79.492 (80.700)\n",
      " * Acc@1 80.640\n",
      "Train: [8][10/98]\tBT 0.686 (0.681)\tDT 0.272 (0.267)\tloss 0.321 (0.288)\tKL loss 0.349 (0.326)\tAcc@1 94.727 (94.531)\n",
      "Train: [8][20/98]\tBT 0.681 (0.691)\tDT 0.267 (0.277)\tloss 0.268 (0.274)\tKL loss 0.326 (0.312)\tAcc@1 93.555 (94.443)\n",
      "Train: [8][30/98]\tBT 0.690 (0.689)\tDT 0.273 (0.275)\tloss 0.257 (0.267)\tKL loss 0.273 (0.303)\tAcc@1 94.531 (94.648)\n",
      "Train: [8][40/98]\tBT 0.688 (0.689)\tDT 0.273 (0.274)\tloss 0.203 (0.271)\tKL loss 0.226 (0.303)\tAcc@1 94.141 (94.629)\n",
      "Train: [8][50/98]\tBT 0.676 (0.687)\tDT 0.263 (0.273)\tloss 0.297 (0.268)\tKL loss 0.476 (0.302)\tAcc@1 93.750 (94.621)\n",
      "Train: [8][60/98]\tBT 0.678 (0.687)\tDT 0.266 (0.273)\tloss 0.274 (0.268)\tKL loss 0.331 (0.302)\tAcc@1 95.508 (94.665)\n",
      "Train: [8][70/98]\tBT 0.681 (0.686)\tDT 0.268 (0.272)\tloss 0.177 (0.273)\tKL loss 0.189 (0.304)\tAcc@1 95.508 (94.637)\n",
      "Train: [8][80/98]\tBT 0.689 (0.686)\tDT 0.273 (0.272)\tloss 0.448 (0.276)\tKL loss 0.440 (0.306)\tAcc@1 90.625 (94.548)\n",
      "Train: [8][90/98]\tBT 0.671 (0.685)\tDT 0.260 (0.271)\tloss 0.276 (0.274)\tKL loss 0.321 (0.303)\tAcc@1 94.531 (94.551)\n",
      "Train epoch 8, total time 67.16, accuracy:94.56\n",
      "Test: [0/20]\tTime 0.288 (0.288)\tLoss 2.9298 (2.9298)\tKL loss 2.978 (2.978)\tAcc@1 80.664 (80.664)\n",
      "Test: [10/20]\tTime 0.300 (0.294)\tLoss 2.8405 (2.9670)\tKL loss 2.936 (3.050)\tAcc@1 78.711 (80.362)\n",
      " * Acc@1 80.660\n",
      "Train: [9][10/98]\tBT 0.678 (0.682)\tDT 0.263 (0.268)\tloss 0.291 (0.321)\tKL loss 0.298 (0.345)\tAcc@1 93.555 (94.570)\n",
      "Train: [9][20/98]\tBT 0.671 (0.683)\tDT 0.257 (0.269)\tloss 0.383 (0.322)\tKL loss 0.398 (0.347)\tAcc@1 92.773 (94.404)\n",
      "Train: [9][30/98]\tBT 0.676 (0.682)\tDT 0.263 (0.269)\tloss 0.413 (0.319)\tKL loss 0.500 (0.346)\tAcc@1 92.773 (94.362)\n",
      "Train: [9][40/98]\tBT 0.680 (0.683)\tDT 0.266 (0.269)\tloss 0.241 (0.315)\tKL loss 0.290 (0.345)\tAcc@1 94.922 (94.292)\n",
      "Train: [9][50/98]\tBT 0.676 (0.682)\tDT 0.258 (0.268)\tloss 0.272 (0.314)\tKL loss 0.351 (0.346)\tAcc@1 94.336 (94.250)\n",
      "Train: [9][60/98]\tBT 0.687 (0.682)\tDT 0.270 (0.268)\tloss 0.261 (0.314)\tKL loss 0.281 (0.347)\tAcc@1 95.312 (94.245)\n",
      "Train: [9][70/98]\tBT 0.672 (0.681)\tDT 0.259 (0.268)\tloss 0.195 (0.305)\tKL loss 0.220 (0.338)\tAcc@1 95.508 (94.300)\n",
      "Train: [9][80/98]\tBT 0.690 (0.682)\tDT 0.278 (0.268)\tloss 0.352 (0.300)\tKL loss 0.355 (0.332)\tAcc@1 93.555 (94.385)\n",
      "Train: [9][90/98]\tBT 0.682 (0.685)\tDT 0.269 (0.271)\tloss 0.313 (0.294)\tKL loss 0.332 (0.326)\tAcc@1 93.164 (94.392)\n",
      "Train epoch 9, total time 67.09, accuracy:94.38\n",
      "Test: [0/20]\tTime 0.287 (0.287)\tLoss 2.6522 (2.6522)\tKL loss 2.688 (2.688)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.297 (0.295)\tLoss 2.5933 (2.7213)\tKL loss 2.692 (2.798)\tAcc@1 78.711 (80.700)\n",
      " * Acc@1 80.810\n",
      "Train: [10][10/98]\tBT 0.688 (0.684)\tDT 0.273 (0.270)\tloss 0.211 (0.264)\tKL loss 0.280 (0.288)\tAcc@1 95.312 (94.336)\n",
      "Train: [10][20/98]\tBT 0.682 (0.686)\tDT 0.268 (0.272)\tloss 0.468 (0.303)\tKL loss 0.505 (0.328)\tAcc@1 94.141 (94.219)\n",
      "Train: [10][30/98]\tBT 0.686 (0.686)\tDT 0.271 (0.272)\tloss 0.286 (0.298)\tKL loss 0.388 (0.323)\tAcc@1 93.945 (94.297)\n",
      "Train: [10][40/98]\tBT 0.677 (0.685)\tDT 0.265 (0.271)\tloss 0.342 (0.295)\tKL loss 0.354 (0.319)\tAcc@1 92.969 (94.341)\n",
      "Train: [10][50/98]\tBT 0.671 (0.685)\tDT 0.261 (0.271)\tloss 0.267 (0.286)\tKL loss 0.305 (0.312)\tAcc@1 94.336 (94.312)\n",
      "Train: [10][60/98]\tBT 0.674 (0.685)\tDT 0.262 (0.271)\tloss 0.315 (0.288)\tKL loss 0.340 (0.312)\tAcc@1 93.750 (94.326)\n",
      "Train: [10][70/98]\tBT 0.689 (0.689)\tDT 0.275 (0.275)\tloss 0.300 (0.286)\tKL loss 0.309 (0.311)\tAcc@1 93.750 (94.322)\n",
      "Train: [10][80/98]\tBT 0.694 (0.688)\tDT 0.281 (0.274)\tloss 0.283 (0.285)\tKL loss 0.283 (0.310)\tAcc@1 95.117 (94.314)\n",
      "Train: [10][90/98]\tBT 0.695 (0.688)\tDT 0.281 (0.274)\tloss 0.210 (0.284)\tKL loss 0.231 (0.310)\tAcc@1 94.922 (94.286)\n",
      "Train epoch 10, total time 67.40, accuracy:94.32\n",
      "Test: [0/20]\tTime 0.301 (0.301)\tLoss 2.3861 (2.3861)\tKL loss 2.400 (2.400)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.287 (0.292)\tLoss 2.2019 (2.4274)\tKL loss 2.276 (2.492)\tAcc@1 79.688 (81.019)\n",
      " * Acc@1 80.860\n",
      "Train: [11][10/98]\tBT 0.675 (0.682)\tDT 0.262 (0.270)\tloss 0.161 (0.228)\tKL loss 0.169 (0.261)\tAcc@1 94.531 (94.316)\n",
      "Train: [11][20/98]\tBT 0.680 (0.682)\tDT 0.265 (0.269)\tloss 0.231 (0.242)\tKL loss 0.239 (0.270)\tAcc@1 95.117 (94.404)\n",
      "Train: [11][30/98]\tBT 0.682 (0.684)\tDT 0.270 (0.271)\tloss 0.228 (0.252)\tKL loss 0.301 (0.283)\tAcc@1 94.922 (94.401)\n",
      "Train: [11][40/98]\tBT 0.674 (0.684)\tDT 0.264 (0.270)\tloss 0.208 (0.268)\tKL loss 0.222 (0.297)\tAcc@1 94.531 (94.395)\n",
      "Train: [11][50/98]\tBT 0.675 (0.688)\tDT 0.261 (0.274)\tloss 0.177 (0.268)\tKL loss 0.192 (0.297)\tAcc@1 95.508 (94.359)\n",
      "Train: [11][60/98]\tBT 0.671 (0.688)\tDT 0.257 (0.274)\tloss 0.190 (0.270)\tKL loss 0.213 (0.296)\tAcc@1 94.141 (94.287)\n",
      "Train: [11][70/98]\tBT 0.680 (0.687)\tDT 0.267 (0.273)\tloss 0.200 (0.267)\tKL loss 0.237 (0.291)\tAcc@1 93.945 (94.275)\n",
      "Train: [11][80/98]\tBT 0.680 (0.686)\tDT 0.267 (0.273)\tloss 0.229 (0.266)\tKL loss 0.239 (0.289)\tAcc@1 95.117 (94.287)\n",
      "Train: [11][90/98]\tBT 0.685 (0.687)\tDT 0.268 (0.273)\tloss 0.305 (0.266)\tKL loss 0.350 (0.290)\tAcc@1 93.945 (94.314)\n",
      "Train epoch 11, total time 67.41, accuracy:94.27\n",
      "Test: [0/20]\tTime 0.286 (0.286)\tLoss 2.1738 (2.1738)\tKL loss 2.187 (2.187)\tAcc@1 79.102 (79.102)\n",
      "Test: [10/20]\tTime 0.287 (0.289)\tLoss 2.0328 (2.2082)\tKL loss 2.095 (2.253)\tAcc@1 79.297 (80.558)\n",
      " * Acc@1 80.560\n",
      "Train: [12][10/98]\tBT 0.668 (0.676)\tDT 0.257 (0.264)\tloss 0.206 (0.245)\tKL loss 0.218 (0.268)\tAcc@1 94.141 (94.277)\n",
      "Train: [12][20/98]\tBT 0.669 (0.677)\tDT 0.257 (0.266)\tloss 0.231 (0.262)\tKL loss 0.273 (0.275)\tAcc@1 94.336 (94.326)\n",
      "Train: [12][30/98]\tBT 0.685 (0.678)\tDT 0.265 (0.266)\tloss 0.260 (0.259)\tKL loss 0.290 (0.277)\tAcc@1 95.117 (94.290)\n",
      "Train: [12][40/98]\tBT 0.688 (0.686)\tDT 0.273 (0.273)\tloss 0.158 (0.257)\tKL loss 0.185 (0.274)\tAcc@1 95.312 (94.302)\n",
      "Train: [12][50/98]\tBT 0.669 (0.685)\tDT 0.258 (0.272)\tloss 0.242 (0.255)\tKL loss 0.275 (0.273)\tAcc@1 94.922 (94.301)\n",
      "Train: [12][60/98]\tBT 0.679 (0.685)\tDT 0.265 (0.271)\tloss 0.206 (0.253)\tKL loss 0.223 (0.272)\tAcc@1 94.922 (94.333)\n",
      "Train: [12][70/98]\tBT 0.678 (0.684)\tDT 0.265 (0.271)\tloss 0.089 (0.248)\tKL loss 0.116 (0.268)\tAcc@1 96.875 (94.378)\n",
      "Train: [12][80/98]\tBT 0.677 (0.684)\tDT 0.263 (0.270)\tloss 0.291 (0.252)\tKL loss 0.302 (0.271)\tAcc@1 93.164 (94.333)\n",
      "Train: [12][90/98]\tBT 0.679 (0.684)\tDT 0.264 (0.270)\tloss 0.233 (0.255)\tKL loss 0.255 (0.274)\tAcc@1 95.508 (94.273)\n",
      "Train epoch 12, total time 66.99, accuracy:94.30\n",
      "Test: [0/20]\tTime 0.284 (0.284)\tLoss 1.9673 (1.9673)\tKL loss 1.989 (1.989)\tAcc@1 78.516 (78.516)\n",
      "Test: [10/20]\tTime 0.283 (0.288)\tLoss 2.0999 (2.0860)\tKL loss 2.143 (2.134)\tAcc@1 78.320 (80.362)\n",
      " * Acc@1 80.660\n",
      "Train: [13][10/98]\tBT 0.669 (0.685)\tDT 0.257 (0.271)\tloss 0.249 (0.274)\tKL loss 0.278 (0.295)\tAcc@1 93.945 (93.672)\n",
      "Train: [13][20/98]\tBT 0.676 (0.698)\tDT 0.268 (0.283)\tloss 0.325 (0.260)\tKL loss 0.328 (0.284)\tAcc@1 91.406 (93.857)\n",
      "Train: [13][30/98]\tBT 0.680 (0.693)\tDT 0.268 (0.278)\tloss 0.288 (0.265)\tKL loss 0.309 (0.289)\tAcc@1 92.969 (93.867)\n",
      "Train: [13][40/98]\tBT 0.678 (0.690)\tDT 0.267 (0.275)\tloss 0.193 (0.277)\tKL loss 0.231 (0.296)\tAcc@1 94.922 (93.838)\n",
      "Train: [13][50/98]\tBT 0.677 (0.688)\tDT 0.265 (0.274)\tloss 0.131 (0.264)\tKL loss 0.174 (0.282)\tAcc@1 95.703 (94.004)\n",
      "Train: [13][60/98]\tBT 0.679 (0.687)\tDT 0.265 (0.273)\tloss 0.245 (0.267)\tKL loss 0.264 (0.284)\tAcc@1 95.117 (94.040)\n",
      "Train: [13][70/98]\tBT 0.677 (0.686)\tDT 0.263 (0.272)\tloss 0.226 (0.266)\tKL loss 0.243 (0.282)\tAcc@1 95.117 (94.099)\n",
      "Train: [13][80/98]\tBT 0.678 (0.686)\tDT 0.265 (0.272)\tloss 0.157 (0.268)\tKL loss 0.175 (0.285)\tAcc@1 96.680 (94.072)\n",
      "Train: [13][90/98]\tBT 0.677 (0.685)\tDT 0.261 (0.271)\tloss 0.363 (0.268)\tKL loss 0.404 (0.285)\tAcc@1 91.797 (94.067)\n",
      "Train epoch 13, total time 67.09, accuracy:94.05\n",
      "Test: [0/20]\tTime 0.306 (0.306)\tLoss 2.0094 (2.0094)\tKL loss 2.000 (2.000)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.291 (0.325)\tLoss 1.9447 (1.9436)\tKL loss 1.968 (1.974)\tAcc@1 78.711 (81.197)\n",
      " * Acc@1 81.140\n",
      "Train: [14][10/98]\tBT 0.685 (0.685)\tDT 0.267 (0.271)\tloss 0.377 (0.225)\tKL loss 0.425 (0.264)\tAcc@1 93.750 (94.727)\n",
      "Train: [14][20/98]\tBT 0.678 (0.684)\tDT 0.261 (0.269)\tloss 0.139 (0.233)\tKL loss 0.151 (0.266)\tAcc@1 96.875 (94.717)\n",
      "Train: [14][30/98]\tBT 0.687 (0.684)\tDT 0.274 (0.270)\tloss 0.267 (0.245)\tKL loss 0.274 (0.270)\tAcc@1 94.531 (94.518)\n",
      "Train: [14][40/98]\tBT 0.680 (0.685)\tDT 0.269 (0.271)\tloss 0.176 (0.236)\tKL loss 0.228 (0.259)\tAcc@1 95.312 (94.595)\n",
      "Train: [14][50/98]\tBT 0.678 (0.685)\tDT 0.265 (0.271)\tloss 0.343 (0.234)\tKL loss 0.376 (0.255)\tAcc@1 93.164 (94.582)\n",
      "Train: [14][60/98]\tBT 0.683 (0.684)\tDT 0.271 (0.270)\tloss 0.200 (0.236)\tKL loss 0.208 (0.257)\tAcc@1 94.531 (94.561)\n",
      "Train: [14][70/98]\tBT 0.667 (0.684)\tDT 0.256 (0.270)\tloss 0.278 (0.240)\tKL loss 0.314 (0.261)\tAcc@1 93.945 (94.528)\n",
      "Train: [14][80/98]\tBT 0.869 (0.686)\tDT 0.437 (0.272)\tloss 0.208 (0.243)\tKL loss 0.240 (0.264)\tAcc@1 94.922 (94.495)\n",
      "Train: [14][90/98]\tBT 0.691 (0.686)\tDT 0.276 (0.272)\tloss 0.186 (0.242)\tKL loss 0.231 (0.262)\tAcc@1 95.312 (94.520)\n",
      "Train epoch 14, total time 67.22, accuracy:94.53\n",
      "Test: [0/20]\tTime 0.284 (0.284)\tLoss 1.7798 (1.7798)\tKL loss 1.797 (1.797)\tAcc@1 80.859 (80.859)\n",
      "Test: [10/20]\tTime 0.290 (0.289)\tLoss 1.7548 (1.8716)\tKL loss 1.783 (1.903)\tAcc@1 79.492 (81.179)\n",
      " * Acc@1 80.970\n",
      "Train: [15][10/98]\tBT 0.677 (0.679)\tDT 0.263 (0.266)\tloss 0.135 (0.207)\tKL loss 0.183 (0.231)\tAcc@1 96.289 (94.473)\n",
      "Train: [15][20/98]\tBT 0.686 (0.679)\tDT 0.272 (0.266)\tloss 0.213 (0.222)\tKL loss 0.220 (0.243)\tAcc@1 93.359 (94.189)\n",
      "Train: [15][30/98]\tBT 0.686 (0.681)\tDT 0.272 (0.268)\tloss 0.506 (0.234)\tKL loss 0.405 (0.252)\tAcc@1 97.070 (94.421)\n",
      "Train: [15][40/98]\tBT 0.684 (0.680)\tDT 0.271 (0.267)\tloss 0.189 (0.227)\tKL loss 0.196 (0.244)\tAcc@1 96.289 (94.497)\n",
      "Train: [15][50/98]\tBT 0.684 (0.681)\tDT 0.268 (0.268)\tloss 0.363 (0.245)\tKL loss 0.362 (0.259)\tAcc@1 94.531 (94.391)\n",
      "Train: [15][60/98]\tBT 0.692 (0.682)\tDT 0.278 (0.268)\tloss 0.248 (0.250)\tKL loss 0.265 (0.265)\tAcc@1 93.359 (94.300)\n",
      "Train: [15][70/98]\tBT 0.675 (0.684)\tDT 0.262 (0.271)\tloss 0.217 (0.245)\tKL loss 0.222 (0.262)\tAcc@1 94.141 (94.333)\n",
      "Train: [15][80/98]\tBT 0.678 (0.683)\tDT 0.269 (0.270)\tloss 0.291 (0.248)\tKL loss 0.306 (0.264)\tAcc@1 91.602 (94.246)\n",
      "Train: [15][90/98]\tBT 0.676 (0.683)\tDT 0.264 (0.270)\tloss 0.418 (0.247)\tKL loss 0.382 (0.261)\tAcc@1 93.359 (94.266)\n",
      "Train epoch 15, total time 66.91, accuracy:94.26\n",
      "Test: [0/20]\tTime 0.290 (0.290)\tLoss 1.8410 (1.8410)\tKL loss 1.852 (1.852)\tAcc@1 79.492 (79.492)\n",
      "Test: [10/20]\tTime 0.282 (0.289)\tLoss 1.7695 (1.8374)\tKL loss 1.781 (1.869)\tAcc@1 80.273 (80.380)\n",
      " * Acc@1 80.480\n",
      "Train: [16][10/98]\tBT 0.683 (0.684)\tDT 0.272 (0.272)\tloss 0.246 (0.251)\tKL loss 0.256 (0.266)\tAcc@1 93.164 (94.082)\n",
      "Train: [16][20/98]\tBT 0.673 (0.683)\tDT 0.259 (0.271)\tloss 0.188 (0.247)\tKL loss 0.228 (0.264)\tAcc@1 95.312 (93.975)\n",
      "Train: [16][30/98]\tBT 0.679 (0.683)\tDT 0.264 (0.270)\tloss 0.145 (0.224)\tKL loss 0.162 (0.244)\tAcc@1 95.898 (94.284)\n",
      "Train: [16][40/98]\tBT 0.673 (0.682)\tDT 0.259 (0.268)\tloss 0.256 (0.227)\tKL loss 0.268 (0.246)\tAcc@1 93.555 (94.355)\n",
      "Train: [16][50/98]\tBT 0.684 (0.686)\tDT 0.268 (0.273)\tloss 0.146 (0.227)\tKL loss 0.172 (0.243)\tAcc@1 95.898 (94.430)\n",
      "Train: [16][60/98]\tBT 0.674 (0.686)\tDT 0.262 (0.272)\tloss 0.181 (0.227)\tKL loss 0.194 (0.246)\tAcc@1 93.555 (94.424)\n",
      "Train: [16][70/98]\tBT 0.689 (0.686)\tDT 0.275 (0.273)\tloss 0.173 (0.224)\tKL loss 0.189 (0.242)\tAcc@1 95.508 (94.475)\n",
      "Train: [16][80/98]\tBT 0.684 (0.685)\tDT 0.268 (0.272)\tloss 0.146 (0.221)\tKL loss 0.152 (0.239)\tAcc@1 95.508 (94.500)\n",
      "Train: [16][90/98]\tBT 0.677 (0.685)\tDT 0.264 (0.271)\tloss 0.228 (0.221)\tKL loss 0.239 (0.239)\tAcc@1 95.117 (94.507)\n",
      "Train epoch 16, total time 67.06, accuracy:94.50\n",
      "Test: [0/20]\tTime 0.297 (0.297)\tLoss 1.8943 (1.8943)\tKL loss 1.873 (1.873)\tAcc@1 80.664 (80.664)\n",
      "Test: [10/20]\tTime 0.287 (0.292)\tLoss 1.9868 (1.9366)\tKL loss 1.990 (1.961)\tAcc@1 78.320 (80.451)\n",
      " * Acc@1 80.150\n",
      "Train: [17][10/98]\tBT 0.692 (0.681)\tDT 0.277 (0.268)\tloss 0.175 (0.197)\tKL loss 0.208 (0.220)\tAcc@1 94.531 (94.883)\n",
      "Train: [17][20/98]\tBT 0.673 (0.684)\tDT 0.262 (0.271)\tloss 0.165 (0.200)\tKL loss 0.191 (0.218)\tAcc@1 94.336 (94.893)\n",
      "Train: [17][30/98]\tBT 0.657 (0.689)\tDT 0.247 (0.275)\tloss 0.182 (0.196)\tKL loss 0.187 (0.213)\tAcc@1 95.508 (94.987)\n",
      "Train: [17][40/98]\tBT 0.692 (0.689)\tDT 0.278 (0.275)\tloss 0.275 (0.202)\tKL loss 0.300 (0.217)\tAcc@1 93.750 (94.990)\n",
      "Train: [17][50/98]\tBT 0.683 (0.688)\tDT 0.269 (0.274)\tloss 0.245 (0.204)\tKL loss 0.239 (0.218)\tAcc@1 95.312 (94.949)\n",
      "Train: [17][60/98]\tBT 0.677 (0.687)\tDT 0.265 (0.273)\tloss 0.151 (0.210)\tKL loss 0.162 (0.226)\tAcc@1 95.508 (94.759)\n",
      "Train: [17][70/98]\tBT 0.676 (0.686)\tDT 0.264 (0.272)\tloss 0.322 (0.217)\tKL loss 0.333 (0.232)\tAcc@1 91.992 (94.679)\n",
      "Train: [17][80/98]\tBT 0.706 (0.685)\tDT 0.292 (0.271)\tloss 0.246 (0.221)\tKL loss 0.275 (0.236)\tAcc@1 94.336 (94.639)\n",
      "Train: [17][90/98]\tBT 0.684 (0.685)\tDT 0.272 (0.272)\tloss 0.162 (0.220)\tKL loss 0.193 (0.236)\tAcc@1 95.703 (94.646)\n",
      "Train epoch 17, total time 67.12, accuracy:94.66\n",
      "Test: [0/20]\tTime 0.289 (0.289)\tLoss 1.7951 (1.7951)\tKL loss 1.798 (1.798)\tAcc@1 82.031 (82.031)\n",
      "Test: [10/20]\tTime 0.283 (0.289)\tLoss 1.8339 (1.8025)\tKL loss 1.836 (1.822)\tAcc@1 78.711 (80.842)\n",
      " * Acc@1 80.790\n",
      "Train: [18][10/98]\tBT 0.680 (0.704)\tDT 0.268 (0.290)\tloss 0.160 (0.209)\tKL loss 0.185 (0.227)\tAcc@1 95.117 (94.531)\n",
      "Train: [18][20/98]\tBT 0.684 (0.693)\tDT 0.273 (0.280)\tloss 0.249 (0.218)\tKL loss 0.263 (0.237)\tAcc@1 95.117 (94.365)\n",
      "Train: [18][30/98]\tBT 0.680 (0.689)\tDT 0.267 (0.276)\tloss 0.172 (0.221)\tKL loss 0.228 (0.240)\tAcc@1 94.531 (94.297)\n",
      "Train: [18][40/98]\tBT 0.677 (0.686)\tDT 0.264 (0.273)\tloss 0.302 (0.218)\tKL loss 0.328 (0.236)\tAcc@1 93.164 (94.287)\n",
      "Train: [18][50/98]\tBT 0.676 (0.685)\tDT 0.265 (0.272)\tloss 0.169 (0.219)\tKL loss 0.176 (0.235)\tAcc@1 95.117 (94.383)\n",
      "Train: [18][60/98]\tBT 0.682 (0.684)\tDT 0.268 (0.272)\tloss 0.263 (0.219)\tKL loss 0.286 (0.235)\tAcc@1 93.164 (94.352)\n",
      "Train: [18][70/98]\tBT 0.674 (0.685)\tDT 0.261 (0.271)\tloss 0.212 (0.220)\tKL loss 0.206 (0.233)\tAcc@1 95.508 (94.417)\n",
      "Train: [18][80/98]\tBT 0.690 (0.684)\tDT 0.275 (0.271)\tloss 0.244 (0.222)\tKL loss 0.257 (0.235)\tAcc@1 94.531 (94.395)\n",
      "Train: [18][90/98]\tBT 0.673 (0.684)\tDT 0.260 (0.271)\tloss 0.181 (0.225)\tKL loss 0.187 (0.237)\tAcc@1 95.508 (94.358)\n",
      "Train epoch 18, total time 67.25, accuracy:94.35\n",
      "Test: [0/20]\tTime 0.287 (0.287)\tLoss 1.7147 (1.7147)\tKL loss 1.710 (1.710)\tAcc@1 82.422 (82.422)\n",
      "Test: [10/20]\tTime 0.294 (0.289)\tLoss 1.7083 (1.7438)\tKL loss 1.703 (1.756)\tAcc@1 78.711 (80.753)\n",
      " * Acc@1 80.740\n",
      "Train: [19][10/98]\tBT 0.682 (0.688)\tDT 0.266 (0.273)\tloss 0.332 (0.247)\tKL loss 0.345 (0.264)\tAcc@1 92.578 (94.297)\n",
      "Train: [19][20/98]\tBT 0.686 (0.686)\tDT 0.270 (0.272)\tloss 0.244 (0.241)\tKL loss 0.253 (0.257)\tAcc@1 94.336 (94.307)\n",
      "Train: [19][30/98]\tBT 0.683 (0.685)\tDT 0.267 (0.270)\tloss 0.155 (0.232)\tKL loss 0.204 (0.250)\tAcc@1 96.484 (94.421)\n",
      "Train: [19][40/98]\tBT 0.694 (0.685)\tDT 0.279 (0.270)\tloss 0.146 (0.231)\tKL loss 0.169 (0.248)\tAcc@1 95.117 (94.316)\n",
      "Train: [19][50/98]\tBT 0.678 (0.685)\tDT 0.266 (0.271)\tloss 0.202 (0.228)\tKL loss 0.224 (0.246)\tAcc@1 94.531 (94.406)\n",
      "Train: [19][60/98]\tBT 0.683 (0.684)\tDT 0.269 (0.270)\tloss 0.228 (0.230)\tKL loss 0.240 (0.247)\tAcc@1 93.555 (94.388)\n",
      "Train: [19][70/98]\tBT 0.692 (0.684)\tDT 0.274 (0.270)\tloss 0.178 (0.233)\tKL loss 0.182 (0.248)\tAcc@1 95.703 (94.378)\n",
      "Train: [19][80/98]\tBT 0.678 (0.687)\tDT 0.266 (0.273)\tloss 0.284 (0.230)\tKL loss 0.263 (0.245)\tAcc@1 94.531 (94.463)\n",
      "Train: [19][90/98]\tBT 0.690 (0.686)\tDT 0.275 (0.272)\tloss 0.200 (0.229)\tKL loss 0.232 (0.244)\tAcc@1 95.312 (94.475)\n",
      "Train epoch 19, total time 67.20, accuracy:94.41\n",
      "Test: [0/20]\tTime 0.286 (0.286)\tLoss 1.6055 (1.6055)\tKL loss 1.598 (1.598)\tAcc@1 81.250 (81.250)\n",
      "Test: [10/20]\tTime 0.285 (0.290)\tLoss 1.5606 (1.6429)\tKL loss 1.583 (1.660)\tAcc@1 79.297 (80.771)\n",
      " * Acc@1 80.810\n",
      "Train: [20][10/98]\tBT 0.673 (0.677)\tDT 0.260 (0.266)\tloss 0.131 (0.225)\tKL loss 0.135 (0.244)\tAcc@1 96.094 (94.043)\n",
      "Train: [20][20/98]\tBT 0.672 (0.680)\tDT 0.261 (0.268)\tloss 0.260 (0.244)\tKL loss 0.267 (0.263)\tAcc@1 93.945 (93.877)\n",
      "Train: [20][30/98]\tBT 0.687 (0.682)\tDT 0.275 (0.270)\tloss 0.182 (0.238)\tKL loss 0.202 (0.254)\tAcc@1 95.312 (94.069)\n",
      "Train: [20][40/98]\tBT 0.682 (0.682)\tDT 0.268 (0.270)\tloss 0.308 (0.230)\tKL loss 0.320 (0.246)\tAcc@1 92.773 (94.282)\n",
      "Train: [20][50/98]\tBT 0.686 (0.683)\tDT 0.272 (0.270)\tloss 0.223 (0.221)\tKL loss 0.254 (0.237)\tAcc@1 94.141 (94.445)\n",
      "Train: [20][60/98]\tBT 0.676 (0.686)\tDT 0.265 (0.273)\tloss 0.235 (0.222)\tKL loss 0.245 (0.237)\tAcc@1 93.359 (94.437)\n",
      "Train: [20][70/98]\tBT 0.677 (0.685)\tDT 0.265 (0.272)\tloss 0.214 (0.221)\tKL loss 0.194 (0.236)\tAcc@1 95.703 (94.445)\n",
      "Train: [20][80/98]\tBT 0.676 (0.684)\tDT 0.261 (0.271)\tloss 0.275 (0.224)\tKL loss 0.283 (0.239)\tAcc@1 93.945 (94.380)\n",
      "Train: [20][90/98]\tBT 0.674 (0.684)\tDT 0.262 (0.271)\tloss 0.305 (0.228)\tKL loss 0.312 (0.242)\tAcc@1 92.578 (94.323)\n",
      "Train epoch 20, total time 66.95, accuracy:94.30\n",
      "Test: [0/20]\tTime 0.292 (0.292)\tLoss 1.7113 (1.7113)\tKL loss 1.706 (1.706)\tAcc@1 81.836 (81.836)\n",
      "Test: [10/20]\tTime 0.296 (0.290)\tLoss 1.6512 (1.6314)\tKL loss 1.652 (1.641)\tAcc@1 79.102 (80.984)\n",
      " * Acc@1 80.970\n",
      "Train: [21][10/98]\tBT 0.713 (0.684)\tDT 0.300 (0.270)\tloss 0.191 (0.194)\tKL loss 0.202 (0.208)\tAcc@1 94.922 (94.961)\n",
      "Train: [21][20/98]\tBT 0.676 (0.683)\tDT 0.263 (0.269)\tloss 0.204 (0.200)\tKL loss 0.209 (0.211)\tAcc@1 94.922 (94.824)\n",
      "Train: [21][30/98]\tBT 0.679 (0.683)\tDT 0.262 (0.269)\tloss 0.214 (0.214)\tKL loss 0.224 (0.228)\tAcc@1 93.359 (94.466)\n",
      "Train: [21][40/98]\tBT 0.669 (0.689)\tDT 0.255 (0.275)\tloss 0.268 (0.221)\tKL loss 0.280 (0.235)\tAcc@1 92.383 (94.287)\n",
      "Train: [21][50/98]\tBT 0.681 (0.687)\tDT 0.271 (0.274)\tloss 0.295 (0.223)\tKL loss 0.345 (0.236)\tAcc@1 93.359 (94.305)\n",
      "Train: [21][60/98]\tBT 0.674 (0.687)\tDT 0.259 (0.273)\tloss 0.107 (0.225)\tKL loss 0.139 (0.239)\tAcc@1 96.484 (94.316)\n",
      "Train: [21][70/98]\tBT 0.676 (0.686)\tDT 0.261 (0.272)\tloss 0.288 (0.229)\tKL loss 0.276 (0.243)\tAcc@1 93.555 (94.258)\n",
      "Train: [21][80/98]\tBT 0.680 (0.685)\tDT 0.264 (0.271)\tloss 0.259 (0.228)\tKL loss 0.283 (0.243)\tAcc@1 94.141 (94.280)\n",
      "Train: [21][90/98]\tBT 0.676 (0.684)\tDT 0.264 (0.271)\tloss 0.253 (0.227)\tKL loss 0.286 (0.243)\tAcc@1 92.773 (94.306)\n",
      "Train epoch 21, total time 67.01, accuracy:94.21\n",
      "Test: [0/20]\tTime 0.296 (0.296)\tLoss 1.6540 (1.6540)\tKL loss 1.650 (1.650)\tAcc@1 80.469 (80.469)\n",
      "Test: [10/20]\tTime 0.284 (0.293)\tLoss 1.4145 (1.5977)\tKL loss 1.438 (1.611)\tAcc@1 79.883 (80.966)\n",
      " * Acc@1 80.900\n",
      "Train: [22][10/98]\tBT 0.673 (0.680)\tDT 0.260 (0.268)\tloss 0.174 (0.181)\tKL loss 0.186 (0.195)\tAcc@1 95.117 (94.746)\n",
      "Train: [22][20/98]\tBT 0.681 (0.698)\tDT 0.267 (0.285)\tloss 0.245 (0.209)\tKL loss 0.249 (0.219)\tAcc@1 93.555 (94.385)\n",
      "Train: [22][30/98]\tBT 0.675 (0.691)\tDT 0.262 (0.279)\tloss 0.253 (0.206)\tKL loss 0.318 (0.220)\tAcc@1 93.750 (94.499)\n",
      "Train: [22][40/98]\tBT 0.685 (0.690)\tDT 0.270 (0.277)\tloss 0.210 (0.207)\tKL loss 0.220 (0.220)\tAcc@1 94.336 (94.570)\n",
      "Train: [22][50/98]\tBT 0.683 (0.689)\tDT 0.272 (0.276)\tloss 0.206 (0.209)\tKL loss 0.217 (0.221)\tAcc@1 93.945 (94.496)\n",
      "Train: [22][60/98]\tBT 0.677 (0.687)\tDT 0.263 (0.274)\tloss 0.150 (0.208)\tKL loss 0.158 (0.221)\tAcc@1 95.703 (94.473)\n",
      "Train: [22][70/98]\tBT 0.682 (0.686)\tDT 0.268 (0.273)\tloss 0.164 (0.205)\tKL loss 0.176 (0.217)\tAcc@1 94.531 (94.565)\n",
      "Train: [22][80/98]\tBT 0.677 (0.685)\tDT 0.260 (0.272)\tloss 0.199 (0.203)\tKL loss 0.202 (0.215)\tAcc@1 95.312 (94.597)\n",
      "Train: [22][90/98]\tBT 0.664 (0.685)\tDT 0.253 (0.272)\tloss 0.249 (0.204)\tKL loss 0.267 (0.217)\tAcc@1 94.727 (94.566)\n",
      "Train epoch 22, total time 67.08, accuracy:94.59\n",
      "Test: [0/20]\tTime 0.291 (0.291)\tLoss 1.5648 (1.5648)\tKL loss 1.567 (1.567)\tAcc@1 80.664 (80.664)\n",
      "Test: [10/20]\tTime 0.288 (0.290)\tLoss 1.5780 (1.5523)\tKL loss 1.575 (1.562)\tAcc@1 78.711 (80.788)\n",
      " * Acc@1 80.630\n",
      "Train: [23][10/98]\tBT 0.668 (0.675)\tDT 0.255 (0.262)\tloss 0.178 (0.227)\tKL loss 0.201 (0.233)\tAcc@1 95.508 (94.492)\n",
      "Train: [23][20/98]\tBT 0.673 (0.676)\tDT 0.261 (0.264)\tloss 0.171 (0.219)\tKL loss 0.194 (0.234)\tAcc@1 94.727 (94.482)\n",
      "Train: [23][30/98]\tBT 0.669 (0.676)\tDT 0.259 (0.263)\tloss 0.247 (0.234)\tKL loss 0.248 (0.249)\tAcc@1 93.945 (94.303)\n",
      "Train: [23][40/98]\tBT 0.696 (0.676)\tDT 0.282 (0.264)\tloss 0.230 (0.228)\tKL loss 0.234 (0.243)\tAcc@1 94.531 (94.312)\n",
      "Train: [23][50/98]\tBT 0.676 (0.677)\tDT 0.261 (0.264)\tloss 0.168 (0.234)\tKL loss 0.175 (0.247)\tAcc@1 95.703 (94.242)\n",
      "Train: [23][60/98]\tBT 0.712 (0.678)\tDT 0.298 (0.265)\tloss 0.213 (0.237)\tKL loss 0.235 (0.250)\tAcc@1 94.922 (94.199)\n",
      "Train: [23][70/98]\tBT 0.680 (0.679)\tDT 0.264 (0.266)\tloss 0.269 (0.237)\tKL loss 0.272 (0.249)\tAcc@1 93.359 (94.174)\n",
      "Train: [23][80/98]\tBT 0.680 (0.680)\tDT 0.268 (0.266)\tloss 0.205 (0.237)\tKL loss 0.210 (0.248)\tAcc@1 94.531 (94.180)\n",
      "Train: [23][90/98]\tBT 0.671 (0.683)\tDT 0.258 (0.269)\tloss 0.162 (0.234)\tKL loss 0.201 (0.247)\tAcc@1 95.898 (94.199)\n",
      "Train epoch 23, total time 66.91, accuracy:94.21\n",
      "Test: [0/20]\tTime 0.285 (0.285)\tLoss 1.5164 (1.5164)\tKL loss 1.488 (1.488)\tAcc@1 81.836 (81.836)\n",
      "Test: [10/20]\tTime 0.280 (0.304)\tLoss 1.6181 (1.5144)\tKL loss 1.599 (1.518)\tAcc@1 79.297 (81.197)\n",
      " * Acc@1 81.200\n",
      "Train: [24][10/98]\tBT 0.671 (0.680)\tDT 0.254 (0.265)\tloss 0.159 (0.219)\tKL loss 0.164 (0.231)\tAcc@1 95.312 (94.492)\n",
      "Train: [24][20/98]\tBT 0.676 (0.680)\tDT 0.262 (0.267)\tloss 0.222 (0.209)\tKL loss 0.256 (0.225)\tAcc@1 92.383 (94.463)\n",
      "Train: [24][30/98]\tBT 0.689 (0.681)\tDT 0.268 (0.267)\tloss 0.267 (0.209)\tKL loss 0.278 (0.223)\tAcc@1 93.750 (94.388)\n",
      "Train: [24][40/98]\tBT 0.673 (0.682)\tDT 0.258 (0.268)\tloss 0.185 (0.211)\tKL loss 0.210 (0.226)\tAcc@1 94.922 (94.268)\n",
      "Train: [24][50/98]\tBT 0.679 (0.683)\tDT 0.266 (0.269)\tloss 0.215 (0.211)\tKL loss 0.222 (0.225)\tAcc@1 93.359 (94.258)\n",
      "Train: [24][60/98]\tBT 0.685 (0.683)\tDT 0.271 (0.269)\tloss 0.237 (0.209)\tKL loss 0.252 (0.223)\tAcc@1 93.750 (94.339)\n",
      "Train: [24][70/98]\tBT 0.694 (0.687)\tDT 0.279 (0.272)\tloss 0.193 (0.207)\tKL loss 0.224 (0.220)\tAcc@1 95.117 (94.364)\n",
      "Train: [24][80/98]\tBT 0.713 (0.688)\tDT 0.297 (0.273)\tloss 0.241 (0.207)\tKL loss 0.254 (0.219)\tAcc@1 94.531 (94.456)\n",
      "Train: [24][90/98]\tBT 0.679 (0.689)\tDT 0.264 (0.274)\tloss 0.228 (0.209)\tKL loss 0.243 (0.222)\tAcc@1 95.117 (94.455)\n",
      "Train epoch 24, total time 67.50, accuracy:94.47\n",
      "Test: [0/20]\tTime 0.297 (0.297)\tLoss 1.5383 (1.5383)\tKL loss 1.522 (1.522)\tAcc@1 80.078 (80.078)\n",
      "Test: [10/20]\tTime 0.289 (0.293)\tLoss 1.5416 (1.5487)\tKL loss 1.535 (1.550)\tAcc@1 79.688 (80.540)\n",
      " * Acc@1 80.760\n",
      "Train: [25][10/98]\tBT 0.671 (0.682)\tDT 0.258 (0.269)\tloss 0.243 (0.221)\tKL loss 0.265 (0.232)\tAcc@1 94.727 (93.809)\n",
      "Train: [25][20/98]\tBT 0.681 (0.682)\tDT 0.265 (0.269)\tloss 0.207 (0.210)\tKL loss 0.215 (0.221)\tAcc@1 94.727 (94.326)\n",
      "Train: [25][30/98]\tBT 0.680 (0.682)\tDT 0.268 (0.268)\tloss 0.116 (0.207)\tKL loss 0.142 (0.218)\tAcc@1 96.484 (94.518)\n",
      "Train: [25][40/98]\tBT 0.691 (0.684)\tDT 0.275 (0.270)\tloss 0.204 (0.209)\tKL loss 0.245 (0.222)\tAcc@1 94.531 (94.541)\n",
      "Train: [25][50/98]\tBT 0.716 (0.688)\tDT 0.301 (0.273)\tloss 0.244 (0.215)\tKL loss 0.254 (0.228)\tAcc@1 93.945 (94.484)\n",
      "Train: [25][60/98]\tBT 0.672 (0.687)\tDT 0.259 (0.272)\tloss 0.208 (0.214)\tKL loss 0.226 (0.226)\tAcc@1 94.336 (94.443)\n",
      "Train: [25][70/98]\tBT 0.673 (0.686)\tDT 0.261 (0.271)\tloss 0.146 (0.210)\tKL loss 0.194 (0.223)\tAcc@1 94.922 (94.509)\n",
      "Train: [25][80/98]\tBT 0.679 (0.686)\tDT 0.264 (0.271)\tloss 0.170 (0.210)\tKL loss 0.174 (0.222)\tAcc@1 94.727 (94.512)\n",
      "Train: [25][90/98]\tBT 0.676 (0.686)\tDT 0.261 (0.271)\tloss 0.196 (0.216)\tKL loss 0.213 (0.228)\tAcc@1 93.945 (94.427)\n",
      "Train epoch 25, total time 67.27, accuracy:94.46\n",
      "Test: [0/20]\tTime 0.284 (0.284)\tLoss 1.5011 (1.5011)\tKL loss 1.472 (1.472)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.286 (0.289)\tLoss 1.4274 (1.4906)\tKL loss 1.408 (1.492)\tAcc@1 79.883 (80.895)\n",
      " * Acc@1 80.880\n",
      "Train: [26][10/98]\tBT 0.677 (0.682)\tDT 0.265 (0.270)\tloss 0.244 (0.212)\tKL loss 0.238 (0.222)\tAcc@1 93.750 (94.121)\n",
      "Train: [26][20/98]\tBT 0.680 (0.684)\tDT 0.266 (0.271)\tloss 0.235 (0.210)\tKL loss 0.234 (0.219)\tAcc@1 95.508 (94.434)\n",
      "Train: [26][30/98]\tBT 0.854 (0.688)\tDT 0.427 (0.275)\tloss 0.181 (0.211)\tKL loss 0.185 (0.222)\tAcc@1 95.508 (94.382)\n",
      "Train: [26][40/98]\tBT 0.679 (0.687)\tDT 0.266 (0.273)\tloss 0.265 (0.218)\tKL loss 0.264 (0.229)\tAcc@1 93.945 (94.282)\n",
      "Train: [26][50/98]\tBT 0.680 (0.685)\tDT 0.265 (0.272)\tloss 0.274 (0.224)\tKL loss 0.321 (0.237)\tAcc@1 92.969 (94.168)\n",
      "Train: [26][60/98]\tBT 0.692 (0.685)\tDT 0.277 (0.272)\tloss 0.177 (0.222)\tKL loss 0.196 (0.235)\tAcc@1 94.922 (94.183)\n",
      "Train: [26][70/98]\tBT 0.692 (0.685)\tDT 0.275 (0.272)\tloss 0.166 (0.220)\tKL loss 0.177 (0.233)\tAcc@1 95.508 (94.258)\n",
      "Train: [26][80/98]\tBT 0.683 (0.685)\tDT 0.270 (0.271)\tloss 0.202 (0.220)\tKL loss 0.206 (0.233)\tAcc@1 95.898 (94.292)\n",
      "Train: [26][90/98]\tBT 0.676 (0.684)\tDT 0.262 (0.270)\tloss 0.237 (0.218)\tKL loss 0.249 (0.232)\tAcc@1 93.555 (94.314)\n",
      "Train epoch 26, total time 67.00, accuracy:94.35\n",
      "Test: [0/20]\tTime 0.291 (0.291)\tLoss 1.4800 (1.4800)\tKL loss 1.463 (1.463)\tAcc@1 79.102 (79.102)\n",
      "Test: [10/20]\tTime 0.309 (0.293)\tLoss 1.5129 (1.4778)\tKL loss 1.506 (1.481)\tAcc@1 79.883 (81.339)\n",
      " * Acc@1 81.500\n",
      "Train: [27][10/98]\tBT 0.683 (0.679)\tDT 0.270 (0.265)\tloss 0.201 (0.212)\tKL loss 0.225 (0.223)\tAcc@1 94.922 (94.609)\n",
      "Train: [27][20/98]\tBT 0.678 (0.689)\tDT 0.265 (0.275)\tloss 0.259 (0.195)\tKL loss 0.270 (0.209)\tAcc@1 93.750 (94.785)\n",
      "Train: [27][30/98]\tBT 0.666 (0.686)\tDT 0.252 (0.272)\tloss 0.228 (0.206)\tKL loss 0.243 (0.218)\tAcc@1 93.750 (94.473)\n",
      "Train: [27][40/98]\tBT 0.680 (0.685)\tDT 0.267 (0.271)\tloss 0.188 (0.201)\tKL loss 0.200 (0.216)\tAcc@1 94.922 (94.590)\n",
      "Train: [27][50/98]\tBT 0.685 (0.685)\tDT 0.271 (0.271)\tloss 0.214 (0.200)\tKL loss 0.234 (0.214)\tAcc@1 94.727 (94.602)\n",
      "Train: [27][60/98]\tBT 0.676 (0.684)\tDT 0.262 (0.270)\tloss 0.134 (0.199)\tKL loss 0.154 (0.213)\tAcc@1 96.484 (94.635)\n",
      "Train: [27][70/98]\tBT 0.673 (0.684)\tDT 0.259 (0.269)\tloss 0.159 (0.204)\tKL loss 0.179 (0.217)\tAcc@1 94.922 (94.545)\n",
      "Train: [27][80/98]\tBT 0.689 (0.683)\tDT 0.272 (0.269)\tloss 0.211 (0.209)\tKL loss 0.228 (0.222)\tAcc@1 95.117 (94.497)\n",
      "Train: [27][90/98]\tBT 0.674 (0.683)\tDT 0.264 (0.269)\tloss 0.159 (0.208)\tKL loss 0.169 (0.221)\tAcc@1 94.531 (94.512)\n",
      "Train epoch 27, total time 66.91, accuracy:94.50\n",
      "Test: [0/20]\tTime 0.458 (0.458)\tLoss 1.5505 (1.5505)\tKL loss 1.529 (1.529)\tAcc@1 80.078 (80.078)\n",
      "Test: [10/20]\tTime 0.289 (0.320)\tLoss 1.5282 (1.4734)\tKL loss 1.514 (1.476)\tAcc@1 79.883 (81.250)\n",
      " * Acc@1 81.160\n",
      "Train: [28][10/98]\tBT 0.683 (0.683)\tDT 0.273 (0.270)\tloss 0.236 (0.191)\tKL loss 0.229 (0.205)\tAcc@1 94.141 (94.746)\n",
      "Train: [28][20/98]\tBT 0.684 (0.681)\tDT 0.273 (0.269)\tloss 0.221 (0.195)\tKL loss 0.221 (0.212)\tAcc@1 94.141 (94.570)\n",
      "Train: [28][30/98]\tBT 0.703 (0.683)\tDT 0.288 (0.271)\tloss 0.279 (0.201)\tKL loss 0.295 (0.216)\tAcc@1 93.359 (94.518)\n",
      "Train: [28][40/98]\tBT 0.672 (0.682)\tDT 0.258 (0.269)\tloss 0.203 (0.207)\tKL loss 0.207 (0.222)\tAcc@1 94.336 (94.473)\n",
      "Train: [28][50/98]\tBT 0.677 (0.683)\tDT 0.265 (0.270)\tloss 0.248 (0.209)\tKL loss 0.254 (0.223)\tAcc@1 95.117 (94.516)\n",
      "Train: [28][60/98]\tBT 0.685 (0.683)\tDT 0.267 (0.270)\tloss 0.201 (0.207)\tKL loss 0.209 (0.222)\tAcc@1 96.094 (94.593)\n",
      "Train: [28][70/98]\tBT 0.683 (0.683)\tDT 0.270 (0.269)\tloss 0.238 (0.210)\tKL loss 0.229 (0.224)\tAcc@1 94.336 (94.528)\n",
      "Train: [28][80/98]\tBT 0.908 (0.685)\tDT 0.464 (0.271)\tloss 0.243 (0.214)\tKL loss 0.237 (0.227)\tAcc@1 93.164 (94.478)\n",
      "Train: [28][90/98]\tBT 0.685 (0.685)\tDT 0.270 (0.271)\tloss 0.173 (0.212)\tKL loss 0.186 (0.225)\tAcc@1 94.531 (94.488)\n",
      "Train epoch 28, total time 67.14, accuracy:94.50\n",
      "Test: [0/20]\tTime 0.410 (0.410)\tLoss 1.5878 (1.5878)\tKL loss 1.566 (1.566)\tAcc@1 80.664 (80.664)\n",
      "Test: [10/20]\tTime 0.291 (0.305)\tLoss 1.6112 (1.5082)\tKL loss 1.589 (1.508)\tAcc@1 78.516 (80.948)\n",
      " * Acc@1 81.100\n",
      "Train: [29][10/98]\tBT 0.675 (0.684)\tDT 0.263 (0.270)\tloss 0.118 (0.172)\tKL loss 0.125 (0.188)\tAcc@1 95.508 (94.980)\n",
      "Train: [29][20/98]\tBT 0.679 (0.683)\tDT 0.264 (0.269)\tloss 0.175 (0.204)\tKL loss 0.180 (0.214)\tAcc@1 94.336 (94.561)\n",
      "Train: [29][30/98]\tBT 0.686 (0.683)\tDT 0.270 (0.269)\tloss 0.167 (0.219)\tKL loss 0.174 (0.229)\tAcc@1 93.555 (94.329)\n",
      "Train: [29][40/98]\tBT 0.685 (0.683)\tDT 0.272 (0.269)\tloss 0.195 (0.216)\tKL loss 0.197 (0.226)\tAcc@1 93.945 (94.287)\n",
      "Train: [29][50/98]\tBT 0.708 (0.683)\tDT 0.293 (0.269)\tloss 0.238 (0.210)\tKL loss 0.287 (0.223)\tAcc@1 94.141 (94.430)\n",
      "Train: [29][60/98]\tBT 0.682 (0.683)\tDT 0.270 (0.269)\tloss 0.193 (0.210)\tKL loss 0.224 (0.223)\tAcc@1 93.359 (94.391)\n",
      "Train: [29][70/98]\tBT 0.671 (0.686)\tDT 0.259 (0.272)\tloss 0.245 (0.207)\tKL loss 0.245 (0.219)\tAcc@1 93.555 (94.448)\n",
      "Train: [29][80/98]\tBT 0.682 (0.685)\tDT 0.267 (0.271)\tloss 0.294 (0.207)\tKL loss 0.286 (0.219)\tAcc@1 92.188 (94.426)\n",
      "Train: [29][90/98]\tBT 0.682 (0.685)\tDT 0.269 (0.271)\tloss 0.307 (0.210)\tKL loss 0.347 (0.222)\tAcc@1 92.773 (94.397)\n",
      "Train epoch 29, total time 67.11, accuracy:94.43\n",
      "Test: [0/20]\tTime 0.284 (0.284)\tLoss 1.5658 (1.5658)\tKL loss 1.545 (1.545)\tAcc@1 78.516 (78.516)\n",
      "Test: [10/20]\tTime 0.291 (0.288)\tLoss 1.5491 (1.4805)\tKL loss 1.525 (1.478)\tAcc@1 78.516 (80.895)\n",
      " * Acc@1 80.660\n",
      "Train: [30][10/98]\tBT 0.676 (0.679)\tDT 0.264 (0.267)\tloss 0.224 (0.205)\tKL loss 0.228 (0.212)\tAcc@1 93.945 (94.766)\n",
      "Train: [30][20/98]\tBT 0.678 (0.678)\tDT 0.266 (0.266)\tloss 0.172 (0.209)\tKL loss 0.184 (0.218)\tAcc@1 94.727 (94.678)\n",
      "Train: [30][30/98]\tBT 0.686 (0.678)\tDT 0.273 (0.267)\tloss 0.162 (0.206)\tKL loss 0.190 (0.217)\tAcc@1 96.094 (94.727)\n",
      "Train: [30][40/98]\tBT 0.679 (0.681)\tDT 0.267 (0.269)\tloss 0.295 (0.214)\tKL loss 0.335 (0.227)\tAcc@1 94.727 (94.590)\n",
      "Train: [30][50/98]\tBT 0.673 (0.685)\tDT 0.262 (0.272)\tloss 0.268 (0.217)\tKL loss 0.267 (0.230)\tAcc@1 93.750 (94.578)\n",
      "Train: [30][60/98]\tBT 0.675 (0.684)\tDT 0.263 (0.271)\tloss 0.292 (0.221)\tKL loss 0.301 (0.232)\tAcc@1 93.164 (94.499)\n",
      "Train: [30][70/98]\tBT 0.684 (0.684)\tDT 0.271 (0.271)\tloss 0.219 (0.225)\tKL loss 0.227 (0.237)\tAcc@1 94.141 (94.478)\n",
      "Train: [30][80/98]\tBT 0.672 (0.684)\tDT 0.260 (0.270)\tloss 0.264 (0.226)\tKL loss 0.283 (0.237)\tAcc@1 93.750 (94.446)\n",
      "Train: [30][90/98]\tBT 0.668 (0.683)\tDT 0.258 (0.270)\tloss 0.316 (0.229)\tKL loss 0.308 (0.241)\tAcc@1 94.531 (94.438)\n",
      "Train epoch 30, total time 66.95, accuracy:94.42\n",
      "Test: [0/20]\tTime 0.292 (0.292)\tLoss 1.5699 (1.5699)\tKL loss 1.573 (1.573)\tAcc@1 81.445 (81.445)\n",
      "Test: [10/20]\tTime 0.293 (0.292)\tLoss 1.6206 (1.5093)\tKL loss 1.585 (1.505)\tAcc@1 78.711 (80.824)\n",
      " * Acc@1 80.670\n",
      "Train: [31][10/98]\tBT 0.671 (0.685)\tDT 0.257 (0.273)\tloss 0.221 (0.219)\tKL loss 0.225 (0.231)\tAcc@1 93.750 (94.668)\n",
      "Train: [31][20/98]\tBT 0.679 (0.684)\tDT 0.266 (0.271)\tloss 0.168 (0.211)\tKL loss 0.185 (0.222)\tAcc@1 95.312 (94.629)\n",
      "Train: [31][30/98]\tBT 0.669 (0.688)\tDT 0.256 (0.275)\tloss 0.210 (0.204)\tKL loss 0.238 (0.217)\tAcc@1 94.531 (94.759)\n",
      "Train: [31][40/98]\tBT 0.684 (0.687)\tDT 0.269 (0.273)\tloss 0.190 (0.202)\tKL loss 0.206 (0.215)\tAcc@1 94.727 (94.751)\n",
      "Train: [31][50/98]\tBT 0.682 (0.685)\tDT 0.269 (0.272)\tloss 0.168 (0.204)\tKL loss 0.200 (0.218)\tAcc@1 95.508 (94.727)\n",
      "Train: [31][60/98]\tBT 0.671 (0.685)\tDT 0.261 (0.272)\tloss 0.275 (0.204)\tKL loss 0.281 (0.217)\tAcc@1 94.336 (94.730)\n",
      "Train: [31][70/98]\tBT 0.683 (0.684)\tDT 0.271 (0.271)\tloss 0.240 (0.202)\tKL loss 0.248 (0.215)\tAcc@1 93.359 (94.774)\n",
      "Train: [31][80/98]\tBT 0.691 (0.684)\tDT 0.278 (0.271)\tloss 0.190 (0.206)\tKL loss 0.186 (0.218)\tAcc@1 94.141 (94.692)\n",
      "Train: [31][90/98]\tBT 0.676 (0.684)\tDT 0.265 (0.271)\tloss 0.207 (0.207)\tKL loss 0.217 (0.219)\tAcc@1 94.336 (94.625)\n",
      "Train epoch 31, total time 67.00, accuracy:94.63\n",
      "Test: [0/20]\tTime 0.285 (0.285)\tLoss 1.4393 (1.4393)\tKL loss 1.429 (1.429)\tAcc@1 80.664 (80.664)\n",
      "Test: [10/20]\tTime 0.292 (0.291)\tLoss 1.4185 (1.4572)\tKL loss 1.393 (1.456)\tAcc@1 79.102 (80.415)\n",
      " * Acc@1 80.750\n",
      "Train: [32][10/98]\tBT 0.674 (0.701)\tDT 0.265 (0.288)\tloss 0.237 (0.220)\tKL loss 0.241 (0.232)\tAcc@1 94.336 (94.199)\n",
      "Train: [32][20/98]\tBT 0.682 (0.693)\tDT 0.268 (0.279)\tloss 0.156 (0.216)\tKL loss 0.160 (0.230)\tAcc@1 94.727 (94.180)\n",
      "Train: [32][30/98]\tBT 0.687 (0.690)\tDT 0.272 (0.276)\tloss 0.199 (0.211)\tKL loss 0.213 (0.224)\tAcc@1 94.141 (94.206)\n",
      "Train: [32][40/98]\tBT 0.688 (0.687)\tDT 0.275 (0.273)\tloss 0.222 (0.210)\tKL loss 0.228 (0.223)\tAcc@1 94.531 (94.272)\n",
      "Train: [32][50/98]\tBT 0.688 (0.688)\tDT 0.270 (0.274)\tloss 0.214 (0.208)\tKL loss 0.226 (0.222)\tAcc@1 94.336 (94.332)\n",
      "Train: [32][60/98]\tBT 0.691 (0.687)\tDT 0.277 (0.273)\tloss 0.243 (0.212)\tKL loss 0.249 (0.225)\tAcc@1 94.336 (94.287)\n",
      "Train: [32][70/98]\tBT 0.674 (0.686)\tDT 0.263 (0.272)\tloss 0.143 (0.210)\tKL loss 0.154 (0.223)\tAcc@1 95.898 (94.353)\n",
      "Train: [32][80/98]\tBT 0.698 (0.687)\tDT 0.286 (0.273)\tloss 0.363 (0.209)\tKL loss 0.390 (0.222)\tAcc@1 91.992 (94.343)\n",
      "Train: [32][90/98]\tBT 0.673 (0.686)\tDT 0.259 (0.272)\tloss 0.170 (0.211)\tKL loss 0.172 (0.224)\tAcc@1 94.727 (94.312)\n",
      "Train epoch 32, total time 67.47, accuracy:94.26\n",
      "Test: [0/20]\tTime 0.287 (0.287)\tLoss 1.6553 (1.6553)\tKL loss 1.629 (1.629)\tAcc@1 79.297 (79.297)\n",
      "Test: [10/20]\tTime 0.287 (0.292)\tLoss 1.4734 (1.5353)\tKL loss 1.453 (1.534)\tAcc@1 78.711 (80.646)\n",
      " * Acc@1 80.270\n",
      "Train: [33][10/98]\tBT 0.683 (0.682)\tDT 0.271 (0.268)\tloss 0.211 (0.246)\tKL loss 0.217 (0.258)\tAcc@1 95.312 (94.023)\n",
      "Train: [33][20/98]\tBT 0.675 (0.680)\tDT 0.264 (0.268)\tloss 0.204 (0.242)\tKL loss 0.211 (0.255)\tAcc@1 95.508 (94.199)\n",
      "Train: [33][30/98]\tBT 0.700 (0.684)\tDT 0.288 (0.271)\tloss 0.153 (0.231)\tKL loss 0.190 (0.246)\tAcc@1 96.094 (94.382)\n",
      "Train: [33][40/98]\tBT 0.673 (0.685)\tDT 0.260 (0.271)\tloss 0.210 (0.226)\tKL loss 0.220 (0.241)\tAcc@1 94.531 (94.482)\n",
      "Train: [33][50/98]\tBT 0.686 (0.684)\tDT 0.268 (0.270)\tloss 0.275 (0.229)\tKL loss 0.284 (0.244)\tAcc@1 94.531 (94.438)\n",
      "Train: [33][60/98]\tBT 0.680 (0.683)\tDT 0.266 (0.269)\tloss 0.219 (0.226)\tKL loss 0.256 (0.241)\tAcc@1 94.727 (94.424)\n",
      "Train: [33][70/98]\tBT 0.680 (0.683)\tDT 0.270 (0.269)\tloss 0.315 (0.227)\tKL loss 0.339 (0.241)\tAcc@1 92.383 (94.408)\n",
      "Train: [33][80/98]\tBT 0.683 (0.687)\tDT 0.268 (0.273)\tloss 0.267 (0.226)\tKL loss 0.275 (0.240)\tAcc@1 93.359 (94.385)\n",
      "Train: [33][90/98]\tBT 0.668 (0.686)\tDT 0.255 (0.272)\tloss 0.212 (0.226)\tKL loss 0.219 (0.240)\tAcc@1 94.727 (94.329)\n",
      "Train epoch 33, total time 67.20, accuracy:94.33\n",
      "Test: [0/20]\tTime 0.287 (0.287)\tLoss 1.4263 (1.4263)\tKL loss 1.434 (1.434)\tAcc@1 80.469 (80.469)\n",
      "Test: [10/20]\tTime 0.283 (0.290)\tLoss 1.4827 (1.4905)\tKL loss 1.468 (1.489)\tAcc@1 79.688 (80.948)\n",
      " * Acc@1 80.960\n",
      "Train: [34][10/98]\tBT 0.681 (0.679)\tDT 0.268 (0.266)\tloss 0.291 (0.234)\tKL loss 0.319 (0.244)\tAcc@1 93.359 (94.043)\n",
      "Train: [34][20/98]\tBT 0.679 (0.679)\tDT 0.266 (0.266)\tloss 0.274 (0.228)\tKL loss 0.274 (0.238)\tAcc@1 94.922 (94.219)\n",
      "Train: [34][30/98]\tBT 0.673 (0.680)\tDT 0.258 (0.267)\tloss 0.213 (0.232)\tKL loss 0.206 (0.241)\tAcc@1 94.531 (94.199)\n",
      "Train: [34][40/98]\tBT 0.682 (0.681)\tDT 0.267 (0.267)\tloss 0.267 (0.240)\tKL loss 0.273 (0.249)\tAcc@1 93.359 (94.038)\n",
      "Train: [34][50/98]\tBT 0.670 (0.681)\tDT 0.258 (0.268)\tloss 0.257 (0.244)\tKL loss 0.266 (0.254)\tAcc@1 93.555 (93.926)\n",
      "Train: [34][60/98]\tBT 0.669 (0.686)\tDT 0.254 (0.271)\tloss 0.196 (0.238)\tKL loss 0.215 (0.248)\tAcc@1 93.945 (94.004)\n",
      "Train: [34][70/98]\tBT 0.681 (0.685)\tDT 0.269 (0.271)\tloss 0.256 (0.236)\tKL loss 0.272 (0.246)\tAcc@1 94.922 (94.043)\n",
      "Train: [34][80/98]\tBT 0.677 (0.685)\tDT 0.265 (0.271)\tloss 0.245 (0.232)\tKL loss 0.254 (0.242)\tAcc@1 92.969 (94.131)\n",
      "Train: [34][90/98]\tBT 0.682 (0.685)\tDT 0.265 (0.271)\tloss 0.177 (0.228)\tKL loss 0.193 (0.238)\tAcc@1 95.898 (94.212)\n",
      "Train epoch 34, total time 67.11, accuracy:94.22\n",
      "Test: [0/20]\tTime 0.297 (0.297)\tLoss 1.5362 (1.5362)\tKL loss 1.509 (1.509)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.289 (0.290)\tLoss 1.4924 (1.4670)\tKL loss 1.464 (1.465)\tAcc@1 78.516 (81.126)\n",
      " * Acc@1 80.870\n",
      "Train: [35][10/98]\tBT 0.677 (0.672)\tDT 0.266 (0.261)\tloss 0.184 (0.230)\tKL loss 0.187 (0.237)\tAcc@1 96.289 (94.609)\n",
      "Train: [35][20/98]\tBT 0.663 (0.675)\tDT 0.251 (0.263)\tloss 0.126 (0.219)\tKL loss 0.167 (0.233)\tAcc@1 96.289 (94.492)\n",
      "Train: [35][30/98]\tBT 0.667 (0.676)\tDT 0.256 (0.263)\tloss 0.183 (0.225)\tKL loss 0.211 (0.238)\tAcc@1 93.555 (94.271)\n",
      "Train: [35][40/98]\tBT 0.692 (0.684)\tDT 0.274 (0.270)\tloss 0.452 (0.226)\tKL loss 0.480 (0.240)\tAcc@1 93.359 (94.199)\n",
      "Train: [35][50/98]\tBT 0.672 (0.682)\tDT 0.258 (0.269)\tloss 0.186 (0.234)\tKL loss 0.204 (0.247)\tAcc@1 94.727 (94.188)\n",
      "Train: [35][60/98]\tBT 0.667 (0.680)\tDT 0.254 (0.267)\tloss 0.161 (0.231)\tKL loss 0.155 (0.242)\tAcc@1 95.312 (94.176)\n",
      "Train: [35][70/98]\tBT 0.672 (0.679)\tDT 0.261 (0.266)\tloss 0.213 (0.227)\tKL loss 0.223 (0.238)\tAcc@1 93.555 (94.205)\n",
      "Train: [35][80/98]\tBT 0.681 (0.678)\tDT 0.269 (0.265)\tloss 0.286 (0.228)\tKL loss 0.288 (0.238)\tAcc@1 93.945 (94.304)\n",
      "Train: [35][90/98]\tBT 0.671 (0.678)\tDT 0.257 (0.265)\tloss 0.219 (0.225)\tKL loss 0.222 (0.235)\tAcc@1 94.922 (94.321)\n",
      "Train epoch 35, total time 66.41, accuracy:94.32\n",
      "Test: [0/20]\tTime 0.287 (0.287)\tLoss 1.4146 (1.4146)\tKL loss 1.399 (1.399)\tAcc@1 80.664 (80.664)\n",
      "Test: [10/20]\tTime 0.290 (0.285)\tLoss 1.5359 (1.4334)\tKL loss 1.517 (1.428)\tAcc@1 79.297 (81.055)\n",
      " * Acc@1 81.130\n",
      "Train: [36][10/98]\tBT 0.683 (0.674)\tDT 0.272 (0.261)\tloss 0.244 (0.189)\tKL loss 0.259 (0.199)\tAcc@1 93.750 (94.961)\n",
      "Train: [36][20/98]\tBT 0.871 (0.686)\tDT 0.432 (0.272)\tloss 0.245 (0.205)\tKL loss 0.249 (0.215)\tAcc@1 94.727 (94.609)\n",
      "Train: [36][30/98]\tBT 0.678 (0.684)\tDT 0.263 (0.270)\tloss 0.146 (0.205)\tKL loss 0.158 (0.214)\tAcc@1 94.922 (94.564)\n",
      "Train: [36][40/98]\tBT 0.680 (0.681)\tDT 0.265 (0.267)\tloss 0.208 (0.198)\tKL loss 0.221 (0.208)\tAcc@1 93.750 (94.536)\n",
      "Train: [36][50/98]\tBT 0.663 (0.679)\tDT 0.248 (0.265)\tloss 0.226 (0.202)\tKL loss 0.220 (0.214)\tAcc@1 95.508 (94.488)\n",
      "Train: [36][60/98]\tBT 0.661 (0.678)\tDT 0.251 (0.265)\tloss 0.189 (0.200)\tKL loss 0.206 (0.212)\tAcc@1 93.945 (94.502)\n",
      "Train: [36][70/98]\tBT 0.695 (0.678)\tDT 0.277 (0.264)\tloss 0.214 (0.205)\tKL loss 0.218 (0.216)\tAcc@1 95.508 (94.487)\n",
      "Train: [36][80/98]\tBT 0.670 (0.677)\tDT 0.259 (0.264)\tloss 0.212 (0.207)\tKL loss 0.217 (0.219)\tAcc@1 95.508 (94.448)\n",
      "Train: [36][90/98]\tBT 0.670 (0.677)\tDT 0.257 (0.263)\tloss 0.314 (0.209)\tKL loss 0.319 (0.221)\tAcc@1 92.969 (94.431)\n",
      "Train epoch 36, total time 66.26, accuracy:94.37\n",
      "Test: [0/20]\tTime 0.285 (0.285)\tLoss 1.5679 (1.5679)\tKL loss 1.540 (1.540)\tAcc@1 80.078 (80.078)\n",
      "Test: [10/20]\tTime 0.289 (0.287)\tLoss 1.6333 (1.5179)\tKL loss 1.587 (1.508)\tAcc@1 78.906 (80.415)\n",
      " * Acc@1 80.630\n",
      "Train: [37][10/98]\tBT 0.669 (0.694)\tDT 0.255 (0.281)\tloss 0.150 (0.259)\tKL loss 0.159 (0.269)\tAcc@1 96.094 (94.023)\n",
      "Train: [37][20/98]\tBT 0.670 (0.686)\tDT 0.256 (0.272)\tloss 0.201 (0.231)\tKL loss 0.227 (0.244)\tAcc@1 95.703 (94.385)\n",
      "Train: [37][30/98]\tBT 0.675 (0.682)\tDT 0.259 (0.268)\tloss 0.278 (0.221)\tKL loss 0.275 (0.233)\tAcc@1 93.359 (94.538)\n",
      "Train: [37][40/98]\tBT 0.680 (0.680)\tDT 0.262 (0.266)\tloss 0.227 (0.220)\tKL loss 0.229 (0.231)\tAcc@1 94.727 (94.600)\n",
      "Train: [37][50/98]\tBT 0.677 (0.679)\tDT 0.262 (0.265)\tloss 0.184 (0.219)\tKL loss 0.187 (0.229)\tAcc@1 94.141 (94.621)\n",
      "Train: [37][60/98]\tBT 0.674 (0.679)\tDT 0.260 (0.265)\tloss 0.163 (0.216)\tKL loss 0.172 (0.227)\tAcc@1 95.508 (94.603)\n",
      "Train: [37][70/98]\tBT 0.666 (0.678)\tDT 0.253 (0.264)\tloss 0.193 (0.215)\tKL loss 0.200 (0.227)\tAcc@1 95.508 (94.612)\n",
      "Train: [37][80/98]\tBT 0.666 (0.679)\tDT 0.255 (0.265)\tloss 0.230 (0.217)\tKL loss 0.247 (0.228)\tAcc@1 92.969 (94.490)\n",
      "Train: [37][90/98]\tBT 0.682 (0.678)\tDT 0.253 (0.263)\tloss 0.178 (0.215)\tKL loss 0.188 (0.226)\tAcc@1 95.117 (94.497)\n",
      "Train epoch 37, total time 66.79, accuracy:94.49\n",
      "Test: [0/20]\tTime 0.281 (0.281)\tLoss 1.5266 (1.5266)\tKL loss 1.509 (1.509)\tAcc@1 79.492 (79.492)\n",
      "Test: [10/20]\tTime 0.287 (0.285)\tLoss 1.5880 (1.4670)\tKL loss 1.561 (1.460)\tAcc@1 78.711 (80.753)\n",
      " * Acc@1 80.510\n",
      "Train: [38][10/98]\tBT 0.677 (0.678)\tDT 0.266 (0.265)\tloss 0.185 (0.205)\tKL loss 0.221 (0.217)\tAcc@1 95.898 (94.707)\n",
      "Train: [38][20/98]\tBT 0.670 (0.675)\tDT 0.256 (0.261)\tloss 0.242 (0.202)\tKL loss 0.255 (0.214)\tAcc@1 93.555 (94.619)\n",
      "Train: [38][30/98]\tBT 0.678 (0.674)\tDT 0.266 (0.261)\tloss 0.216 (0.195)\tKL loss 0.220 (0.205)\tAcc@1 94.336 (94.714)\n",
      "Train: [38][40/98]\tBT 0.675 (0.674)\tDT 0.264 (0.260)\tloss 0.207 (0.196)\tKL loss 0.202 (0.205)\tAcc@1 95.117 (94.697)\n",
      "Train: [38][50/98]\tBT 0.676 (0.674)\tDT 0.262 (0.260)\tloss 0.161 (0.204)\tKL loss 0.168 (0.213)\tAcc@1 95.703 (94.629)\n",
      "Train: [38][60/98]\tBT 0.674 (0.674)\tDT 0.262 (0.260)\tloss 0.127 (0.202)\tKL loss 0.151 (0.212)\tAcc@1 95.703 (94.648)\n",
      "Train: [38][70/98]\tBT 0.663 (0.674)\tDT 0.251 (0.260)\tloss 0.236 (0.208)\tKL loss 0.245 (0.218)\tAcc@1 93.750 (94.526)\n",
      "Train: [38][80/98]\tBT 0.675 (0.677)\tDT 0.262 (0.263)\tloss 0.258 (0.210)\tKL loss 0.245 (0.220)\tAcc@1 94.727 (94.458)\n",
      "Train: [38][90/98]\tBT 0.662 (0.676)\tDT 0.249 (0.263)\tloss 0.155 (0.217)\tKL loss 0.168 (0.226)\tAcc@1 95.703 (94.431)\n",
      "Train epoch 38, total time 66.25, accuracy:94.41\n",
      "Test: [0/20]\tTime 0.280 (0.280)\tLoss 1.6097 (1.6097)\tKL loss 1.635 (1.635)\tAcc@1 81.445 (81.445)\n",
      "Test: [10/20]\tTime 0.284 (0.283)\tLoss 1.6881 (1.5669)\tKL loss 1.667 (1.562)\tAcc@1 79.102 (80.629)\n",
      " * Acc@1 80.700\n",
      "Train: [39][10/98]\tBT 0.669 (0.670)\tDT 0.259 (0.257)\tloss 0.336 (0.261)\tKL loss 0.344 (0.274)\tAcc@1 93.555 (93.770)\n",
      "Train: [39][20/98]\tBT 0.668 (0.671)\tDT 0.256 (0.257)\tloss 0.254 (0.259)\tKL loss 0.257 (0.269)\tAcc@1 93.164 (93.848)\n",
      "Train: [39][30/98]\tBT 0.668 (0.670)\tDT 0.255 (0.257)\tloss 0.259 (0.254)\tKL loss 0.257 (0.263)\tAcc@1 93.945 (93.893)\n",
      "Train: [39][40/98]\tBT 0.668 (0.671)\tDT 0.255 (0.257)\tloss 0.161 (0.243)\tKL loss 0.165 (0.254)\tAcc@1 95.703 (94.141)\n",
      "Train: [39][50/98]\tBT 0.657 (0.671)\tDT 0.246 (0.257)\tloss 0.126 (0.232)\tKL loss 0.133 (0.243)\tAcc@1 95.312 (94.301)\n",
      "Train: [39][60/98]\tBT 0.663 (0.676)\tDT 0.252 (0.262)\tloss 0.264 (0.231)\tKL loss 0.287 (0.243)\tAcc@1 93.359 (94.235)\n",
      "Train: [39][70/98]\tBT 0.678 (0.676)\tDT 0.266 (0.263)\tloss 0.126 (0.226)\tKL loss 0.142 (0.237)\tAcc@1 96.289 (94.294)\n",
      "Train: [39][80/98]\tBT 0.676 (0.676)\tDT 0.263 (0.263)\tloss 0.195 (0.227)\tKL loss 0.199 (0.238)\tAcc@1 94.531 (94.307)\n",
      "Train: [39][90/98]\tBT 0.669 (0.676)\tDT 0.255 (0.263)\tloss 0.147 (0.226)\tKL loss 0.160 (0.237)\tAcc@1 96.094 (94.340)\n",
      "Train epoch 39, total time 66.25, accuracy:94.31\n",
      "Test: [0/20]\tTime 0.279 (0.279)\tLoss 1.3940 (1.3940)\tKL loss 1.389 (1.389)\tAcc@1 80.664 (80.664)\n",
      "Test: [10/20]\tTime 0.275 (0.283)\tLoss 1.5527 (1.4550)\tKL loss 1.532 (1.450)\tAcc@1 78.516 (81.108)\n",
      " * Acc@1 80.990\n",
      "Train: [40][10/98]\tBT 0.672 (0.677)\tDT 0.258 (0.261)\tloss 0.203 (0.193)\tKL loss 0.226 (0.204)\tAcc@1 94.141 (94.727)\n",
      "Train: [40][20/98]\tBT 0.668 (0.674)\tDT 0.256 (0.260)\tloss 0.180 (0.193)\tKL loss 0.191 (0.205)\tAcc@1 94.727 (94.629)\n",
      "Train: [40][30/98]\tBT 0.663 (0.673)\tDT 0.252 (0.259)\tloss 0.196 (0.190)\tKL loss 0.230 (0.204)\tAcc@1 94.336 (94.668)\n",
      "Train: [40][40/98]\tBT 0.670 (0.676)\tDT 0.254 (0.262)\tloss 0.276 (0.196)\tKL loss 0.271 (0.209)\tAcc@1 93.750 (94.487)\n",
      "Train: [40][50/98]\tBT 0.665 (0.676)\tDT 0.250 (0.262)\tloss 0.161 (0.198)\tKL loss 0.161 (0.210)\tAcc@1 95.898 (94.516)\n",
      "Train: [40][60/98]\tBT 0.680 (0.675)\tDT 0.266 (0.261)\tloss 0.216 (0.198)\tKL loss 0.224 (0.209)\tAcc@1 94.727 (94.508)\n",
      "Train: [40][70/98]\tBT 0.666 (0.674)\tDT 0.251 (0.260)\tloss 0.180 (0.203)\tKL loss 0.183 (0.214)\tAcc@1 94.336 (94.445)\n",
      "Train: [40][80/98]\tBT 0.666 (0.674)\tDT 0.254 (0.260)\tloss 0.183 (0.205)\tKL loss 0.193 (0.215)\tAcc@1 94.336 (94.468)\n",
      "Train: [40][90/98]\tBT 0.678 (0.674)\tDT 0.263 (0.260)\tloss 0.295 (0.205)\tKL loss 0.310 (0.216)\tAcc@1 93.164 (94.501)\n",
      "Train epoch 40, total time 66.03, accuracy:94.54\n",
      "Test: [0/20]\tTime 0.288 (0.288)\tLoss 1.4358 (1.4358)\tKL loss 1.418 (1.418)\tAcc@1 80.078 (80.078)\n",
      "Test: [10/20]\tTime 0.390 (0.295)\tLoss 1.3966 (1.4096)\tKL loss 1.393 (1.407)\tAcc@1 79.102 (81.179)\n",
      " * Acc@1 80.860\n",
      "Train: [41][10/98]\tBT 0.673 (0.668)\tDT 0.260 (0.255)\tloss 0.267 (0.225)\tKL loss 0.263 (0.240)\tAcc@1 92.383 (94.102)\n",
      "Train: [41][20/98]\tBT 0.663 (0.670)\tDT 0.252 (0.257)\tloss 0.286 (0.221)\tKL loss 0.294 (0.233)\tAcc@1 92.578 (94.102)\n",
      "Train: [41][30/98]\tBT 0.696 (0.678)\tDT 0.280 (0.264)\tloss 0.203 (0.218)\tKL loss 0.207 (0.231)\tAcc@1 93.945 (94.219)\n",
      "Train: [41][40/98]\tBT 0.667 (0.677)\tDT 0.252 (0.263)\tloss 0.265 (0.221)\tKL loss 0.273 (0.233)\tAcc@1 93.945 (94.292)\n",
      "Train: [41][50/98]\tBT 0.663 (0.677)\tDT 0.253 (0.263)\tloss 0.231 (0.219)\tKL loss 0.248 (0.231)\tAcc@1 93.750 (94.277)\n",
      "Train: [41][60/98]\tBT 0.671 (0.677)\tDT 0.260 (0.263)\tloss 0.249 (0.218)\tKL loss 0.265 (0.230)\tAcc@1 93.945 (94.316)\n",
      "Train: [41][70/98]\tBT 0.668 (0.676)\tDT 0.253 (0.262)\tloss 0.261 (0.217)\tKL loss 0.259 (0.228)\tAcc@1 93.945 (94.397)\n",
      "Train: [41][80/98]\tBT 0.657 (0.676)\tDT 0.246 (0.262)\tloss 0.182 (0.213)\tKL loss 0.185 (0.224)\tAcc@1 93.750 (94.421)\n",
      "Train: [41][90/98]\tBT 0.667 (0.676)\tDT 0.254 (0.262)\tloss 0.402 (0.214)\tKL loss 0.366 (0.224)\tAcc@1 93.555 (94.429)\n",
      "Train epoch 41, total time 66.19, accuracy:94.42\n",
      "Test: [0/20]\tTime 0.289 (0.289)\tLoss 1.4793 (1.4793)\tKL loss 1.476 (1.476)\tAcc@1 80.469 (80.469)\n",
      "Test: [10/20]\tTime 0.279 (0.286)\tLoss 1.6530 (1.4995)\tKL loss 1.612 (1.492)\tAcc@1 77.734 (79.847)\n",
      " * Acc@1 79.770\n",
      "Train: [42][10/98]\tBT 0.682 (0.698)\tDT 0.269 (0.284)\tloss 0.193 (0.226)\tKL loss 0.196 (0.237)\tAcc@1 94.922 (93.984)\n",
      "Train: [42][20/98]\tBT 0.658 (0.686)\tDT 0.245 (0.273)\tloss 0.293 (0.217)\tKL loss 0.297 (0.226)\tAcc@1 92.773 (94.268)\n",
      "Train: [42][30/98]\tBT 0.665 (0.681)\tDT 0.255 (0.268)\tloss 0.218 (0.210)\tKL loss 0.231 (0.223)\tAcc@1 94.727 (94.414)\n",
      "Train: [42][40/98]\tBT 0.661 (0.678)\tDT 0.251 (0.265)\tloss 0.161 (0.219)\tKL loss 0.175 (0.230)\tAcc@1 95.117 (94.292)\n",
      "Train: [42][50/98]\tBT 0.670 (0.677)\tDT 0.254 (0.264)\tloss 0.283 (0.220)\tKL loss 0.280 (0.231)\tAcc@1 93.945 (94.234)\n",
      "Train: [42][60/98]\tBT 0.669 (0.676)\tDT 0.252 (0.263)\tloss 0.196 (0.216)\tKL loss 0.236 (0.227)\tAcc@1 94.141 (94.274)\n",
      "Train: [42][70/98]\tBT 0.673 (0.677)\tDT 0.260 (0.263)\tloss 0.227 (0.214)\tKL loss 0.225 (0.226)\tAcc@1 94.531 (94.350)\n",
      "Train: [42][80/98]\tBT 0.681 (0.676)\tDT 0.268 (0.262)\tloss 0.165 (0.214)\tKL loss 0.177 (0.225)\tAcc@1 95.898 (94.326)\n",
      "Train: [42][90/98]\tBT 0.664 (0.676)\tDT 0.253 (0.262)\tloss 0.207 (0.214)\tKL loss 0.217 (0.225)\tAcc@1 93.945 (94.316)\n",
      "Train epoch 42, total time 66.48, accuracy:94.32\n",
      "Test: [0/20]\tTime 0.280 (0.280)\tLoss 1.5166 (1.5166)\tKL loss 1.496 (1.496)\tAcc@1 81.250 (81.250)\n",
      "Test: [10/20]\tTime 0.280 (0.284)\tLoss 1.6693 (1.5462)\tKL loss 1.639 (1.535)\tAcc@1 79.883 (80.380)\n",
      " * Acc@1 80.140\n",
      "Train: [43][10/98]\tBT 0.676 (0.670)\tDT 0.259 (0.257)\tloss 0.197 (0.221)\tKL loss 0.201 (0.230)\tAcc@1 94.336 (94.355)\n",
      "Train: [43][20/98]\tBT 0.679 (0.671)\tDT 0.266 (0.257)\tloss 0.204 (0.213)\tKL loss 0.213 (0.224)\tAcc@1 93.164 (94.434)\n",
      "Train: [43][30/98]\tBT 0.669 (0.671)\tDT 0.255 (0.257)\tloss 0.115 (0.212)\tKL loss 0.121 (0.223)\tAcc@1 96.680 (94.408)\n",
      "Train: [43][40/98]\tBT 0.677 (0.671)\tDT 0.263 (0.257)\tloss 0.249 (0.220)\tKL loss 0.276 (0.232)\tAcc@1 93.555 (94.307)\n",
      "Train: [43][50/98]\tBT 0.668 (0.672)\tDT 0.255 (0.259)\tloss 0.196 (0.216)\tKL loss 0.197 (0.230)\tAcc@1 94.922 (94.336)\n",
      "Train: [43][60/98]\tBT 0.670 (0.672)\tDT 0.255 (0.258)\tloss 0.238 (0.211)\tKL loss 0.257 (0.224)\tAcc@1 94.531 (94.434)\n",
      "Train: [43][70/98]\tBT 0.668 (0.672)\tDT 0.253 (0.258)\tloss 0.384 (0.214)\tKL loss 0.397 (0.226)\tAcc@1 93.164 (94.436)\n",
      "Train: [43][80/98]\tBT 0.663 (0.674)\tDT 0.250 (0.260)\tloss 0.185 (0.215)\tKL loss 0.203 (0.227)\tAcc@1 94.141 (94.412)\n",
      "Train: [43][90/98]\tBT 0.673 (0.674)\tDT 0.259 (0.260)\tloss 0.184 (0.215)\tKL loss 0.207 (0.228)\tAcc@1 93.945 (94.373)\n",
      "Train epoch 43, total time 66.05, accuracy:94.39\n",
      "Test: [0/20]\tTime 0.282 (0.282)\tLoss 1.3958 (1.3958)\tKL loss 1.397 (1.397)\tAcc@1 79.102 (79.102)\n",
      "Test: [10/20]\tTime 0.278 (0.283)\tLoss 1.4262 (1.4169)\tKL loss 1.416 (1.415)\tAcc@1 80.469 (81.037)\n",
      " * Acc@1 80.880\n",
      "Train: [44][10/98]\tBT 0.677 (0.669)\tDT 0.263 (0.255)\tloss 0.213 (0.227)\tKL loss 0.214 (0.234)\tAcc@1 94.922 (94.316)\n",
      "Train: [44][20/98]\tBT 0.673 (0.669)\tDT 0.257 (0.256)\tloss 0.186 (0.219)\tKL loss 0.200 (0.227)\tAcc@1 94.922 (94.434)\n",
      "Train: [44][30/98]\tBT 0.699 (0.671)\tDT 0.284 (0.257)\tloss 0.293 (0.221)\tKL loss 0.297 (0.230)\tAcc@1 92.773 (94.323)\n",
      "Train: [44][40/98]\tBT 0.664 (0.672)\tDT 0.251 (0.258)\tloss 0.132 (0.218)\tKL loss 0.144 (0.227)\tAcc@1 95.703 (94.380)\n",
      "Train: [44][50/98]\tBT 0.663 (0.672)\tDT 0.250 (0.258)\tloss 0.217 (0.219)\tKL loss 0.219 (0.228)\tAcc@1 94.531 (94.332)\n",
      "Train: [44][60/98]\tBT 0.665 (0.676)\tDT 0.252 (0.261)\tloss 0.405 (0.218)\tKL loss 0.404 (0.228)\tAcc@1 89.844 (94.297)\n",
      "Train: [44][70/98]\tBT 0.665 (0.675)\tDT 0.251 (0.261)\tloss 0.296 (0.220)\tKL loss 0.304 (0.229)\tAcc@1 93.750 (94.261)\n",
      "Train: [44][80/98]\tBT 0.673 (0.675)\tDT 0.259 (0.261)\tloss 0.245 (0.224)\tKL loss 0.263 (0.234)\tAcc@1 93.750 (94.229)\n",
      "Train: [44][90/98]\tBT 0.669 (0.675)\tDT 0.255 (0.261)\tloss 0.233 (0.227)\tKL loss 0.233 (0.236)\tAcc@1 94.141 (94.188)\n",
      "Train epoch 44, total time 66.17, accuracy:94.23\n",
      "Test: [0/20]\tTime 0.285 (0.285)\tLoss 1.4386 (1.4386)\tKL loss 1.435 (1.435)\tAcc@1 79.883 (79.883)\n",
      "Test: [10/20]\tTime 0.291 (0.293)\tLoss 1.4212 (1.4399)\tKL loss 1.419 (1.437)\tAcc@1 80.469 (81.463)\n",
      " * Acc@1 81.210\n",
      "Train: [45][10/98]\tBT 0.673 (0.671)\tDT 0.262 (0.258)\tloss 0.143 (0.216)\tKL loss 0.149 (0.222)\tAcc@1 95.898 (94.336)\n",
      "Train: [45][20/98]\tBT 0.677 (0.673)\tDT 0.267 (0.261)\tloss 0.291 (0.206)\tKL loss 0.297 (0.220)\tAcc@1 91.992 (94.434)\n",
      "Train: [45][30/98]\tBT 0.685 (0.674)\tDT 0.271 (0.261)\tloss 0.238 (0.207)\tKL loss 0.231 (0.218)\tAcc@1 93.359 (94.466)\n",
      "Train: [45][40/98]\tBT 0.740 (0.679)\tDT 0.325 (0.266)\tloss 0.244 (0.207)\tKL loss 0.232 (0.218)\tAcc@1 94.336 (94.434)\n",
      "Train: [45][50/98]\tBT 0.668 (0.679)\tDT 0.254 (0.265)\tloss 0.199 (0.212)\tKL loss 0.227 (0.222)\tAcc@1 94.922 (94.422)\n",
      "Train: [45][60/98]\tBT 0.674 (0.678)\tDT 0.261 (0.265)\tloss 0.190 (0.214)\tKL loss 0.196 (0.224)\tAcc@1 94.727 (94.368)\n",
      "Train: [45][70/98]\tBT 0.664 (0.678)\tDT 0.252 (0.264)\tloss 0.188 (0.221)\tKL loss 0.213 (0.231)\tAcc@1 94.727 (94.316)\n",
      "Train: [45][80/98]\tBT 0.677 (0.676)\tDT 0.261 (0.263)\tloss 0.283 (0.226)\tKL loss 0.275 (0.236)\tAcc@1 93.750 (94.248)\n",
      "Train: [45][90/98]\tBT 0.667 (0.675)\tDT 0.257 (0.262)\tloss 0.283 (0.227)\tKL loss 0.298 (0.237)\tAcc@1 93.750 (94.277)\n",
      "Train epoch 45, total time 66.17, accuracy:94.29\n",
      "Test: [0/20]\tTime 0.297 (0.297)\tLoss 1.5791 (1.5791)\tKL loss 1.566 (1.566)\tAcc@1 81.250 (81.250)\n",
      "Test: [10/20]\tTime 0.293 (0.288)\tLoss 1.5559 (1.5425)\tKL loss 1.529 (1.536)\tAcc@1 80.664 (81.268)\n",
      " * Acc@1 81.310\n",
      "Train: [46][10/98]\tBT 0.665 (0.670)\tDT 0.251 (0.255)\tloss 0.268 (0.238)\tKL loss 0.281 (0.254)\tAcc@1 93.555 (93.887)\n",
      "Train: [46][20/98]\tBT 0.668 (0.671)\tDT 0.255 (0.256)\tloss 0.177 (0.220)\tKL loss 0.182 (0.233)\tAcc@1 94.727 (94.248)\n",
      "Train: [46][30/98]\tBT 0.667 (0.679)\tDT 0.253 (0.264)\tloss 0.180 (0.220)\tKL loss 0.200 (0.232)\tAcc@1 95.703 (94.342)\n",
      "Train: [46][40/98]\tBT 0.666 (0.678)\tDT 0.254 (0.264)\tloss 0.265 (0.216)\tKL loss 0.273 (0.227)\tAcc@1 92.773 (94.404)\n",
      "Train: [46][50/98]\tBT 0.668 (0.677)\tDT 0.255 (0.263)\tloss 0.140 (0.210)\tKL loss 0.145 (0.222)\tAcc@1 95.508 (94.520)\n",
      "Train: [46][60/98]\tBT 0.670 (0.676)\tDT 0.255 (0.262)\tloss 0.181 (0.204)\tKL loss 0.184 (0.215)\tAcc@1 93.750 (94.583)\n",
      "Train: [46][70/98]\tBT 0.665 (0.675)\tDT 0.253 (0.261)\tloss 0.221 (0.204)\tKL loss 0.238 (0.218)\tAcc@1 94.336 (94.598)\n",
      "Train: [46][80/98]\tBT 0.677 (0.675)\tDT 0.265 (0.261)\tloss 0.208 (0.205)\tKL loss 0.212 (0.220)\tAcc@1 93.945 (94.531)\n",
      "Train: [46][90/98]\tBT 0.663 (0.674)\tDT 0.249 (0.261)\tloss 0.166 (0.203)\tKL loss 0.173 (0.217)\tAcc@1 95.312 (94.575)\n",
      "Train epoch 46, total time 66.06, accuracy:94.57\n",
      "Test: [0/20]\tTime 0.284 (0.284)\tLoss 1.4857 (1.4857)\tKL loss 1.468 (1.468)\tAcc@1 79.883 (79.883)\n",
      "Test: [10/20]\tTime 0.275 (0.283)\tLoss 1.4451 (1.4773)\tKL loss 1.446 (1.473)\tAcc@1 79.883 (80.806)\n",
      " * Acc@1 80.830\n",
      "Train: [47][10/98]\tBT 0.672 (0.702)\tDT 0.260 (0.287)\tloss 0.144 (0.213)\tKL loss 0.118 (0.219)\tAcc@1 97.266 (94.609)\n",
      "Train: [47][20/98]\tBT 0.674 (0.686)\tDT 0.258 (0.272)\tloss 0.199 (0.201)\tKL loss 0.202 (0.210)\tAcc@1 95.117 (94.648)\n",
      "Train: [47][30/98]\tBT 0.678 (0.682)\tDT 0.265 (0.268)\tloss 0.290 (0.199)\tKL loss 0.293 (0.210)\tAcc@1 92.383 (94.668)\n",
      "Train: [47][40/98]\tBT 0.690 (0.681)\tDT 0.276 (0.267)\tloss 0.231 (0.198)\tKL loss 0.236 (0.212)\tAcc@1 93.555 (94.644)\n",
      "Train: [47][50/98]\tBT 0.671 (0.679)\tDT 0.257 (0.265)\tloss 0.289 (0.204)\tKL loss 0.292 (0.216)\tAcc@1 93.555 (94.555)\n",
      "Train: [47][60/98]\tBT 0.666 (0.677)\tDT 0.254 (0.263)\tloss 0.213 (0.205)\tKL loss 0.223 (0.215)\tAcc@1 93.750 (94.518)\n",
      "Train: [47][70/98]\tBT 0.667 (0.676)\tDT 0.254 (0.262)\tloss 0.188 (0.210)\tKL loss 0.205 (0.219)\tAcc@1 95.117 (94.414)\n",
      "Train: [47][80/98]\tBT 0.668 (0.676)\tDT 0.256 (0.262)\tloss 0.293 (0.212)\tKL loss 0.312 (0.222)\tAcc@1 93.555 (94.392)\n",
      "Train: [47][90/98]\tBT 0.670 (0.676)\tDT 0.257 (0.262)\tloss 0.337 (0.220)\tKL loss 0.350 (0.231)\tAcc@1 92.773 (94.295)\n",
      "Train epoch 47, total time 66.41, accuracy:94.25\n",
      "Test: [0/20]\tTime 0.285 (0.285)\tLoss 1.5666 (1.5666)\tKL loss 1.559 (1.559)\tAcc@1 79.883 (79.883)\n",
      "Test: [10/20]\tTime 0.286 (0.283)\tLoss 1.4917 (1.5084)\tKL loss 1.480 (1.509)\tAcc@1 80.469 (81.126)\n",
      " * Acc@1 81.350\n",
      "Train: [48][10/98]\tBT 0.673 (0.671)\tDT 0.263 (0.259)\tloss 0.170 (0.246)\tKL loss 0.179 (0.257)\tAcc@1 94.727 (94.199)\n",
      "Train: [48][20/98]\tBT 0.694 (0.674)\tDT 0.279 (0.262)\tloss 0.252 (0.243)\tKL loss 0.257 (0.253)\tAcc@1 93.750 (94.102)\n",
      "Train: [48][30/98]\tBT 0.661 (0.675)\tDT 0.247 (0.262)\tloss 0.128 (0.235)\tKL loss 0.129 (0.246)\tAcc@1 96.484 (94.141)\n",
      "Train: [48][40/98]\tBT 0.677 (0.674)\tDT 0.264 (0.260)\tloss 0.166 (0.227)\tKL loss 0.191 (0.237)\tAcc@1 93.750 (94.268)\n",
      "Train: [48][50/98]\tBT 0.689 (0.674)\tDT 0.272 (0.260)\tloss 0.192 (0.228)\tKL loss 0.209 (0.237)\tAcc@1 95.117 (94.176)\n",
      "Train: [48][60/98]\tBT 0.668 (0.674)\tDT 0.256 (0.260)\tloss 0.286 (0.230)\tKL loss 0.274 (0.238)\tAcc@1 93.750 (94.141)\n",
      "Train: [48][70/98]\tBT 0.672 (0.673)\tDT 0.262 (0.260)\tloss 0.222 (0.226)\tKL loss 0.229 (0.234)\tAcc@1 94.727 (94.208)\n",
      "Train: [48][80/98]\tBT 0.668 (0.677)\tDT 0.253 (0.263)\tloss 0.161 (0.221)\tKL loss 0.168 (0.230)\tAcc@1 94.727 (94.302)\n",
      "Train: [48][90/98]\tBT 0.668 (0.676)\tDT 0.253 (0.262)\tloss 0.195 (0.224)\tKL loss 0.204 (0.233)\tAcc@1 93.945 (94.258)\n",
      "Train epoch 48, total time 66.23, accuracy:94.28\n",
      "Test: [0/20]\tTime 0.305 (0.305)\tLoss 1.5870 (1.5870)\tKL loss 1.579 (1.579)\tAcc@1 78.906 (78.906)\n",
      "Test: [10/20]\tTime 0.283 (0.291)\tLoss 1.5229 (1.5126)\tKL loss 1.500 (1.503)\tAcc@1 79.883 (80.220)\n",
      " * Acc@1 80.530\n",
      "Train: [49][10/98]\tBT 0.668 (0.669)\tDT 0.254 (0.256)\tloss 0.289 (0.228)\tKL loss 0.316 (0.242)\tAcc@1 92.188 (93.926)\n",
      "Train: [49][20/98]\tBT 0.689 (0.669)\tDT 0.272 (0.255)\tloss 0.274 (0.231)\tKL loss 0.282 (0.243)\tAcc@1 92.773 (94.199)\n",
      "Train: [49][30/98]\tBT 0.660 (0.671)\tDT 0.246 (0.258)\tloss 0.218 (0.228)\tKL loss 0.226 (0.241)\tAcc@1 94.531 (94.115)\n",
      "Train: [49][40/98]\tBT 0.672 (0.672)\tDT 0.259 (0.258)\tloss 0.129 (0.229)\tKL loss 0.139 (0.241)\tAcc@1 96.680 (94.170)\n",
      "Train: [49][50/98]\tBT 0.681 (0.674)\tDT 0.269 (0.261)\tloss 0.165 (0.226)\tKL loss 0.176 (0.238)\tAcc@1 94.336 (94.195)\n",
      "Train: [49][60/98]\tBT 0.672 (0.678)\tDT 0.257 (0.264)\tloss 0.166 (0.223)\tKL loss 0.185 (0.236)\tAcc@1 96.875 (94.238)\n",
      "Train: [49][70/98]\tBT 0.678 (0.677)\tDT 0.265 (0.263)\tloss 0.227 (0.225)\tKL loss 0.239 (0.237)\tAcc@1 93.164 (94.222)\n",
      "Train: [49][80/98]\tBT 0.674 (0.677)\tDT 0.259 (0.263)\tloss 0.221 (0.226)\tKL loss 0.223 (0.238)\tAcc@1 93.945 (94.219)\n",
      "Train: [49][90/98]\tBT 0.665 (0.676)\tDT 0.251 (0.262)\tloss 0.117 (0.220)\tKL loss 0.128 (0.232)\tAcc@1 96.680 (94.351)\n",
      "Train epoch 49, total time 66.34, accuracy:94.33\n",
      "Test: [0/20]\tTime 0.287 (0.287)\tLoss 1.3949 (1.3949)\tKL loss 1.389 (1.389)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.281 (0.283)\tLoss 1.3947 (1.4096)\tKL loss 1.388 (1.405)\tAcc@1 79.688 (80.948)\n",
      " * Acc@1 80.830\n",
      "Train: [50][10/98]\tBT 0.677 (0.672)\tDT 0.265 (0.260)\tloss 0.184 (0.216)\tKL loss 0.194 (0.231)\tAcc@1 94.727 (94.141)\n",
      "Train: [50][20/98]\tBT 0.674 (0.673)\tDT 0.261 (0.260)\tloss 0.130 (0.216)\tKL loss 0.133 (0.230)\tAcc@1 96.289 (94.062)\n",
      "Train: [50][30/98]\tBT 0.658 (0.674)\tDT 0.248 (0.260)\tloss 0.231 (0.216)\tKL loss 0.258 (0.229)\tAcc@1 94.531 (94.284)\n",
      "Train: [50][40/98]\tBT 0.667 (0.673)\tDT 0.252 (0.259)\tloss 0.161 (0.219)\tKL loss 0.172 (0.231)\tAcc@1 96.289 (94.370)\n",
      "Train: [50][50/98]\tBT 0.678 (0.678)\tDT 0.266 (0.264)\tloss 0.245 (0.221)\tKL loss 0.223 (0.231)\tAcc@1 94.336 (94.289)\n",
      "Train: [50][60/98]\tBT 0.671 (0.677)\tDT 0.257 (0.263)\tloss 0.296 (0.223)\tKL loss 0.310 (0.232)\tAcc@1 93.945 (94.261)\n",
      "Train: [50][70/98]\tBT 0.688 (0.676)\tDT 0.274 (0.263)\tloss 0.147 (0.225)\tKL loss 0.160 (0.233)\tAcc@1 95.508 (94.277)\n",
      "Train: [50][80/98]\tBT 0.679 (0.676)\tDT 0.262 (0.263)\tloss 0.111 (0.225)\tKL loss 0.117 (0.234)\tAcc@1 96.289 (94.277)\n",
      "Train: [50][90/98]\tBT 0.681 (0.676)\tDT 0.267 (0.262)\tloss 0.117 (0.221)\tKL loss 0.128 (0.230)\tAcc@1 96.094 (94.386)\n",
      "Train epoch 50, total time 66.21, accuracy:94.37\n",
      "Test: [0/20]\tTime 0.279 (0.279)\tLoss 1.5299 (1.5299)\tKL loss 1.497 (1.497)\tAcc@1 78.711 (78.711)\n",
      "Test: [10/20]\tTime 0.276 (0.281)\tLoss 1.4884 (1.4294)\tKL loss 1.478 (1.427)\tAcc@1 78.906 (80.966)\n",
      " * Acc@1 80.890\n",
      "Train: [51][10/98]\tBT 0.690 (0.671)\tDT 0.275 (0.259)\tloss 0.130 (0.179)\tKL loss 0.158 (0.191)\tAcc@1 95.898 (94.824)\n",
      "Train: [51][20/98]\tBT 0.657 (0.673)\tDT 0.243 (0.260)\tloss 0.226 (0.199)\tKL loss 0.253 (0.212)\tAcc@1 93.750 (94.629)\n",
      "Train: [51][30/98]\tBT 0.664 (0.681)\tDT 0.252 (0.267)\tloss 0.150 (0.200)\tKL loss 0.157 (0.213)\tAcc@1 96.484 (94.596)\n",
      "Train: [51][40/98]\tBT 0.672 (0.679)\tDT 0.258 (0.266)\tloss 0.163 (0.200)\tKL loss 0.167 (0.214)\tAcc@1 95.508 (94.561)\n",
      "Train: [51][50/98]\tBT 0.667 (0.678)\tDT 0.255 (0.265)\tloss 0.192 (0.206)\tKL loss 0.200 (0.219)\tAcc@1 94.727 (94.500)\n",
      "Train: [51][60/98]\tBT 0.679 (0.677)\tDT 0.265 (0.264)\tloss 0.227 (0.202)\tKL loss 0.223 (0.216)\tAcc@1 94.336 (94.587)\n",
      "Train: [51][70/98]\tBT 0.658 (0.677)\tDT 0.243 (0.263)\tloss 0.193 (0.209)\tKL loss 0.199 (0.222)\tAcc@1 94.922 (94.512)\n",
      "Train: [51][80/98]\tBT 0.671 (0.676)\tDT 0.262 (0.262)\tloss 0.250 (0.209)\tKL loss 0.255 (0.221)\tAcc@1 93.164 (94.556)\n",
      "Train: [51][90/98]\tBT 0.678 (0.675)\tDT 0.263 (0.262)\tloss 0.218 (0.208)\tKL loss 0.223 (0.220)\tAcc@1 93.945 (94.564)\n",
      "Train epoch 51, total time 66.13, accuracy:94.53\n",
      "Test: [0/20]\tTime 0.284 (0.284)\tLoss 1.5374 (1.5374)\tKL loss 1.529 (1.529)\tAcc@1 78.906 (78.906)\n",
      "Test: [10/20]\tTime 0.284 (0.285)\tLoss 1.3728 (1.4542)\tKL loss 1.373 (1.452)\tAcc@1 79.883 (80.948)\n",
      " * Acc@1 80.800\n",
      "Train: [52][10/98]\tBT 0.665 (0.690)\tDT 0.253 (0.274)\tloss 0.289 (0.261)\tKL loss 0.293 (0.277)\tAcc@1 92.383 (93.359)\n",
      "Train: [52][20/98]\tBT 0.665 (0.680)\tDT 0.254 (0.265)\tloss 0.202 (0.238)\tKL loss 0.210 (0.254)\tAcc@1 94.531 (93.721)\n",
      "Train: [52][30/98]\tBT 0.666 (0.676)\tDT 0.254 (0.261)\tloss 0.118 (0.224)\tKL loss 0.142 (0.240)\tAcc@1 96.289 (93.997)\n",
      "Train: [52][40/98]\tBT 0.686 (0.676)\tDT 0.272 (0.262)\tloss 0.213 (0.218)\tKL loss 0.220 (0.231)\tAcc@1 94.531 (94.150)\n",
      "Train: [52][50/98]\tBT 0.672 (0.675)\tDT 0.261 (0.261)\tloss 0.192 (0.211)\tKL loss 0.202 (0.224)\tAcc@1 94.727 (94.242)\n",
      "Train: [52][60/98]\tBT 0.678 (0.675)\tDT 0.264 (0.261)\tloss 0.389 (0.216)\tKL loss 0.389 (0.228)\tAcc@1 91.602 (94.209)\n",
      "Train: [52][70/98]\tBT 0.684 (0.674)\tDT 0.271 (0.260)\tloss 0.338 (0.219)\tKL loss 0.326 (0.231)\tAcc@1 92.773 (94.222)\n",
      "Train: [52][80/98]\tBT 0.655 (0.673)\tDT 0.242 (0.260)\tloss 0.196 (0.221)\tKL loss 0.223 (0.233)\tAcc@1 94.727 (94.214)\n",
      "Train: [52][90/98]\tBT 0.675 (0.674)\tDT 0.260 (0.260)\tloss 0.154 (0.220)\tKL loss 0.171 (0.232)\tAcc@1 95.703 (94.253)\n",
      "Train epoch 52, total time 66.22, accuracy:94.25\n",
      "Test: [0/20]\tTime 0.281 (0.281)\tLoss 1.5417 (1.5417)\tKL loss 1.535 (1.535)\tAcc@1 79.102 (79.102)\n",
      "Test: [10/20]\tTime 0.284 (0.283)\tLoss 1.6670 (1.5040)\tKL loss 1.631 (1.498)\tAcc@1 77.344 (80.451)\n",
      " * Acc@1 80.390\n",
      "Train: [53][10/98]\tBT 0.670 (0.668)\tDT 0.256 (0.255)\tloss 0.140 (0.200)\tKL loss 0.157 (0.229)\tAcc@1 96.094 (94.727)\n",
      "Train: [53][20/98]\tBT 0.666 (0.672)\tDT 0.255 (0.258)\tloss 0.179 (0.221)\tKL loss 0.188 (0.241)\tAcc@1 95.508 (94.639)\n",
      "Train: [53][30/98]\tBT 0.679 (0.674)\tDT 0.265 (0.261)\tloss 0.162 (0.211)\tKL loss 0.181 (0.230)\tAcc@1 94.531 (94.648)\n",
      "Train: [53][40/98]\tBT 0.679 (0.674)\tDT 0.265 (0.260)\tloss 0.257 (0.219)\tKL loss 0.259 (0.234)\tAcc@1 93.750 (94.458)\n",
      "Train: [53][50/98]\tBT 0.664 (0.675)\tDT 0.252 (0.261)\tloss 0.226 (0.219)\tKL loss 0.245 (0.234)\tAcc@1 94.141 (94.316)\n",
      "Train: [53][60/98]\tBT 0.673 (0.675)\tDT 0.258 (0.261)\tloss 0.191 (0.220)\tKL loss 0.194 (0.234)\tAcc@1 94.141 (94.222)\n",
      "Train: [53][70/98]\tBT 0.663 (0.675)\tDT 0.249 (0.261)\tloss 0.197 (0.224)\tKL loss 0.211 (0.238)\tAcc@1 94.336 (94.227)\n",
      "Train: [53][80/98]\tBT 0.688 (0.677)\tDT 0.271 (0.263)\tloss 0.311 (0.222)\tKL loss 0.315 (0.236)\tAcc@1 93.164 (94.238)\n",
      "Train: [53][90/98]\tBT 0.671 (0.677)\tDT 0.257 (0.263)\tloss 0.208 (0.223)\tKL loss 0.219 (0.237)\tAcc@1 95.312 (94.262)\n",
      "Train epoch 53, total time 66.26, accuracy:94.27\n",
      "Test: [0/20]\tTime 0.282 (0.282)\tLoss 1.4918 (1.4918)\tKL loss 1.467 (1.467)\tAcc@1 78.711 (78.711)\n",
      "Test: [10/20]\tTime 0.282 (0.282)\tLoss 1.4942 (1.4263)\tKL loss 1.452 (1.417)\tAcc@1 79.297 (80.717)\n",
      " * Acc@1 80.750\n",
      "Train: [54][10/98]\tBT 0.671 (0.670)\tDT 0.256 (0.257)\tloss 0.292 (0.234)\tKL loss 0.268 (0.242)\tAcc@1 93.359 (94.414)\n",
      "Train: [54][20/98]\tBT 0.674 (0.673)\tDT 0.263 (0.260)\tloss 0.229 (0.240)\tKL loss 0.239 (0.247)\tAcc@1 93.359 (94.131)\n",
      "Train: [54][30/98]\tBT 0.681 (0.671)\tDT 0.265 (0.259)\tloss 0.272 (0.232)\tKL loss 0.272 (0.240)\tAcc@1 93.359 (94.284)\n",
      "Train: [54][40/98]\tBT 0.658 (0.671)\tDT 0.248 (0.258)\tloss 0.224 (0.224)\tKL loss 0.234 (0.232)\tAcc@1 94.336 (94.365)\n",
      "Train: [54][50/98]\tBT 0.669 (0.671)\tDT 0.253 (0.258)\tloss 0.184 (0.222)\tKL loss 0.205 (0.232)\tAcc@1 95.312 (94.348)\n",
      "Train: [54][60/98]\tBT 0.663 (0.672)\tDT 0.254 (0.259)\tloss 0.191 (0.220)\tKL loss 0.188 (0.230)\tAcc@1 94.922 (94.388)\n",
      "Train: [54][70/98]\tBT 0.677 (0.674)\tDT 0.264 (0.261)\tloss 0.238 (0.221)\tKL loss 0.237 (0.232)\tAcc@1 93.750 (94.408)\n",
      "Train: [54][80/98]\tBT 0.669 (0.674)\tDT 0.257 (0.261)\tloss 0.186 (0.220)\tKL loss 0.189 (0.230)\tAcc@1 93.945 (94.355)\n",
      "Train: [54][90/98]\tBT 0.682 (0.674)\tDT 0.269 (0.260)\tloss 0.222 (0.223)\tKL loss 0.238 (0.233)\tAcc@1 93.359 (94.280)\n",
      "Train epoch 54, total time 66.06, accuracy:94.27\n",
      "Test: [0/20]\tTime 0.275 (0.275)\tLoss 1.3828 (1.3828)\tKL loss 1.368 (1.368)\tAcc@1 79.883 (79.883)\n",
      "Test: [10/20]\tTime 0.279 (0.284)\tLoss 1.4213 (1.4026)\tKL loss 1.388 (1.398)\tAcc@1 77.930 (80.788)\n",
      " * Acc@1 80.950\n",
      "Train: [55][10/98]\tBT 0.666 (0.668)\tDT 0.254 (0.256)\tloss 0.243 (0.215)\tKL loss 0.254 (0.221)\tAcc@1 94.336 (94.219)\n",
      "Train: [55][20/98]\tBT 0.661 (0.667)\tDT 0.247 (0.255)\tloss 0.201 (0.200)\tKL loss 0.222 (0.211)\tAcc@1 93.555 (94.453)\n",
      "Train: [55][30/98]\tBT 0.662 (0.666)\tDT 0.249 (0.255)\tloss 0.179 (0.206)\tKL loss 0.179 (0.216)\tAcc@1 95.312 (94.375)\n",
      "Train: [55][40/98]\tBT 0.661 (0.667)\tDT 0.249 (0.255)\tloss 0.224 (0.213)\tKL loss 0.245 (0.223)\tAcc@1 95.703 (94.360)\n",
      "Train: [55][50/98]\tBT 0.657 (0.672)\tDT 0.244 (0.260)\tloss 0.145 (0.214)\tKL loss 0.143 (0.222)\tAcc@1 96.094 (94.457)\n",
      "Train: [55][60/98]\tBT 0.661 (0.671)\tDT 0.249 (0.258)\tloss 0.127 (0.211)\tKL loss 0.137 (0.221)\tAcc@1 96.875 (94.561)\n",
      "Train: [55][70/98]\tBT 0.687 (0.671)\tDT 0.270 (0.258)\tloss 0.236 (0.211)\tKL loss 0.240 (0.221)\tAcc@1 93.555 (94.526)\n",
      "Train: [55][80/98]\tBT 0.677 (0.670)\tDT 0.260 (0.257)\tloss 0.155 (0.209)\tKL loss 0.170 (0.220)\tAcc@1 95.312 (94.531)\n",
      "Train: [55][90/98]\tBT 0.662 (0.668)\tDT 0.250 (0.256)\tloss 0.228 (0.211)\tKL loss 0.240 (0.222)\tAcc@1 93.750 (94.501)\n",
      "Train epoch 55, total time 65.40, accuracy:94.46\n",
      "Test: [0/20]\tTime 0.278 (0.278)\tLoss 1.5260 (1.5260)\tKL loss 1.510 (1.510)\tAcc@1 79.492 (79.492)\n",
      "Test: [10/20]\tTime 0.279 (0.276)\tLoss 1.5336 (1.4903)\tKL loss 1.494 (1.488)\tAcc@1 78.516 (80.522)\n",
      " * Acc@1 80.380\n",
      "Train: [56][10/98]\tBT 0.648 (0.653)\tDT 0.236 (0.241)\tloss 0.163 (0.219)\tKL loss 0.181 (0.233)\tAcc@1 95.898 (94.102)\n",
      "Train: [56][20/98]\tBT 0.656 (0.654)\tDT 0.242 (0.242)\tloss 0.192 (0.219)\tKL loss 0.209 (0.229)\tAcc@1 95.312 (94.111)\n",
      "Train: [56][30/98]\tBT 0.654 (0.662)\tDT 0.241 (0.249)\tloss 0.182 (0.220)\tKL loss 0.204 (0.231)\tAcc@1 94.336 (94.219)\n",
      "Train: [56][40/98]\tBT 0.670 (0.661)\tDT 0.255 (0.249)\tloss 0.274 (0.221)\tKL loss 0.284 (0.232)\tAcc@1 94.922 (94.229)\n",
      "Train: [56][50/98]\tBT 0.683 (0.663)\tDT 0.269 (0.250)\tloss 0.389 (0.225)\tKL loss 0.391 (0.235)\tAcc@1 91.602 (94.258)\n",
      "Train: [56][60/98]\tBT 0.662 (0.664)\tDT 0.247 (0.251)\tloss 0.182 (0.226)\tKL loss 0.191 (0.236)\tAcc@1 95.508 (94.167)\n",
      "Train: [56][70/98]\tBT 0.656 (0.663)\tDT 0.242 (0.250)\tloss 0.175 (0.225)\tKL loss 0.183 (0.235)\tAcc@1 95.117 (94.202)\n",
      "Train: [56][80/98]\tBT 0.670 (0.663)\tDT 0.256 (0.250)\tloss 0.263 (0.223)\tKL loss 0.261 (0.232)\tAcc@1 94.336 (94.268)\n",
      "Train: [56][90/98]\tBT 0.664 (0.663)\tDT 0.249 (0.250)\tloss 0.288 (0.222)\tKL loss 0.316 (0.232)\tAcc@1 92.773 (94.264)\n",
      "Train epoch 56, total time 64.96, accuracy:94.27\n",
      "Test: [0/20]\tTime 0.276 (0.276)\tLoss 1.4544 (1.4544)\tKL loss 1.459 (1.459)\tAcc@1 79.492 (79.492)\n",
      "Test: [10/20]\tTime 0.274 (0.271)\tLoss 1.4240 (1.3963)\tKL loss 1.401 (1.392)\tAcc@1 80.078 (81.055)\n",
      " * Acc@1 80.930\n",
      "Train: [57][10/98]\tBT 0.664 (0.669)\tDT 0.249 (0.256)\tloss 0.254 (0.219)\tKL loss 0.243 (0.227)\tAcc@1 94.727 (94.180)\n",
      "Train: [57][20/98]\tBT 0.663 (0.683)\tDT 0.249 (0.269)\tloss 0.299 (0.242)\tKL loss 0.316 (0.249)\tAcc@1 92.773 (93.721)\n",
      "Train: [57][30/98]\tBT 0.652 (0.678)\tDT 0.240 (0.265)\tloss 0.137 (0.228)\tKL loss 0.167 (0.238)\tAcc@1 95.312 (94.010)\n",
      "Train: [57][40/98]\tBT 0.671 (0.676)\tDT 0.258 (0.262)\tloss 0.257 (0.225)\tKL loss 0.263 (0.235)\tAcc@1 93.164 (94.087)\n",
      "Train: [57][50/98]\tBT 0.666 (0.675)\tDT 0.252 (0.262)\tloss 0.228 (0.220)\tKL loss 0.236 (0.229)\tAcc@1 94.922 (94.145)\n",
      "Train: [57][60/98]\tBT 0.664 (0.675)\tDT 0.251 (0.261)\tloss 0.184 (0.213)\tKL loss 0.207 (0.223)\tAcc@1 96.289 (94.281)\n",
      "Train: [57][70/98]\tBT 0.657 (0.674)\tDT 0.245 (0.261)\tloss 0.233 (0.213)\tKL loss 0.252 (0.223)\tAcc@1 94.531 (94.280)\n",
      "Train: [57][80/98]\tBT 0.668 (0.674)\tDT 0.256 (0.260)\tloss 0.204 (0.212)\tKL loss 0.206 (0.221)\tAcc@1 94.531 (94.326)\n",
      "Train: [57][90/98]\tBT 0.654 (0.673)\tDT 0.243 (0.259)\tloss 0.259 (0.215)\tKL loss 0.240 (0.224)\tAcc@1 94.531 (94.293)\n",
      "Train epoch 57, total time 65.94, accuracy:94.27\n",
      "Test: [0/20]\tTime 0.282 (0.282)\tLoss 1.5640 (1.5640)\tKL loss 1.555 (1.555)\tAcc@1 78.711 (78.711)\n",
      "Test: [10/20]\tTime 0.273 (0.305)\tLoss 1.5167 (1.5071)\tKL loss 1.501 (1.502)\tAcc@1 77.148 (80.007)\n",
      " * Acc@1 80.170\n",
      "Train: [58][10/98]\tBT 0.666 (0.662)\tDT 0.256 (0.250)\tloss 0.257 (0.201)\tKL loss 0.295 (0.217)\tAcc@1 94.141 (94.453)\n",
      "Train: [58][20/98]\tBT 0.663 (0.666)\tDT 0.249 (0.253)\tloss 0.172 (0.192)\tKL loss 0.182 (0.204)\tAcc@1 94.727 (94.590)\n",
      "Train: [58][30/98]\tBT 0.668 (0.667)\tDT 0.257 (0.253)\tloss 0.176 (0.194)\tKL loss 0.199 (0.206)\tAcc@1 95.508 (94.655)\n",
      "Train: [58][40/98]\tBT 0.687 (0.668)\tDT 0.273 (0.254)\tloss 0.209 (0.199)\tKL loss 0.195 (0.208)\tAcc@1 94.922 (94.526)\n",
      "Train: [58][50/98]\tBT 0.660 (0.668)\tDT 0.248 (0.255)\tloss 0.198 (0.197)\tKL loss 0.217 (0.209)\tAcc@1 94.531 (94.539)\n",
      "Train: [58][60/98]\tBT 0.666 (0.667)\tDT 0.252 (0.254)\tloss 0.231 (0.198)\tKL loss 0.237 (0.208)\tAcc@1 93.945 (94.541)\n",
      "Train: [58][70/98]\tBT 0.672 (0.667)\tDT 0.260 (0.254)\tloss 0.155 (0.201)\tKL loss 0.180 (0.211)\tAcc@1 94.727 (94.506)\n",
      "Train: [58][80/98]\tBT 0.663 (0.668)\tDT 0.249 (0.254)\tloss 0.230 (0.200)\tKL loss 0.269 (0.210)\tAcc@1 93.945 (94.526)\n",
      "Train: [58][90/98]\tBT 0.661 (0.670)\tDT 0.246 (0.256)\tloss 0.275 (0.203)\tKL loss 0.277 (0.213)\tAcc@1 92.188 (94.462)\n",
      "Train epoch 58, total time 65.64, accuracy:94.43\n",
      "Test: [0/20]\tTime 0.278 (0.278)\tLoss 1.4203 (1.4203)\tKL loss 1.402 (1.402)\tAcc@1 79.492 (79.492)\n",
      "Test: [10/20]\tTime 0.285 (0.282)\tLoss 1.3891 (1.3439)\tKL loss 1.359 (1.336)\tAcc@1 79.297 (80.753)\n",
      " * Acc@1 80.830\n",
      "Train: [59][10/98]\tBT 0.663 (0.669)\tDT 0.249 (0.255)\tloss 0.215 (0.208)\tKL loss 0.216 (0.215)\tAcc@1 94.141 (94.199)\n",
      "Train: [59][20/98]\tBT 0.667 (0.669)\tDT 0.251 (0.255)\tloss 0.305 (0.211)\tKL loss 0.318 (0.222)\tAcc@1 91.992 (94.092)\n",
      "Train: [59][30/98]\tBT 0.664 (0.669)\tDT 0.251 (0.255)\tloss 0.203 (0.212)\tKL loss 0.194 (0.222)\tAcc@1 95.312 (94.316)\n",
      "Train: [59][40/98]\tBT 0.658 (0.669)\tDT 0.242 (0.255)\tloss 0.176 (0.209)\tKL loss 0.189 (0.221)\tAcc@1 94.727 (94.404)\n",
      "Train: [59][50/98]\tBT 0.667 (0.668)\tDT 0.255 (0.255)\tloss 0.227 (0.209)\tKL loss 0.219 (0.220)\tAcc@1 94.531 (94.438)\n",
      "Train: [59][60/98]\tBT 0.661 (0.668)\tDT 0.248 (0.254)\tloss 0.223 (0.208)\tKL loss 0.237 (0.219)\tAcc@1 94.922 (94.456)\n",
      "Train: [59][70/98]\tBT 0.679 (0.671)\tDT 0.262 (0.257)\tloss 0.162 (0.207)\tKL loss 0.167 (0.218)\tAcc@1 96.094 (94.495)\n",
      "Train: [59][80/98]\tBT 0.661 (0.670)\tDT 0.247 (0.256)\tloss 0.282 (0.207)\tKL loss 0.326 (0.219)\tAcc@1 92.969 (94.475)\n",
      "Train: [59][90/98]\tBT 0.672 (0.670)\tDT 0.260 (0.256)\tloss 0.288 (0.209)\tKL loss 0.311 (0.220)\tAcc@1 93.555 (94.442)\n",
      "Train epoch 59, total time 65.62, accuracy:94.42\n",
      "Test: [0/20]\tTime 0.276 (0.276)\tLoss 1.5873 (1.5873)\tKL loss 1.576 (1.576)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.288 (0.282)\tLoss 1.5597 (1.4638)\tKL loss 1.528 (1.460)\tAcc@1 79.688 (80.682)\n",
      " * Acc@1 80.640\n",
      "Train: [60][10/98]\tBT 0.661 (0.667)\tDT 0.250 (0.255)\tloss 0.239 (0.222)\tKL loss 0.250 (0.234)\tAcc@1 93.945 (94.004)\n",
      "Train: [60][20/98]\tBT 0.672 (0.667)\tDT 0.259 (0.254)\tloss 0.289 (0.245)\tKL loss 0.298 (0.254)\tAcc@1 92.773 (93.828)\n",
      "Train: [60][30/98]\tBT 0.661 (0.667)\tDT 0.248 (0.254)\tloss 0.196 (0.222)\tKL loss 0.208 (0.232)\tAcc@1 94.727 (94.186)\n",
      "Train: [60][40/98]\tBT 0.648 (0.667)\tDT 0.234 (0.254)\tloss 0.198 (0.216)\tKL loss 0.200 (0.226)\tAcc@1 93.164 (94.229)\n",
      "Train: [60][50/98]\tBT 0.657 (0.668)\tDT 0.244 (0.254)\tloss 0.174 (0.214)\tKL loss 0.180 (0.225)\tAcc@1 94.531 (94.320)\n",
      "Train: [60][60/98]\tBT 0.660 (0.672)\tDT 0.246 (0.258)\tloss 0.212 (0.211)\tKL loss 0.219 (0.222)\tAcc@1 92.969 (94.372)\n",
      "Train: [60][70/98]\tBT 0.684 (0.671)\tDT 0.272 (0.258)\tloss 0.239 (0.210)\tKL loss 0.248 (0.221)\tAcc@1 94.141 (94.369)\n",
      "Train: [60][80/98]\tBT 0.655 (0.670)\tDT 0.241 (0.256)\tloss 0.168 (0.213)\tKL loss 0.172 (0.224)\tAcc@1 95.703 (94.365)\n",
      "Train: [60][90/98]\tBT 0.659 (0.669)\tDT 0.246 (0.256)\tloss 0.235 (0.215)\tKL loss 0.233 (0.225)\tAcc@1 93.359 (94.312)\n",
      "Train epoch 60, total time 65.56, accuracy:94.26\n",
      "Test: [0/20]\tTime 0.283 (0.283)\tLoss 1.4983 (1.4983)\tKL loss 1.482 (1.482)\tAcc@1 81.641 (81.641)\n",
      "Test: [10/20]\tTime 0.267 (0.276)\tLoss 1.5506 (1.4044)\tKL loss 1.517 (1.396)\tAcc@1 79.297 (81.516)\n",
      " * Acc@1 81.240\n",
      "Train: [61][10/98]\tBT 0.648 (0.659)\tDT 0.238 (0.245)\tloss 0.155 (0.194)\tKL loss 0.161 (0.205)\tAcc@1 95.898 (95.020)\n",
      "Train: [61][20/98]\tBT 0.666 (0.660)\tDT 0.252 (0.246)\tloss 0.149 (0.190)\tKL loss 0.160 (0.202)\tAcc@1 95.703 (95.000)\n",
      "Train: [61][30/98]\tBT 0.659 (0.659)\tDT 0.242 (0.246)\tloss 0.134 (0.182)\tKL loss 0.149 (0.195)\tAcc@1 96.289 (95.059)\n",
      "Train: [61][40/98]\tBT 0.676 (0.665)\tDT 0.264 (0.251)\tloss 0.125 (0.175)\tKL loss 0.134 (0.187)\tAcc@1 96.094 (95.161)\n",
      "Train: [61][50/98]\tBT 0.654 (0.665)\tDT 0.237 (0.252)\tloss 0.192 (0.177)\tKL loss 0.204 (0.190)\tAcc@1 94.336 (95.133)\n",
      "Train: [61][60/98]\tBT 0.659 (0.664)\tDT 0.248 (0.250)\tloss 0.209 (0.178)\tKL loss 0.225 (0.190)\tAcc@1 95.898 (95.065)\n",
      "Train: [61][70/98]\tBT 0.658 (0.663)\tDT 0.246 (0.250)\tloss 0.112 (0.176)\tKL loss 0.120 (0.187)\tAcc@1 96.094 (95.075)\n",
      "Train: [61][80/98]\tBT 0.664 (0.662)\tDT 0.250 (0.249)\tloss 0.151 (0.172)\tKL loss 0.167 (0.184)\tAcc@1 95.117 (95.144)\n",
      "Train: [61][90/98]\tBT 0.648 (0.663)\tDT 0.235 (0.249)\tloss 0.191 (0.174)\tKL loss 0.207 (0.185)\tAcc@1 93.750 (95.091)\n",
      "Train epoch 61, total time 64.90, accuracy:95.08\n",
      "Test: [0/20]\tTime 0.269 (0.269)\tLoss 1.3581 (1.3581)\tKL loss 1.342 (1.342)\tAcc@1 81.445 (81.445)\n",
      "Test: [10/20]\tTime 0.282 (0.275)\tLoss 1.3232 (1.2648)\tKL loss 1.295 (1.253)\tAcc@1 79.883 (81.534)\n",
      " * Acc@1 81.540\n",
      "Train: [62][10/98]\tBT 0.661 (0.656)\tDT 0.248 (0.242)\tloss 0.160 (0.187)\tKL loss 0.185 (0.201)\tAcc@1 95.898 (94.551)\n",
      "Train: [62][20/98]\tBT 0.831 (0.669)\tDT 0.416 (0.254)\tloss 0.147 (0.179)\tKL loss 0.178 (0.191)\tAcc@1 94.922 (94.863)\n",
      "Train: [62][30/98]\tBT 0.656 (0.667)\tDT 0.243 (0.252)\tloss 0.201 (0.177)\tKL loss 0.206 (0.188)\tAcc@1 94.727 (94.876)\n",
      "Train: [62][40/98]\tBT 0.652 (0.665)\tDT 0.240 (0.251)\tloss 0.187 (0.172)\tKL loss 0.206 (0.183)\tAcc@1 94.336 (95.000)\n",
      "Train: [62][50/98]\tBT 0.651 (0.663)\tDT 0.240 (0.250)\tloss 0.154 (0.172)\tKL loss 0.165 (0.183)\tAcc@1 95.508 (95.008)\n",
      "Train: [62][60/98]\tBT 0.654 (0.663)\tDT 0.241 (0.249)\tloss 0.149 (0.172)\tKL loss 0.159 (0.182)\tAcc@1 95.117 (95.007)\n",
      "Train: [62][70/98]\tBT 0.672 (0.662)\tDT 0.257 (0.249)\tloss 0.208 (0.170)\tKL loss 0.224 (0.180)\tAcc@1 94.141 (95.031)\n",
      "Train: [62][80/98]\tBT 0.658 (0.662)\tDT 0.245 (0.249)\tloss 0.100 (0.167)\tKL loss 0.107 (0.177)\tAcc@1 96.875 (95.078)\n",
      "Train: [62][90/98]\tBT 0.652 (0.662)\tDT 0.238 (0.248)\tloss 0.166 (0.165)\tKL loss 0.171 (0.176)\tAcc@1 94.336 (95.102)\n",
      "Train epoch 62, total time 64.82, accuracy:95.14\n",
      "Test: [0/20]\tTime 0.277 (0.277)\tLoss 1.3049 (1.3049)\tKL loss 1.283 (1.283)\tAcc@1 80.078 (80.078)\n",
      "Test: [10/20]\tTime 0.279 (0.277)\tLoss 1.2380 (1.2212)\tKL loss 1.218 (1.207)\tAcc@1 78.711 (80.877)\n",
      " * Acc@1 81.280\n",
      "Train: [63][10/98]\tBT 0.677 (0.678)\tDT 0.261 (0.264)\tloss 0.127 (0.154)\tKL loss 0.131 (0.173)\tAcc@1 95.508 (94.941)\n",
      "Train: [63][20/98]\tBT 0.653 (0.668)\tDT 0.239 (0.254)\tloss 0.219 (0.155)\tKL loss 0.228 (0.167)\tAcc@1 92.969 (95.010)\n",
      "Train: [63][30/98]\tBT 0.668 (0.665)\tDT 0.254 (0.251)\tloss 0.162 (0.159)\tKL loss 0.158 (0.171)\tAcc@1 95.898 (95.052)\n",
      "Train: [63][40/98]\tBT 0.658 (0.666)\tDT 0.245 (0.252)\tloss 0.161 (0.157)\tKL loss 0.165 (0.169)\tAcc@1 94.531 (95.186)\n",
      "Train: [63][50/98]\tBT 0.659 (0.664)\tDT 0.245 (0.251)\tloss 0.158 (0.158)\tKL loss 0.175 (0.170)\tAcc@1 95.508 (95.160)\n",
      "Train: [63][60/98]\tBT 0.664 (0.666)\tDT 0.249 (0.252)\tloss 0.192 (0.158)\tKL loss 0.206 (0.169)\tAcc@1 94.727 (95.195)\n",
      "Train: [63][70/98]\tBT 0.649 (0.665)\tDT 0.240 (0.251)\tloss 0.137 (0.159)\tKL loss 0.143 (0.170)\tAcc@1 96.680 (95.176)\n",
      "Train: [63][80/98]\tBT 0.653 (0.664)\tDT 0.239 (0.250)\tloss 0.156 (0.162)\tKL loss 0.160 (0.171)\tAcc@1 95.898 (95.120)\n",
      "Train: [63][90/98]\tBT 0.667 (0.664)\tDT 0.254 (0.250)\tloss 0.142 (0.160)\tKL loss 0.149 (0.169)\tAcc@1 95.703 (95.148)\n",
      "Train epoch 63, total time 65.33, accuracy:95.12\n",
      "Test: [0/20]\tTime 0.272 (0.272)\tLoss 1.2406 (1.2406)\tKL loss 1.221 (1.221)\tAcc@1 79.688 (79.688)\n",
      "Test: [10/20]\tTime 0.281 (0.283)\tLoss 1.1839 (1.1799)\tKL loss 1.167 (1.164)\tAcc@1 79.297 (80.788)\n",
      " * Acc@1 80.890\n",
      "Train: [64][10/98]\tBT 0.649 (0.657)\tDT 0.236 (0.243)\tloss 0.110 (0.160)\tKL loss 0.117 (0.174)\tAcc@1 96.289 (95.078)\n",
      "Train: [64][20/98]\tBT 0.664 (0.659)\tDT 0.255 (0.246)\tloss 0.105 (0.156)\tKL loss 0.112 (0.169)\tAcc@1 96.680 (94.990)\n",
      "Train: [64][30/98]\tBT 0.664 (0.659)\tDT 0.250 (0.246)\tloss 0.224 (0.154)\tKL loss 0.221 (0.165)\tAcc@1 94.336 (95.033)\n",
      "Train: [64][40/98]\tBT 0.663 (0.659)\tDT 0.250 (0.246)\tloss 0.136 (0.156)\tKL loss 0.156 (0.166)\tAcc@1 96.094 (94.961)\n",
      "Train: [64][50/98]\tBT 0.664 (0.660)\tDT 0.249 (0.247)\tloss 0.149 (0.156)\tKL loss 0.156 (0.165)\tAcc@1 95.703 (95.027)\n",
      "Train: [64][60/98]\tBT 0.660 (0.660)\tDT 0.243 (0.246)\tloss 0.151 (0.153)\tKL loss 0.165 (0.163)\tAcc@1 94.141 (95.091)\n",
      "Train: [64][70/98]\tBT 0.660 (0.660)\tDT 0.250 (0.247)\tloss 0.193 (0.156)\tKL loss 0.188 (0.166)\tAcc@1 94.141 (95.028)\n",
      "Train: [64][80/98]\tBT 0.768 (0.662)\tDT 0.354 (0.248)\tloss 0.173 (0.155)\tKL loss 0.184 (0.165)\tAcc@1 94.922 (95.090)\n",
      "Train: [64][90/98]\tBT 0.663 (0.662)\tDT 0.250 (0.249)\tloss 0.173 (0.156)\tKL loss 0.168 (0.165)\tAcc@1 94.531 (95.082)\n",
      "Train epoch 64, total time 64.83, accuracy:95.05\n",
      "Test: [0/20]\tTime 0.268 (0.268)\tLoss 1.1789 (1.1789)\tKL loss 1.162 (1.162)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.271 (0.275)\tLoss 1.1675 (1.1210)\tKL loss 1.139 (1.104)\tAcc@1 78.320 (81.445)\n",
      " * Acc@1 81.430\n",
      "Train: [65][10/98]\tBT 0.660 (0.659)\tDT 0.243 (0.245)\tloss 0.155 (0.158)\tKL loss 0.157 (0.160)\tAcc@1 94.922 (95.234)\n",
      "Train: [65][20/98]\tBT 0.660 (0.660)\tDT 0.244 (0.246)\tloss 0.146 (0.150)\tKL loss 0.160 (0.155)\tAcc@1 95.508 (95.254)\n",
      "Train: [65][30/98]\tBT 0.654 (0.661)\tDT 0.242 (0.247)\tloss 0.122 (0.150)\tKL loss 0.136 (0.156)\tAcc@1 95.508 (95.234)\n",
      "Train: [65][40/98]\tBT 0.660 (0.660)\tDT 0.246 (0.246)\tloss 0.177 (0.151)\tKL loss 0.163 (0.155)\tAcc@1 95.508 (95.229)\n",
      "Train: [65][50/98]\tBT 0.649 (0.661)\tDT 0.240 (0.248)\tloss 0.194 (0.152)\tKL loss 0.205 (0.156)\tAcc@1 95.117 (95.199)\n",
      "Train: [65][60/98]\tBT 0.698 (0.661)\tDT 0.284 (0.248)\tloss 0.170 (0.152)\tKL loss 0.182 (0.157)\tAcc@1 94.727 (95.156)\n",
      "Train: [65][70/98]\tBT 0.649 (0.664)\tDT 0.236 (0.250)\tloss 0.150 (0.152)\tKL loss 0.154 (0.157)\tAcc@1 94.336 (95.156)\n",
      "Train: [65][80/98]\tBT 0.648 (0.664)\tDT 0.236 (0.250)\tloss 0.113 (0.153)\tKL loss 0.118 (0.158)\tAcc@1 95.898 (95.117)\n",
      "Train: [65][90/98]\tBT 0.651 (0.663)\tDT 0.243 (0.250)\tloss 0.129 (0.155)\tKL loss 0.147 (0.161)\tAcc@1 95.117 (95.043)\n",
      "Train epoch 65, total time 64.95, accuracy:94.99\n",
      "Test: [0/20]\tTime 0.274 (0.274)\tLoss 1.1618 (1.1618)\tKL loss 1.145 (1.145)\tAcc@1 81.055 (81.055)\n",
      "Test: [10/20]\tTime 0.264 (0.275)\tLoss 1.1316 (1.0972)\tKL loss 1.108 (1.081)\tAcc@1 78.516 (81.232)\n",
      " * Acc@1 81.300\n",
      "Train: [66][10/98]\tBT 0.658 (0.659)\tDT 0.246 (0.247)\tloss 0.126 (0.179)\tKL loss 0.151 (0.187)\tAcc@1 96.680 (94.512)\n",
      "Train: [66][20/98]\tBT 0.663 (0.662)\tDT 0.251 (0.248)\tloss 0.129 (0.161)\tKL loss 0.130 (0.169)\tAcc@1 96.094 (94.941)\n",
      "Train: [66][30/98]\tBT 0.649 (0.661)\tDT 0.236 (0.247)\tloss 0.206 (0.166)\tKL loss 0.206 (0.174)\tAcc@1 94.531 (94.896)\n",
      "Train: [66][40/98]\tBT 0.648 (0.660)\tDT 0.236 (0.246)\tloss 0.119 (0.162)\tKL loss 0.110 (0.169)\tAcc@1 96.484 (94.888)\n",
      "Train: [66][50/98]\tBT 0.666 (0.665)\tDT 0.251 (0.251)\tloss 0.144 (0.161)\tKL loss 0.158 (0.168)\tAcc@1 94.336 (94.902)\n",
      "Train: [66][60/98]\tBT 0.656 (0.665)\tDT 0.242 (0.251)\tloss 0.109 (0.161)\tKL loss 0.110 (0.168)\tAcc@1 95.898 (94.876)\n",
      "Train: [66][70/98]\tBT 0.656 (0.664)\tDT 0.245 (0.250)\tloss 0.136 (0.157)\tKL loss 0.162 (0.165)\tAcc@1 95.508 (94.986)\n",
      "Train: [66][80/98]\tBT 0.667 (0.664)\tDT 0.255 (0.250)\tloss 0.138 (0.157)\tKL loss 0.141 (0.165)\tAcc@1 95.117 (94.961)\n",
      "Train: [66][90/98]\tBT 0.649 (0.663)\tDT 0.239 (0.249)\tloss 0.150 (0.158)\tKL loss 0.159 (0.165)\tAcc@1 94.922 (94.983)\n",
      "Train epoch 66, total time 64.98, accuracy:94.98\n",
      "Test: [0/20]\tTime 0.273 (0.273)\tLoss 1.1550 (1.1550)\tKL loss 1.134 (1.134)\tAcc@1 80.859 (80.859)\n",
      "Test: [10/20]\tTime 0.275 (0.274)\tLoss 1.1284 (1.0997)\tKL loss 1.105 (1.081)\tAcc@1 78.320 (81.090)\n",
      " * Acc@1 81.130\n",
      "Train: [67][10/98]\tBT 0.663 (0.661)\tDT 0.251 (0.248)\tloss 0.177 (0.155)\tKL loss 0.183 (0.160)\tAcc@1 94.531 (95.020)\n",
      "Train: [67][20/98]\tBT 0.656 (0.663)\tDT 0.243 (0.250)\tloss 0.151 (0.154)\tKL loss 0.154 (0.159)\tAcc@1 95.312 (95.146)\n",
      "Train: [67][30/98]\tBT 0.658 (0.663)\tDT 0.246 (0.249)\tloss 0.159 (0.147)\tKL loss 0.172 (0.153)\tAcc@1 95.312 (95.241)\n",
      "Train: [67][40/98]\tBT 0.650 (0.670)\tDT 0.238 (0.256)\tloss 0.205 (0.148)\tKL loss 0.221 (0.155)\tAcc@1 93.555 (95.288)\n",
      "Train: [67][50/98]\tBT 0.657 (0.669)\tDT 0.244 (0.255)\tloss 0.205 (0.151)\tKL loss 0.199 (0.157)\tAcc@1 94.531 (95.145)\n",
      "Train: [67][60/98]\tBT 0.663 (0.668)\tDT 0.249 (0.254)\tloss 0.143 (0.154)\tKL loss 0.148 (0.160)\tAcc@1 95.117 (95.124)\n",
      "Train: [67][70/98]\tBT 0.667 (0.668)\tDT 0.252 (0.254)\tloss 0.154 (0.154)\tKL loss 0.163 (0.160)\tAcc@1 95.117 (95.109)\n",
      "Train: [67][80/98]\tBT 0.664 (0.667)\tDT 0.250 (0.254)\tloss 0.132 (0.153)\tKL loss 0.135 (0.160)\tAcc@1 95.508 (95.122)\n",
      "Train: [67][90/98]\tBT 0.685 (0.667)\tDT 0.273 (0.253)\tloss 0.204 (0.155)\tKL loss 0.191 (0.162)\tAcc@1 94.531 (95.072)\n",
      "Train epoch 67, total time 65.37, accuracy:95.08\n",
      "Test: [0/20]\tTime 0.272 (0.272)\tLoss 1.1083 (1.1083)\tKL loss 1.088 (1.088)\tAcc@1 79.688 (79.688)\n",
      "Test: [10/20]\tTime 0.283 (0.278)\tLoss 1.1207 (1.0643)\tKL loss 1.093 (1.046)\tAcc@1 78.516 (81.268)\n",
      " * Acc@1 81.230\n",
      "Train: [68][10/98]\tBT 0.653 (0.660)\tDT 0.241 (0.248)\tloss 0.148 (0.167)\tKL loss 0.157 (0.178)\tAcc@1 93.750 (94.590)\n",
      "Train: [68][20/98]\tBT 0.676 (0.675)\tDT 0.261 (0.260)\tloss 0.134 (0.155)\tKL loss 0.152 (0.165)\tAcc@1 95.703 (94.883)\n",
      "Train: [68][30/98]\tBT 0.670 (0.670)\tDT 0.254 (0.256)\tloss 0.132 (0.149)\tKL loss 0.147 (0.160)\tAcc@1 96.094 (95.052)\n",
      "Train: [68][40/98]\tBT 0.655 (0.668)\tDT 0.245 (0.254)\tloss 0.165 (0.147)\tKL loss 0.167 (0.157)\tAcc@1 93.555 (95.098)\n",
      "Train: [68][50/98]\tBT 0.661 (0.667)\tDT 0.244 (0.253)\tloss 0.131 (0.145)\tKL loss 0.137 (0.155)\tAcc@1 95.508 (95.129)\n",
      "Train: [68][60/98]\tBT 0.665 (0.666)\tDT 0.252 (0.253)\tloss 0.143 (0.146)\tKL loss 0.145 (0.155)\tAcc@1 94.531 (95.111)\n",
      "Train: [68][70/98]\tBT 0.648 (0.665)\tDT 0.235 (0.251)\tloss 0.118 (0.147)\tKL loss 0.122 (0.155)\tAcc@1 94.531 (95.114)\n",
      "Train: [68][80/98]\tBT 0.655 (0.665)\tDT 0.241 (0.251)\tloss 0.198 (0.149)\tKL loss 0.211 (0.157)\tAcc@1 93.555 (95.088)\n",
      "Train: [68][90/98]\tBT 0.664 (0.664)\tDT 0.250 (0.251)\tloss 0.129 (0.149)\tKL loss 0.134 (0.156)\tAcc@1 95.312 (95.100)\n",
      "Train epoch 68, total time 65.09, accuracy:95.09\n",
      "Test: [0/20]\tTime 0.279 (0.279)\tLoss 1.0964 (1.0964)\tKL loss 1.075 (1.075)\tAcc@1 79.883 (79.883)\n",
      "Test: [10/20]\tTime 0.279 (0.276)\tLoss 1.1186 (1.0574)\tKL loss 1.093 (1.036)\tAcc@1 78.320 (81.232)\n",
      " * Acc@1 81.370\n",
      "Train: [69][10/98]\tBT 0.670 (0.663)\tDT 0.259 (0.250)\tloss 0.100 (0.153)\tKL loss 0.119 (0.165)\tAcc@1 96.484 (94.707)\n",
      "Train: [69][20/98]\tBT 0.659 (0.664)\tDT 0.243 (0.251)\tloss 0.152 (0.152)\tKL loss 0.153 (0.161)\tAcc@1 94.336 (94.922)\n",
      "Train: [69][30/98]\tBT 0.661 (0.664)\tDT 0.246 (0.251)\tloss 0.176 (0.157)\tKL loss 0.185 (0.166)\tAcc@1 95.898 (94.896)\n",
      "Train: [69][40/98]\tBT 0.663 (0.665)\tDT 0.250 (0.252)\tloss 0.151 (0.158)\tKL loss 0.153 (0.166)\tAcc@1 95.703 (94.932)\n",
      "Train: [69][50/98]\tBT 0.658 (0.664)\tDT 0.246 (0.251)\tloss 0.131 (0.164)\tKL loss 0.126 (0.170)\tAcc@1 95.508 (94.855)\n",
      "Train: [69][60/98]\tBT 0.658 (0.663)\tDT 0.246 (0.250)\tloss 0.141 (0.163)\tKL loss 0.144 (0.170)\tAcc@1 94.922 (94.847)\n",
      "Train: [69][70/98]\tBT 0.661 (0.664)\tDT 0.250 (0.251)\tloss 0.133 (0.163)\tKL loss 0.133 (0.169)\tAcc@1 96.484 (94.849)\n",
      "Train: [69][80/98]\tBT 0.671 (0.664)\tDT 0.256 (0.250)\tloss 0.161 (0.162)\tKL loss 0.159 (0.168)\tAcc@1 94.141 (94.834)\n",
      "Train: [69][90/98]\tBT 0.659 (0.666)\tDT 0.244 (0.253)\tloss 0.149 (0.161)\tKL loss 0.155 (0.167)\tAcc@1 95.117 (94.874)\n",
      "Train epoch 69, total time 65.30, accuracy:94.91\n",
      "Test: [0/20]\tTime 0.272 (0.272)\tLoss 1.0712 (1.0712)\tKL loss 1.045 (1.045)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.279 (0.275)\tLoss 1.0518 (1.0405)\tKL loss 1.030 (1.020)\tAcc@1 78.125 (81.392)\n",
      " * Acc@1 81.370\n",
      "Train: [70][10/98]\tBT 0.672 (0.662)\tDT 0.258 (0.249)\tloss 0.132 (0.157)\tKL loss 0.150 (0.166)\tAcc@1 94.727 (94.902)\n",
      "Train: [70][20/98]\tBT 0.653 (0.663)\tDT 0.241 (0.250)\tloss 0.127 (0.159)\tKL loss 0.138 (0.167)\tAcc@1 95.312 (94.854)\n",
      "Train: [70][30/98]\tBT 0.658 (0.663)\tDT 0.242 (0.250)\tloss 0.243 (0.161)\tKL loss 0.231 (0.168)\tAcc@1 93.750 (94.896)\n",
      "Train: [70][40/98]\tBT 0.657 (0.664)\tDT 0.245 (0.251)\tloss 0.178 (0.157)\tKL loss 0.180 (0.165)\tAcc@1 94.531 (95.029)\n",
      "Train: [70][50/98]\tBT 0.645 (0.664)\tDT 0.235 (0.251)\tloss 0.108 (0.154)\tKL loss 0.110 (0.161)\tAcc@1 96.289 (95.145)\n",
      "Train: [70][60/98]\tBT 0.665 (0.664)\tDT 0.252 (0.251)\tloss 0.176 (0.154)\tKL loss 0.188 (0.161)\tAcc@1 93.750 (95.153)\n",
      "Train: [70][70/98]\tBT 0.670 (0.663)\tDT 0.257 (0.250)\tloss 0.115 (0.152)\tKL loss 0.116 (0.160)\tAcc@1 95.898 (95.193)\n",
      "Train: [70][80/98]\tBT 0.651 (0.666)\tDT 0.237 (0.252)\tloss 0.112 (0.153)\tKL loss 0.127 (0.161)\tAcc@1 95.312 (95.161)\n",
      "Train: [70][90/98]\tBT 0.659 (0.665)\tDT 0.247 (0.252)\tloss 0.116 (0.152)\tKL loss 0.150 (0.160)\tAcc@1 96.094 (95.156)\n",
      "Train epoch 70, total time 65.22, accuracy:95.15\n",
      "Test: [0/20]\tTime 0.274 (0.274)\tLoss 1.0317 (1.0317)\tKL loss 1.007 (1.007)\tAcc@1 79.883 (79.883)\n",
      "Test: [10/20]\tTime 0.291 (0.278)\tLoss 1.0309 (1.0076)\tKL loss 1.005 (0.989)\tAcc@1 79.102 (81.339)\n",
      " * Acc@1 81.550\n",
      "Train: [71][10/98]\tBT 0.663 (0.661)\tDT 0.246 (0.248)\tloss 0.134 (0.141)\tKL loss 0.137 (0.146)\tAcc@1 95.312 (95.566)\n",
      "Train: [71][20/98]\tBT 0.654 (0.661)\tDT 0.240 (0.248)\tloss 0.094 (0.144)\tKL loss 0.101 (0.152)\tAcc@1 96.094 (95.322)\n",
      "Train: [71][30/98]\tBT 0.670 (0.662)\tDT 0.256 (0.249)\tloss 0.126 (0.150)\tKL loss 0.129 (0.157)\tAcc@1 95.508 (95.117)\n",
      "Train: [71][40/98]\tBT 0.656 (0.663)\tDT 0.242 (0.250)\tloss 0.124 (0.147)\tKL loss 0.130 (0.154)\tAcc@1 96.289 (95.215)\n",
      "Train: [71][50/98]\tBT 0.664 (0.663)\tDT 0.252 (0.250)\tloss 0.146 (0.148)\tKL loss 0.150 (0.154)\tAcc@1 94.727 (95.125)\n",
      "Train: [71][60/98]\tBT 0.661 (0.667)\tDT 0.249 (0.253)\tloss 0.154 (0.149)\tKL loss 0.158 (0.155)\tAcc@1 94.922 (95.130)\n",
      "Train: [71][70/98]\tBT 0.664 (0.667)\tDT 0.251 (0.253)\tloss 0.136 (0.149)\tKL loss 0.143 (0.155)\tAcc@1 95.703 (95.126)\n",
      "Train: [71][80/98]\tBT 0.666 (0.667)\tDT 0.250 (0.253)\tloss 0.133 (0.150)\tKL loss 0.140 (0.157)\tAcc@1 95.117 (95.088)\n",
      "Train: [71][90/98]\tBT 0.657 (0.666)\tDT 0.241 (0.253)\tloss 0.187 (0.152)\tKL loss 0.196 (0.158)\tAcc@1 95.508 (95.054)\n",
      "Train epoch 71, total time 65.27, accuracy:95.07\n",
      "Test: [0/20]\tTime 0.278 (0.278)\tLoss 1.0381 (1.0381)\tKL loss 1.017 (1.017)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.268 (0.277)\tLoss 1.0935 (1.0144)\tKL loss 1.065 (0.994)\tAcc@1 78.711 (81.428)\n",
      " * Acc@1 81.510\n",
      "Train: [72][10/98]\tBT 0.669 (0.666)\tDT 0.253 (0.252)\tloss 0.150 (0.157)\tKL loss 0.151 (0.163)\tAcc@1 94.141 (94.648)\n",
      "Train: [72][20/98]\tBT 0.671 (0.668)\tDT 0.257 (0.253)\tloss 0.109 (0.146)\tKL loss 0.114 (0.152)\tAcc@1 95.312 (95.010)\n",
      "Train: [72][30/98]\tBT 0.665 (0.667)\tDT 0.254 (0.254)\tloss 0.185 (0.147)\tKL loss 0.195 (0.154)\tAcc@1 94.922 (95.137)\n",
      "Train: [72][40/98]\tBT 0.662 (0.667)\tDT 0.247 (0.253)\tloss 0.165 (0.146)\tKL loss 0.170 (0.152)\tAcc@1 94.727 (95.156)\n",
      "Train: [72][50/98]\tBT 0.655 (0.670)\tDT 0.245 (0.257)\tloss 0.141 (0.151)\tKL loss 0.145 (0.157)\tAcc@1 94.727 (95.051)\n",
      "Train: [72][60/98]\tBT 0.661 (0.669)\tDT 0.247 (0.255)\tloss 0.129 (0.151)\tKL loss 0.134 (0.157)\tAcc@1 96.094 (95.065)\n",
      "Train: [72][70/98]\tBT 0.662 (0.669)\tDT 0.245 (0.255)\tloss 0.141 (0.146)\tKL loss 0.144 (0.153)\tAcc@1 94.336 (95.176)\n",
      "Train: [72][80/98]\tBT 0.666 (0.668)\tDT 0.250 (0.254)\tloss 0.144 (0.147)\tKL loss 0.164 (0.154)\tAcc@1 95.312 (95.190)\n",
      "Train: [72][90/98]\tBT 0.660 (0.667)\tDT 0.246 (0.253)\tloss 0.124 (0.147)\tKL loss 0.129 (0.154)\tAcc@1 95.898 (95.169)\n",
      "Train epoch 72, total time 65.37, accuracy:95.11\n",
      "Test: [0/20]\tTime 0.268 (0.268)\tLoss 1.0310 (1.0310)\tKL loss 1.010 (1.010)\tAcc@1 80.078 (80.078)\n",
      "Test: [10/20]\tTime 0.275 (0.275)\tLoss 1.0427 (0.9825)\tKL loss 1.017 (0.963)\tAcc@1 78.711 (81.587)\n",
      " * Acc@1 81.720\n",
      "Train: [73][10/98]\tBT 0.654 (0.664)\tDT 0.241 (0.251)\tloss 0.142 (0.160)\tKL loss 0.153 (0.168)\tAcc@1 95.312 (95.039)\n",
      "Train: [73][20/98]\tBT 0.651 (0.661)\tDT 0.241 (0.249)\tloss 0.131 (0.152)\tKL loss 0.128 (0.159)\tAcc@1 97.266 (95.273)\n",
      "Train: [73][30/98]\tBT 0.676 (0.670)\tDT 0.264 (0.257)\tloss 0.095 (0.153)\tKL loss 0.104 (0.160)\tAcc@1 97.266 (95.182)\n",
      "Train: [73][40/98]\tBT 0.656 (0.668)\tDT 0.247 (0.255)\tloss 0.183 (0.155)\tKL loss 0.182 (0.162)\tAcc@1 93.555 (95.093)\n",
      "Train: [73][50/98]\tBT 0.660 (0.668)\tDT 0.246 (0.255)\tloss 0.151 (0.151)\tKL loss 0.153 (0.158)\tAcc@1 95.117 (95.160)\n",
      "Train: [73][60/98]\tBT 0.671 (0.668)\tDT 0.259 (0.254)\tloss 0.148 (0.150)\tKL loss 0.154 (0.157)\tAcc@1 95.898 (95.169)\n",
      "Train: [73][70/98]\tBT 0.667 (0.667)\tDT 0.252 (0.254)\tloss 0.173 (0.150)\tKL loss 0.187 (0.157)\tAcc@1 93.555 (95.151)\n",
      "Train: [73][80/98]\tBT 0.668 (0.666)\tDT 0.253 (0.253)\tloss 0.173 (0.151)\tKL loss 0.188 (0.158)\tAcc@1 94.141 (95.105)\n",
      "Train: [73][90/98]\tBT 0.654 (0.666)\tDT 0.242 (0.253)\tloss 0.141 (0.152)\tKL loss 0.155 (0.159)\tAcc@1 95.703 (95.087)\n",
      "Train epoch 73, total time 65.36, accuracy:95.11\n",
      "Test: [0/20]\tTime 0.285 (0.285)\tLoss 1.0392 (1.0392)\tKL loss 1.019 (1.019)\tAcc@1 80.859 (80.859)\n",
      "Test: [10/20]\tTime 0.269 (0.275)\tLoss 1.0456 (1.0007)\tKL loss 1.019 (0.980)\tAcc@1 77.930 (81.090)\n",
      " * Acc@1 81.290\n",
      "Train: [74][10/98]\tBT 0.654 (0.687)\tDT 0.243 (0.271)\tloss 0.179 (0.154)\tKL loss 0.181 (0.163)\tAcc@1 94.531 (95.000)\n",
      "Train: [74][20/98]\tBT 0.660 (0.681)\tDT 0.245 (0.267)\tloss 0.179 (0.149)\tKL loss 0.183 (0.156)\tAcc@1 95.312 (95.215)\n",
      "Train: [74][30/98]\tBT 0.660 (0.673)\tDT 0.247 (0.259)\tloss 0.094 (0.144)\tKL loss 0.099 (0.150)\tAcc@1 98.633 (95.365)\n",
      "Train: [74][40/98]\tBT 0.648 (0.671)\tDT 0.238 (0.257)\tloss 0.158 (0.148)\tKL loss 0.160 (0.155)\tAcc@1 94.141 (95.190)\n",
      "Train: [74][50/98]\tBT 0.654 (0.669)\tDT 0.239 (0.256)\tloss 0.138 (0.145)\tKL loss 0.140 (0.150)\tAcc@1 95.508 (95.320)\n",
      "Train: [74][60/98]\tBT 0.684 (0.669)\tDT 0.266 (0.255)\tloss 0.186 (0.143)\tKL loss 0.189 (0.149)\tAcc@1 94.141 (95.348)\n",
      "Train: [74][70/98]\tBT 0.663 (0.668)\tDT 0.249 (0.255)\tloss 0.171 (0.145)\tKL loss 0.171 (0.152)\tAcc@1 94.531 (95.259)\n",
      "Train: [74][80/98]\tBT 0.682 (0.668)\tDT 0.268 (0.254)\tloss 0.132 (0.146)\tKL loss 0.141 (0.152)\tAcc@1 96.094 (95.261)\n",
      "Train: [74][90/98]\tBT 0.667 (0.667)\tDT 0.255 (0.254)\tloss 0.170 (0.147)\tKL loss 0.186 (0.154)\tAcc@1 93.945 (95.193)\n",
      "Train epoch 74, total time 65.35, accuracy:95.24\n",
      "Test: [0/20]\tTime 0.488 (0.488)\tLoss 1.0438 (1.0438)\tKL loss 1.021 (1.021)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.278 (0.294)\tLoss 1.0773 (1.0096)\tKL loss 1.051 (0.989)\tAcc@1 79.688 (81.232)\n",
      " * Acc@1 81.400\n",
      "Train: [75][10/98]\tBT 0.657 (0.665)\tDT 0.242 (0.251)\tloss 0.108 (0.153)\tKL loss 0.115 (0.164)\tAcc@1 95.508 (94.727)\n",
      "Train: [75][20/98]\tBT 0.660 (0.664)\tDT 0.247 (0.250)\tloss 0.131 (0.155)\tKL loss 0.134 (0.163)\tAcc@1 95.312 (94.678)\n",
      "Train: [75][30/98]\tBT 0.656 (0.663)\tDT 0.242 (0.249)\tloss 0.240 (0.159)\tKL loss 0.245 (0.167)\tAcc@1 92.383 (94.661)\n",
      "Train: [75][40/98]\tBT 0.659 (0.663)\tDT 0.247 (0.249)\tloss 0.152 (0.155)\tKL loss 0.147 (0.162)\tAcc@1 95.508 (94.834)\n",
      "Train: [75][50/98]\tBT 0.659 (0.663)\tDT 0.249 (0.249)\tloss 0.180 (0.153)\tKL loss 0.181 (0.159)\tAcc@1 94.531 (94.934)\n",
      "Train: [75][60/98]\tBT 0.659 (0.662)\tDT 0.245 (0.249)\tloss 0.228 (0.153)\tKL loss 0.246 (0.159)\tAcc@1 92.773 (94.899)\n",
      "Train: [75][70/98]\tBT 0.690 (0.663)\tDT 0.272 (0.249)\tloss 0.094 (0.154)\tKL loss 0.096 (0.160)\tAcc@1 97.461 (94.897)\n",
      "Train: [75][80/98]\tBT 0.660 (0.663)\tDT 0.245 (0.249)\tloss 0.151 (0.153)\tKL loss 0.156 (0.160)\tAcc@1 94.531 (94.910)\n",
      "Train: [75][90/98]\tBT 0.665 (0.665)\tDT 0.251 (0.251)\tloss 0.150 (0.151)\tKL loss 0.186 (0.158)\tAcc@1 94.336 (94.972)\n",
      "Train epoch 75, total time 65.10, accuracy:95.00\n",
      "Test: [0/20]\tTime 0.276 (0.276)\tLoss 1.0474 (1.0474)\tKL loss 1.026 (1.026)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.277 (0.277)\tLoss 1.0209 (0.9997)\tKL loss 0.998 (0.979)\tAcc@1 79.102 (81.339)\n",
      " * Acc@1 81.400\n",
      "Train: [76][10/98]\tBT 0.658 (0.660)\tDT 0.244 (0.248)\tloss 0.218 (0.164)\tKL loss 0.213 (0.170)\tAcc@1 93.164 (94.805)\n",
      "Train: [76][20/98]\tBT 0.658 (0.662)\tDT 0.245 (0.249)\tloss 0.166 (0.150)\tKL loss 0.168 (0.156)\tAcc@1 94.336 (95.039)\n",
      "Train: [76][30/98]\tBT 0.662 (0.661)\tDT 0.249 (0.248)\tloss 0.138 (0.146)\tKL loss 0.163 (0.154)\tAcc@1 95.898 (95.221)\n",
      "Train: [76][40/98]\tBT 0.655 (0.660)\tDT 0.243 (0.247)\tloss 0.125 (0.147)\tKL loss 0.133 (0.154)\tAcc@1 96.094 (95.190)\n",
      "Train: [76][50/98]\tBT 0.655 (0.660)\tDT 0.245 (0.248)\tloss 0.166 (0.148)\tKL loss 0.167 (0.155)\tAcc@1 94.336 (95.164)\n",
      "Train: [76][60/98]\tBT 0.656 (0.661)\tDT 0.245 (0.248)\tloss 0.129 (0.147)\tKL loss 0.131 (0.155)\tAcc@1 95.703 (95.192)\n",
      "Train: [76][70/98]\tBT 0.661 (0.664)\tDT 0.249 (0.251)\tloss 0.225 (0.148)\tKL loss 0.220 (0.155)\tAcc@1 91.992 (95.142)\n",
      "Train: [76][80/98]\tBT 0.661 (0.664)\tDT 0.248 (0.251)\tloss 0.139 (0.147)\tKL loss 0.140 (0.155)\tAcc@1 94.922 (95.212)\n",
      "Train: [76][90/98]\tBT 0.660 (0.664)\tDT 0.250 (0.251)\tloss 0.155 (0.146)\tKL loss 0.158 (0.153)\tAcc@1 94.922 (95.256)\n",
      "Train epoch 76, total time 65.04, accuracy:95.22\n",
      "Test: [0/20]\tTime 0.280 (0.280)\tLoss 1.0050 (1.0050)\tKL loss 0.985 (0.985)\tAcc@1 80.664 (80.664)\n",
      "Test: [10/20]\tTime 0.278 (0.278)\tLoss 1.0007 (0.9721)\tKL loss 0.975 (0.952)\tAcc@1 78.906 (81.694)\n",
      " * Acc@1 81.850\n",
      "Train: [77][10/98]\tBT 0.661 (0.664)\tDT 0.248 (0.250)\tloss 0.148 (0.142)\tKL loss 0.165 (0.155)\tAcc@1 95.117 (95.039)\n",
      "Train: [77][20/98]\tBT 0.667 (0.662)\tDT 0.251 (0.248)\tloss 0.123 (0.139)\tKL loss 0.120 (0.149)\tAcc@1 96.484 (95.322)\n",
      "Train: [77][30/98]\tBT 0.667 (0.661)\tDT 0.256 (0.248)\tloss 0.130 (0.139)\tKL loss 0.134 (0.148)\tAcc@1 94.922 (95.352)\n",
      "Train: [77][40/98]\tBT 0.678 (0.661)\tDT 0.260 (0.248)\tloss 0.144 (0.138)\tKL loss 0.148 (0.147)\tAcc@1 94.922 (95.376)\n",
      "Train: [77][50/98]\tBT 0.661 (0.661)\tDT 0.244 (0.248)\tloss 0.122 (0.141)\tKL loss 0.134 (0.149)\tAcc@1 95.117 (95.258)\n",
      "Train: [77][60/98]\tBT 0.661 (0.666)\tDT 0.244 (0.253)\tloss 0.110 (0.144)\tKL loss 0.112 (0.152)\tAcc@1 96.289 (95.186)\n",
      "Train: [77][70/98]\tBT 0.658 (0.665)\tDT 0.244 (0.252)\tloss 0.217 (0.146)\tKL loss 0.226 (0.153)\tAcc@1 93.750 (95.134)\n",
      "Train: [77][80/98]\tBT 0.662 (0.665)\tDT 0.250 (0.252)\tloss 0.126 (0.146)\tKL loss 0.129 (0.154)\tAcc@1 95.898 (95.154)\n",
      "Train: [77][90/98]\tBT 0.655 (0.665)\tDT 0.242 (0.252)\tloss 0.173 (0.146)\tKL loss 0.149 (0.153)\tAcc@1 96.289 (95.180)\n",
      "Train epoch 77, total time 65.22, accuracy:95.20\n",
      "Test: [0/20]\tTime 0.276 (0.276)\tLoss 0.9938 (0.9938)\tKL loss 0.973 (0.973)\tAcc@1 82.031 (82.031)\n",
      "Test: [10/20]\tTime 0.281 (0.284)\tLoss 1.0104 (0.9731)\tKL loss 0.984 (0.952)\tAcc@1 79.492 (81.907)\n",
      " * Acc@1 81.830\n",
      "Train: [78][10/98]\tBT 0.667 (0.662)\tDT 0.254 (0.249)\tloss 0.178 (0.134)\tKL loss 0.174 (0.140)\tAcc@1 94.727 (95.449)\n",
      "Train: [78][20/98]\tBT 0.661 (0.661)\tDT 0.246 (0.249)\tloss 0.159 (0.141)\tKL loss 0.161 (0.145)\tAcc@1 95.508 (95.264)\n",
      "Train: [78][30/98]\tBT 0.661 (0.662)\tDT 0.247 (0.250)\tloss 0.157 (0.141)\tKL loss 0.165 (0.146)\tAcc@1 95.508 (95.313)\n",
      "Train: [78][40/98]\tBT 0.652 (0.668)\tDT 0.241 (0.256)\tloss 0.149 (0.143)\tKL loss 0.166 (0.148)\tAcc@1 94.727 (95.259)\n",
      "Train: [78][50/98]\tBT 0.661 (0.667)\tDT 0.250 (0.254)\tloss 0.150 (0.145)\tKL loss 0.142 (0.150)\tAcc@1 95.508 (95.234)\n",
      "Train: [78][60/98]\tBT 0.654 (0.666)\tDT 0.240 (0.253)\tloss 0.153 (0.146)\tKL loss 0.156 (0.151)\tAcc@1 94.336 (95.186)\n",
      "Train: [78][70/98]\tBT 0.687 (0.665)\tDT 0.271 (0.252)\tloss 0.152 (0.146)\tKL loss 0.144 (0.151)\tAcc@1 96.094 (95.206)\n",
      "Train: [78][80/98]\tBT 0.659 (0.665)\tDT 0.246 (0.252)\tloss 0.146 (0.145)\tKL loss 0.151 (0.151)\tAcc@1 94.531 (95.188)\n",
      "Train: [78][90/98]\tBT 0.657 (0.665)\tDT 0.242 (0.252)\tloss 0.129 (0.145)\tKL loss 0.132 (0.151)\tAcc@1 95.312 (95.224)\n",
      "Train epoch 78, total time 65.18, accuracy:95.20\n",
      "Test: [0/20]\tTime 0.272 (0.272)\tLoss 1.0004 (1.0004)\tKL loss 0.977 (0.977)\tAcc@1 81.445 (81.445)\n",
      "Test: [10/20]\tTime 0.269 (0.273)\tLoss 1.0125 (0.9694)\tKL loss 0.986 (0.947)\tAcc@1 78.906 (81.658)\n",
      " * Acc@1 81.810\n",
      "Train: [79][10/98]\tBT 0.681 (0.662)\tDT 0.268 (0.250)\tloss 0.152 (0.147)\tKL loss 0.154 (0.155)\tAcc@1 94.336 (94.980)\n",
      "Train: [79][20/98]\tBT 0.826 (0.671)\tDT 0.413 (0.258)\tloss 0.151 (0.142)\tKL loss 0.160 (0.151)\tAcc@1 95.898 (95.215)\n",
      "Train: [79][30/98]\tBT 0.661 (0.668)\tDT 0.247 (0.255)\tloss 0.157 (0.145)\tKL loss 0.160 (0.152)\tAcc@1 95.312 (95.267)\n",
      "Train: [79][40/98]\tBT 0.658 (0.666)\tDT 0.247 (0.253)\tloss 0.135 (0.150)\tKL loss 0.142 (0.157)\tAcc@1 95.312 (95.078)\n",
      "Train: [79][50/98]\tBT 0.649 (0.664)\tDT 0.238 (0.251)\tloss 0.120 (0.148)\tKL loss 0.137 (0.155)\tAcc@1 95.898 (95.152)\n",
      "Train: [79][60/98]\tBT 0.668 (0.664)\tDT 0.255 (0.251)\tloss 0.172 (0.147)\tKL loss 0.185 (0.154)\tAcc@1 93.945 (95.137)\n",
      "Train: [79][70/98]\tBT 0.667 (0.663)\tDT 0.254 (0.250)\tloss 0.174 (0.149)\tKL loss 0.180 (0.157)\tAcc@1 94.336 (95.092)\n",
      "Train: [79][80/98]\tBT 0.657 (0.663)\tDT 0.243 (0.250)\tloss 0.122 (0.151)\tKL loss 0.129 (0.158)\tAcc@1 95.312 (95.081)\n",
      "Train: [79][90/98]\tBT 0.659 (0.663)\tDT 0.247 (0.250)\tloss 0.194 (0.150)\tKL loss 0.185 (0.157)\tAcc@1 94.531 (95.117)\n",
      "Train epoch 79, total time 64.93, accuracy:95.14\n",
      "Test: [0/20]\tTime 0.272 (0.272)\tLoss 0.9950 (0.9950)\tKL loss 0.969 (0.969)\tAcc@1 80.469 (80.469)\n",
      "Test: [10/20]\tTime 0.278 (0.280)\tLoss 0.9971 (0.9618)\tKL loss 0.971 (0.939)\tAcc@1 79.492 (81.694)\n",
      " * Acc@1 81.820\n",
      "Train: [80][10/98]\tBT 0.652 (0.681)\tDT 0.239 (0.265)\tloss 0.147 (0.140)\tKL loss 0.150 (0.143)\tAcc@1 94.727 (95.371)\n",
      "Train: [80][20/98]\tBT 0.660 (0.670)\tDT 0.246 (0.256)\tloss 0.128 (0.145)\tKL loss 0.137 (0.148)\tAcc@1 95.312 (95.361)\n",
      "Train: [80][30/98]\tBT 0.659 (0.667)\tDT 0.246 (0.253)\tloss 0.106 (0.142)\tKL loss 0.109 (0.146)\tAcc@1 96.680 (95.365)\n",
      "Train: [80][40/98]\tBT 0.653 (0.666)\tDT 0.240 (0.252)\tloss 0.123 (0.144)\tKL loss 0.124 (0.148)\tAcc@1 96.094 (95.269)\n",
      "Train: [80][50/98]\tBT 0.665 (0.665)\tDT 0.252 (0.251)\tloss 0.150 (0.144)\tKL loss 0.156 (0.148)\tAcc@1 95.898 (95.250)\n",
      "Train: [80][60/98]\tBT 0.654 (0.665)\tDT 0.242 (0.252)\tloss 0.165 (0.144)\tKL loss 0.168 (0.148)\tAcc@1 94.922 (95.254)\n",
      "Train: [80][70/98]\tBT 0.656 (0.665)\tDT 0.242 (0.251)\tloss 0.144 (0.146)\tKL loss 0.156 (0.150)\tAcc@1 94.531 (95.190)\n",
      "Train: [80][80/98]\tBT 0.659 (0.664)\tDT 0.246 (0.251)\tloss 0.160 (0.145)\tKL loss 0.162 (0.150)\tAcc@1 95.117 (95.195)\n",
      "Train: [80][90/98]\tBT 0.650 (0.664)\tDT 0.234 (0.251)\tloss 0.156 (0.146)\tKL loss 0.160 (0.151)\tAcc@1 94.922 (95.193)\n",
      "Train epoch 80, total time 65.28, accuracy:95.18\n",
      "Test: [0/20]\tTime 0.280 (0.280)\tLoss 0.9926 (0.9926)\tKL loss 0.970 (0.970)\tAcc@1 80.859 (80.859)\n",
      "Test: [10/20]\tTime 0.273 (0.275)\tLoss 0.9892 (0.9511)\tKL loss 0.962 (0.930)\tAcc@1 79.492 (81.765)\n",
      " * Acc@1 81.860\n",
      "Train: [81][10/98]\tBT 0.664 (0.661)\tDT 0.247 (0.248)\tloss 0.139 (0.141)\tKL loss 0.141 (0.151)\tAcc@1 94.727 (95.293)\n",
      "Train: [81][20/98]\tBT 0.651 (0.662)\tDT 0.237 (0.248)\tloss 0.169 (0.141)\tKL loss 0.180 (0.148)\tAcc@1 94.531 (95.352)\n",
      "Train: [81][30/98]\tBT 0.663 (0.663)\tDT 0.251 (0.250)\tloss 0.112 (0.139)\tKL loss 0.122 (0.147)\tAcc@1 95.898 (95.319)\n",
      "Train: [81][40/98]\tBT 0.666 (0.662)\tDT 0.247 (0.249)\tloss 0.096 (0.140)\tKL loss 0.101 (0.147)\tAcc@1 97.070 (95.337)\n",
      "Train: [81][50/98]\tBT 0.672 (0.662)\tDT 0.258 (0.249)\tloss 0.124 (0.141)\tKL loss 0.126 (0.148)\tAcc@1 95.703 (95.289)\n",
      "Train: [81][60/98]\tBT 0.659 (0.662)\tDT 0.246 (0.249)\tloss 0.158 (0.142)\tKL loss 0.169 (0.149)\tAcc@1 94.922 (95.277)\n",
      "Train: [81][70/98]\tBT 0.669 (0.662)\tDT 0.253 (0.249)\tloss 0.127 (0.142)\tKL loss 0.137 (0.149)\tAcc@1 95.898 (95.285)\n",
      "Train: [81][80/98]\tBT 0.654 (0.667)\tDT 0.243 (0.253)\tloss 0.107 (0.142)\tKL loss 0.113 (0.149)\tAcc@1 96.094 (95.266)\n",
      "Train: [81][90/98]\tBT 0.661 (0.666)\tDT 0.248 (0.253)\tloss 0.122 (0.142)\tKL loss 0.127 (0.148)\tAcc@1 95.508 (95.260)\n",
      "Train epoch 81, total time 65.38, accuracy:95.25\n",
      "Test: [0/20]\tTime 0.275 (0.275)\tLoss 0.9826 (0.9826)\tKL loss 0.961 (0.961)\tAcc@1 81.250 (81.250)\n",
      "Test: [10/20]\tTime 0.276 (0.274)\tLoss 0.9809 (0.9480)\tKL loss 0.956 (0.927)\tAcc@1 79.492 (81.854)\n",
      " * Acc@1 81.860\n",
      "Train: [82][10/98]\tBT 0.664 (0.659)\tDT 0.248 (0.245)\tloss 0.146 (0.145)\tKL loss 0.141 (0.148)\tAcc@1 95.117 (95.391)\n",
      "Train: [82][20/98]\tBT 0.659 (0.663)\tDT 0.249 (0.250)\tloss 0.172 (0.156)\tKL loss 0.169 (0.159)\tAcc@1 95.508 (95.137)\n",
      "Train: [82][30/98]\tBT 0.657 (0.662)\tDT 0.244 (0.249)\tloss 0.124 (0.146)\tKL loss 0.138 (0.151)\tAcc@1 96.289 (95.339)\n",
      "Train: [82][40/98]\tBT 0.665 (0.662)\tDT 0.252 (0.249)\tloss 0.140 (0.145)\tKL loss 0.145 (0.151)\tAcc@1 95.898 (95.361)\n",
      "Train: [82][50/98]\tBT 0.662 (0.661)\tDT 0.247 (0.248)\tloss 0.105 (0.143)\tKL loss 0.111 (0.149)\tAcc@1 97.266 (95.391)\n",
      "Train: [82][60/98]\tBT 0.659 (0.662)\tDT 0.244 (0.248)\tloss 0.183 (0.142)\tKL loss 0.187 (0.148)\tAcc@1 93.555 (95.413)\n",
      "Train: [82][70/98]\tBT 0.670 (0.665)\tDT 0.256 (0.252)\tloss 0.136 (0.142)\tKL loss 0.146 (0.148)\tAcc@1 95.703 (95.382)\n",
      "Train: [82][80/98]\tBT 0.653 (0.665)\tDT 0.243 (0.251)\tloss 0.202 (0.142)\tKL loss 0.213 (0.149)\tAcc@1 94.141 (95.352)\n",
      "Train: [82][90/98]\tBT 0.661 (0.664)\tDT 0.247 (0.251)\tloss 0.153 (0.141)\tKL loss 0.162 (0.148)\tAcc@1 94.531 (95.360)\n",
      "Train epoch 82, total time 65.11, accuracy:95.34\n",
      "Test: [0/20]\tTime 0.282 (0.282)\tLoss 0.9855 (0.9855)\tKL loss 0.962 (0.962)\tAcc@1 81.445 (81.445)\n",
      "Test: [10/20]\tTime 0.294 (0.280)\tLoss 1.0075 (0.9529)\tKL loss 0.981 (0.931)\tAcc@1 79.102 (81.889)\n",
      " * Acc@1 81.930\n",
      "Train: [83][10/98]\tBT 0.673 (0.669)\tDT 0.257 (0.255)\tloss 0.110 (0.134)\tKL loss 0.113 (0.137)\tAcc@1 95.898 (95.566)\n",
      "Train: [83][20/98]\tBT 0.651 (0.665)\tDT 0.238 (0.251)\tloss 0.178 (0.142)\tKL loss 0.190 (0.145)\tAcc@1 94.141 (95.254)\n",
      "Train: [83][30/98]\tBT 0.654 (0.664)\tDT 0.242 (0.251)\tloss 0.186 (0.138)\tKL loss 0.183 (0.141)\tAcc@1 93.359 (95.378)\n",
      "Train: [83][40/98]\tBT 0.669 (0.664)\tDT 0.256 (0.251)\tloss 0.121 (0.138)\tKL loss 0.142 (0.142)\tAcc@1 95.898 (95.430)\n",
      "Train: [83][50/98]\tBT 0.660 (0.669)\tDT 0.246 (0.255)\tloss 0.144 (0.138)\tKL loss 0.150 (0.142)\tAcc@1 95.117 (95.406)\n",
      "Train: [83][60/98]\tBT 0.655 (0.667)\tDT 0.243 (0.253)\tloss 0.138 (0.138)\tKL loss 0.126 (0.142)\tAcc@1 96.094 (95.436)\n",
      "Train: [83][70/98]\tBT 0.659 (0.667)\tDT 0.249 (0.253)\tloss 0.188 (0.139)\tKL loss 0.193 (0.144)\tAcc@1 93.945 (95.455)\n",
      "Train: [83][80/98]\tBT 0.664 (0.667)\tDT 0.250 (0.253)\tloss 0.093 (0.139)\tKL loss 0.105 (0.144)\tAcc@1 96.875 (95.442)\n",
      "Train: [83][90/98]\tBT 0.665 (0.667)\tDT 0.251 (0.253)\tloss 0.166 (0.140)\tKL loss 0.174 (0.145)\tAcc@1 94.141 (95.408)\n",
      "Train epoch 83, total time 65.33, accuracy:95.37\n",
      "Test: [0/20]\tTime 0.276 (0.276)\tLoss 0.9900 (0.9900)\tKL loss 0.966 (0.966)\tAcc@1 81.250 (81.250)\n",
      "Test: [10/20]\tTime 0.283 (0.278)\tLoss 0.9899 (0.9529)\tKL loss 0.965 (0.932)\tAcc@1 79.102 (81.818)\n",
      " * Acc@1 81.830\n",
      "Train: [84][10/98]\tBT 0.676 (0.660)\tDT 0.264 (0.248)\tloss 0.112 (0.144)\tKL loss 0.123 (0.149)\tAcc@1 95.703 (94.883)\n",
      "Train: [84][20/98]\tBT 0.673 (0.661)\tDT 0.259 (0.249)\tloss 0.190 (0.146)\tKL loss 0.191 (0.151)\tAcc@1 93.750 (95.059)\n",
      "Train: [84][30/98]\tBT 0.654 (0.659)\tDT 0.243 (0.248)\tloss 0.157 (0.146)\tKL loss 0.156 (0.151)\tAcc@1 94.727 (95.052)\n",
      "Train: [84][40/98]\tBT 0.656 (0.667)\tDT 0.248 (0.256)\tloss 0.146 (0.147)\tKL loss 0.153 (0.152)\tAcc@1 94.922 (95.059)\n",
      "Train: [84][50/98]\tBT 0.654 (0.666)\tDT 0.245 (0.255)\tloss 0.126 (0.146)\tKL loss 0.161 (0.152)\tAcc@1 96.094 (95.066)\n",
      "Train: [84][60/98]\tBT 0.645 (0.664)\tDT 0.238 (0.253)\tloss 0.078 (0.146)\tKL loss 0.087 (0.151)\tAcc@1 97.656 (95.104)\n",
      "Train: [84][70/98]\tBT 0.655 (0.663)\tDT 0.245 (0.253)\tloss 0.103 (0.146)\tKL loss 0.103 (0.151)\tAcc@1 97.461 (95.179)\n",
      "Train: [84][80/98]\tBT 0.653 (0.664)\tDT 0.241 (0.253)\tloss 0.118 (0.149)\tKL loss 0.130 (0.153)\tAcc@1 95.703 (95.151)\n",
      "Train: [84][90/98]\tBT 0.663 (0.663)\tDT 0.252 (0.253)\tloss 0.173 (0.149)\tKL loss 0.170 (0.153)\tAcc@1 95.117 (95.176)\n",
      "Train epoch 84, total time 64.94, accuracy:95.17\n",
      "Test: [0/20]\tTime 0.267 (0.267)\tLoss 0.9679 (0.9679)\tKL loss 0.947 (0.947)\tAcc@1 80.078 (80.078)\n",
      "Test: [10/20]\tTime 0.276 (0.274)\tLoss 0.9654 (0.9475)\tKL loss 0.943 (0.926)\tAcc@1 78.906 (81.392)\n",
      " * Acc@1 81.600\n",
      "Train: [85][10/98]\tBT 0.648 (0.657)\tDT 0.239 (0.248)\tloss 0.142 (0.147)\tKL loss 0.147 (0.154)\tAcc@1 94.922 (95.156)\n",
      "Train: [85][20/98]\tBT 0.658 (0.667)\tDT 0.250 (0.258)\tloss 0.089 (0.139)\tKL loss 0.091 (0.147)\tAcc@1 96.484 (95.273)\n",
      "Train: [85][30/98]\tBT 0.649 (0.662)\tDT 0.243 (0.253)\tloss 0.102 (0.142)\tKL loss 0.116 (0.151)\tAcc@1 95.898 (95.332)\n",
      "Train: [85][40/98]\tBT 0.664 (0.660)\tDT 0.256 (0.252)\tloss 0.188 (0.144)\tKL loss 0.189 (0.151)\tAcc@1 94.141 (95.234)\n",
      "Train: [85][50/98]\tBT 0.645 (0.659)\tDT 0.240 (0.251)\tloss 0.147 (0.142)\tKL loss 0.169 (0.150)\tAcc@1 95.508 (95.352)\n",
      "Train: [85][60/98]\tBT 0.652 (0.658)\tDT 0.245 (0.250)\tloss 0.184 (0.146)\tKL loss 0.207 (0.153)\tAcc@1 93.555 (95.205)\n",
      "Train: [85][70/98]\tBT 0.643 (0.658)\tDT 0.239 (0.251)\tloss 0.129 (0.147)\tKL loss 0.141 (0.154)\tAcc@1 95.898 (95.181)\n",
      "Train: [85][80/98]\tBT 0.656 (0.658)\tDT 0.249 (0.250)\tloss 0.171 (0.147)\tKL loss 0.162 (0.154)\tAcc@1 95.508 (95.193)\n",
      "Train: [85][90/98]\tBT 0.668 (0.658)\tDT 0.260 (0.250)\tloss 0.104 (0.146)\tKL loss 0.107 (0.153)\tAcc@1 97.266 (95.232)\n",
      "Train epoch 85, total time 64.48, accuracy:95.23\n",
      "Test: [0/20]\tTime 0.271 (0.271)\tLoss 0.9755 (0.9755)\tKL loss 0.952 (0.952)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.264 (0.275)\tLoss 0.9589 (0.9409)\tKL loss 0.936 (0.919)\tAcc@1 79.297 (81.641)\n",
      " * Acc@1 81.740\n",
      "Train: [86][10/98]\tBT 0.663 (0.667)\tDT 0.249 (0.254)\tloss 0.125 (0.136)\tKL loss 0.129 (0.144)\tAcc@1 95.703 (95.703)\n",
      "Train: [86][20/98]\tBT 0.656 (0.664)\tDT 0.242 (0.252)\tloss 0.145 (0.136)\tKL loss 0.147 (0.142)\tAcc@1 95.508 (95.605)\n",
      "Train: [86][30/98]\tBT 0.658 (0.664)\tDT 0.247 (0.252)\tloss 0.093 (0.137)\tKL loss 0.105 (0.141)\tAcc@1 96.875 (95.566)\n",
      "Train: [86][40/98]\tBT 0.669 (0.665)\tDT 0.256 (0.252)\tloss 0.141 (0.136)\tKL loss 0.146 (0.141)\tAcc@1 96.289 (95.591)\n",
      "Train: [86][50/98]\tBT 0.668 (0.665)\tDT 0.255 (0.253)\tloss 0.193 (0.140)\tKL loss 0.198 (0.145)\tAcc@1 93.359 (95.477)\n",
      "Train: [86][60/98]\tBT 0.652 (0.665)\tDT 0.238 (0.252)\tloss 0.115 (0.140)\tKL loss 0.125 (0.146)\tAcc@1 96.094 (95.436)\n",
      "Train: [86][70/98]\tBT 0.663 (0.664)\tDT 0.252 (0.252)\tloss 0.169 (0.139)\tKL loss 0.160 (0.145)\tAcc@1 94.922 (95.474)\n",
      "Train: [86][80/98]\tBT 0.662 (0.664)\tDT 0.249 (0.251)\tloss 0.178 (0.139)\tKL loss 0.181 (0.145)\tAcc@1 92.969 (95.449)\n",
      "Train: [86][90/98]\tBT 0.798 (0.666)\tDT 0.386 (0.254)\tloss 0.129 (0.138)\tKL loss 0.142 (0.143)\tAcc@1 95.898 (95.473)\n",
      "Train epoch 86, total time 65.26, accuracy:95.39\n",
      "Test: [0/20]\tTime 0.273 (0.273)\tLoss 0.9698 (0.9698)\tKL loss 0.947 (0.947)\tAcc@1 80.469 (80.469)\n",
      "Test: [10/20]\tTime 0.273 (0.277)\tLoss 0.9686 (0.9379)\tKL loss 0.943 (0.916)\tAcc@1 79.492 (81.694)\n",
      " * Acc@1 81.750\n",
      "Train: [87][10/98]\tBT 0.658 (0.661)\tDT 0.246 (0.248)\tloss 0.145 (0.126)\tKL loss 0.156 (0.133)\tAcc@1 95.898 (95.566)\n",
      "Train: [87][20/98]\tBT 0.662 (0.662)\tDT 0.247 (0.249)\tloss 0.151 (0.134)\tKL loss 0.161 (0.141)\tAcc@1 94.922 (95.449)\n",
      "Train: [87][30/98]\tBT 0.668 (0.662)\tDT 0.252 (0.250)\tloss 0.216 (0.140)\tKL loss 0.215 (0.146)\tAcc@1 94.531 (95.267)\n",
      "Train: [87][40/98]\tBT 0.652 (0.663)\tDT 0.240 (0.250)\tloss 0.119 (0.136)\tKL loss 0.120 (0.145)\tAcc@1 95.898 (95.425)\n",
      "Train: [87][50/98]\tBT 0.650 (0.662)\tDT 0.236 (0.249)\tloss 0.194 (0.138)\tKL loss 0.209 (0.147)\tAcc@1 94.141 (95.379)\n",
      "Train: [87][60/98]\tBT 0.666 (0.662)\tDT 0.250 (0.249)\tloss 0.110 (0.139)\tKL loss 0.122 (0.147)\tAcc@1 96.094 (95.368)\n",
      "Train: [87][70/98]\tBT 0.652 (0.663)\tDT 0.239 (0.249)\tloss 0.117 (0.139)\tKL loss 0.121 (0.146)\tAcc@1 96.094 (95.393)\n",
      "Train: [87][80/98]\tBT 0.676 (0.665)\tDT 0.262 (0.252)\tloss 0.110 (0.138)\tKL loss 0.114 (0.145)\tAcc@1 96.289 (95.391)\n",
      "Train: [87][90/98]\tBT 0.652 (0.665)\tDT 0.239 (0.251)\tloss 0.180 (0.137)\tKL loss 0.188 (0.145)\tAcc@1 92.578 (95.384)\n",
      "Train epoch 87, total time 65.11, accuracy:95.31\n",
      "Test: [0/20]\tTime 0.270 (0.270)\tLoss 0.9731 (0.9731)\tKL loss 0.950 (0.950)\tAcc@1 80.469 (80.469)\n",
      "Test: [10/20]\tTime 0.281 (0.275)\tLoss 0.9678 (0.9393)\tKL loss 0.944 (0.917)\tAcc@1 78.711 (81.516)\n",
      " * Acc@1 81.710\n",
      "Train: [88][10/98]\tBT 0.655 (0.658)\tDT 0.243 (0.246)\tloss 0.125 (0.143)\tKL loss 0.127 (0.148)\tAcc@1 94.531 (95.566)\n",
      "Train: [88][20/98]\tBT 0.670 (0.662)\tDT 0.255 (0.249)\tloss 0.128 (0.143)\tKL loss 0.150 (0.150)\tAcc@1 95.703 (95.430)\n",
      "Train: [88][30/98]\tBT 0.656 (0.662)\tDT 0.246 (0.249)\tloss 0.137 (0.145)\tKL loss 0.139 (0.151)\tAcc@1 94.727 (95.371)\n",
      "Train: [88][40/98]\tBT 0.649 (0.662)\tDT 0.239 (0.249)\tloss 0.146 (0.144)\tKL loss 0.150 (0.151)\tAcc@1 94.727 (95.283)\n",
      "Train: [88][50/98]\tBT 0.654 (0.662)\tDT 0.240 (0.249)\tloss 0.154 (0.147)\tKL loss 0.161 (0.153)\tAcc@1 95.703 (95.238)\n",
      "Train: [88][60/98]\tBT 0.662 (0.666)\tDT 0.250 (0.252)\tloss 0.110 (0.143)\tKL loss 0.121 (0.149)\tAcc@1 96.680 (95.326)\n",
      "Train: [88][70/98]\tBT 0.670 (0.666)\tDT 0.258 (0.252)\tloss 0.107 (0.141)\tKL loss 0.110 (0.148)\tAcc@1 96.875 (95.357)\n",
      "Train: [88][80/98]\tBT 0.668 (0.666)\tDT 0.256 (0.252)\tloss 0.132 (0.142)\tKL loss 0.136 (0.148)\tAcc@1 95.117 (95.320)\n",
      "Train: [88][90/98]\tBT 0.670 (0.666)\tDT 0.256 (0.252)\tloss 0.123 (0.142)\tKL loss 0.134 (0.149)\tAcc@1 95.117 (95.273)\n",
      "Train epoch 88, total time 65.23, accuracy:95.28\n",
      "Test: [0/20]\tTime 0.278 (0.278)\tLoss 0.9633 (0.9633)\tKL loss 0.941 (0.941)\tAcc@1 80.469 (80.469)\n",
      "Test: [10/20]\tTime 0.267 (0.273)\tLoss 0.9675 (0.9362)\tKL loss 0.946 (0.914)\tAcc@1 79.492 (81.694)\n",
      " * Acc@1 81.700\n",
      "Train: [89][10/98]\tBT 0.662 (0.662)\tDT 0.251 (0.250)\tloss 0.140 (0.131)\tKL loss 0.147 (0.136)\tAcc@1 96.094 (95.430)\n",
      "Train: [89][20/98]\tBT 0.665 (0.660)\tDT 0.252 (0.248)\tloss 0.160 (0.142)\tKL loss 0.178 (0.146)\tAcc@1 94.727 (95.264)\n",
      "Train: [89][30/98]\tBT 0.642 (0.658)\tDT 0.230 (0.246)\tloss 0.167 (0.139)\tKL loss 0.172 (0.145)\tAcc@1 94.336 (95.352)\n",
      "Train: [89][40/98]\tBT 0.693 (0.661)\tDT 0.277 (0.248)\tloss 0.080 (0.140)\tKL loss 0.096 (0.146)\tAcc@1 97.070 (95.337)\n",
      "Train: [89][50/98]\tBT 0.662 (0.664)\tDT 0.250 (0.252)\tloss 0.160 (0.142)\tKL loss 0.161 (0.148)\tAcc@1 93.164 (95.254)\n",
      "Train: [89][60/98]\tBT 0.664 (0.664)\tDT 0.247 (0.251)\tloss 0.126 (0.140)\tKL loss 0.138 (0.145)\tAcc@1 95.508 (95.299)\n",
      "Train: [89][70/98]\tBT 0.662 (0.664)\tDT 0.246 (0.251)\tloss 0.113 (0.139)\tKL loss 0.119 (0.144)\tAcc@1 96.094 (95.324)\n",
      "Train: [89][80/98]\tBT 0.669 (0.665)\tDT 0.254 (0.252)\tloss 0.166 (0.141)\tKL loss 0.172 (0.146)\tAcc@1 92.188 (95.234)\n",
      "Train: [89][90/98]\tBT 0.659 (0.664)\tDT 0.245 (0.251)\tloss 0.127 (0.141)\tKL loss 0.128 (0.147)\tAcc@1 95.898 (95.208)\n",
      "Train epoch 89, total time 65.13, accuracy:95.20\n",
      "Test: [0/20]\tTime 0.281 (0.281)\tLoss 0.9678 (0.9678)\tKL loss 0.944 (0.944)\tAcc@1 80.664 (80.664)\n",
      "Test: [10/20]\tTime 0.283 (0.288)\tLoss 0.9841 (0.9379)\tKL loss 0.961 (0.916)\tAcc@1 79.297 (81.729)\n",
      " * Acc@1 81.940\n",
      "Train: [90][10/98]\tBT 0.654 (0.658)\tDT 0.243 (0.247)\tloss 0.176 (0.146)\tKL loss 0.188 (0.155)\tAcc@1 94.727 (95.020)\n",
      "Train: [90][20/98]\tBT 0.663 (0.660)\tDT 0.249 (0.247)\tloss 0.133 (0.139)\tKL loss 0.151 (0.145)\tAcc@1 95.312 (95.322)\n",
      "Train: [90][30/98]\tBT 0.671 (0.667)\tDT 0.259 (0.254)\tloss 0.122 (0.142)\tKL loss 0.127 (0.148)\tAcc@1 95.312 (95.280)\n",
      "Train: [90][40/98]\tBT 0.655 (0.666)\tDT 0.242 (0.254)\tloss 0.154 (0.142)\tKL loss 0.163 (0.148)\tAcc@1 95.117 (95.347)\n",
      "Train: [90][50/98]\tBT 0.659 (0.666)\tDT 0.245 (0.253)\tloss 0.160 (0.142)\tKL loss 0.159 (0.149)\tAcc@1 95.117 (95.301)\n",
      "Train: [90][60/98]\tBT 0.655 (0.666)\tDT 0.244 (0.252)\tloss 0.112 (0.141)\tKL loss 0.112 (0.147)\tAcc@1 95.703 (95.326)\n",
      "Train: [90][70/98]\tBT 0.666 (0.665)\tDT 0.253 (0.252)\tloss 0.186 (0.140)\tKL loss 0.195 (0.147)\tAcc@1 94.141 (95.326)\n",
      "Train: [90][80/98]\tBT 0.649 (0.665)\tDT 0.236 (0.251)\tloss 0.157 (0.143)\tKL loss 0.173 (0.150)\tAcc@1 95.312 (95.315)\n",
      "Train: [90][90/98]\tBT 0.654 (0.664)\tDT 0.242 (0.251)\tloss 0.111 (0.143)\tKL loss 0.116 (0.149)\tAcc@1 96.484 (95.265)\n",
      "Train epoch 90, total time 65.00, accuracy:95.22\n",
      "Test: [0/20]\tTime 0.282 (0.282)\tLoss 0.9671 (0.9671)\tKL loss 0.943 (0.943)\tAcc@1 81.250 (81.250)\n",
      "Test: [10/20]\tTime 0.278 (0.277)\tLoss 0.9835 (0.9392)\tKL loss 0.959 (0.917)\tAcc@1 78.906 (81.587)\n",
      " * Acc@1 81.690\n",
      "Train: [91][10/98]\tBT 0.650 (0.656)\tDT 0.240 (0.245)\tloss 0.135 (0.134)\tKL loss 0.152 (0.139)\tAcc@1 96.094 (95.859)\n",
      "Train: [91][20/98]\tBT 0.662 (0.670)\tDT 0.249 (0.257)\tloss 0.150 (0.134)\tKL loss 0.152 (0.141)\tAcc@1 95.703 (95.566)\n",
      "Train: [91][30/98]\tBT 0.666 (0.669)\tDT 0.250 (0.255)\tloss 0.125 (0.136)\tKL loss 0.127 (0.143)\tAcc@1 96.094 (95.514)\n",
      "Train: [91][40/98]\tBT 0.664 (0.667)\tDT 0.251 (0.254)\tloss 0.154 (0.138)\tKL loss 0.155 (0.144)\tAcc@1 94.336 (95.474)\n",
      "Train: [91][50/98]\tBT 0.661 (0.665)\tDT 0.247 (0.252)\tloss 0.146 (0.138)\tKL loss 0.151 (0.144)\tAcc@1 95.312 (95.484)\n",
      "Train: [91][60/98]\tBT 0.658 (0.665)\tDT 0.242 (0.251)\tloss 0.106 (0.139)\tKL loss 0.123 (0.145)\tAcc@1 96.094 (95.410)\n",
      "Train: [91][70/98]\tBT 0.656 (0.665)\tDT 0.245 (0.252)\tloss 0.196 (0.142)\tKL loss 0.198 (0.148)\tAcc@1 93.555 (95.287)\n",
      "Train: [91][80/98]\tBT 0.660 (0.664)\tDT 0.246 (0.251)\tloss 0.136 (0.141)\tKL loss 0.142 (0.147)\tAcc@1 95.117 (95.298)\n",
      "Train: [91][90/98]\tBT 0.658 (0.665)\tDT 0.245 (0.251)\tloss 0.111 (0.141)\tKL loss 0.115 (0.147)\tAcc@1 96.094 (95.282)\n",
      "Train epoch 91, total time 65.20, accuracy:95.22\n",
      "Test: [0/20]\tTime 0.275 (0.275)\tLoss 0.9580 (0.9580)\tKL loss 0.934 (0.934)\tAcc@1 80.664 (80.664)\n",
      "Test: [10/20]\tTime 0.281 (0.302)\tLoss 0.9759 (0.9360)\tKL loss 0.952 (0.914)\tAcc@1 78.516 (81.516)\n",
      " * Acc@1 81.730\n",
      "Train: [92][10/98]\tBT 0.659 (0.663)\tDT 0.244 (0.250)\tloss 0.109 (0.127)\tKL loss 0.119 (0.136)\tAcc@1 96.680 (95.742)\n",
      "Train: [92][20/98]\tBT 0.664 (0.664)\tDT 0.252 (0.250)\tloss 0.191 (0.136)\tKL loss 0.193 (0.143)\tAcc@1 93.359 (95.498)\n",
      "Train: [92][30/98]\tBT 0.650 (0.664)\tDT 0.234 (0.250)\tloss 0.142 (0.137)\tKL loss 0.151 (0.143)\tAcc@1 95.117 (95.501)\n",
      "Train: [92][40/98]\tBT 0.657 (0.663)\tDT 0.246 (0.250)\tloss 0.133 (0.138)\tKL loss 0.151 (0.144)\tAcc@1 95.898 (95.444)\n",
      "Train: [92][50/98]\tBT 0.664 (0.663)\tDT 0.252 (0.249)\tloss 0.178 (0.140)\tKL loss 0.183 (0.146)\tAcc@1 94.727 (95.430)\n",
      "Train: [92][60/98]\tBT 0.664 (0.663)\tDT 0.248 (0.250)\tloss 0.136 (0.139)\tKL loss 0.139 (0.144)\tAcc@1 95.312 (95.469)\n",
      "Train: [92][70/98]\tBT 0.668 (0.662)\tDT 0.254 (0.249)\tloss 0.166 (0.141)\tKL loss 0.170 (0.147)\tAcc@1 95.898 (95.424)\n",
      "Train: [92][80/98]\tBT 0.655 (0.662)\tDT 0.243 (0.249)\tloss 0.173 (0.142)\tKL loss 0.175 (0.147)\tAcc@1 95.117 (95.400)\n",
      "Train: [92][90/98]\tBT 0.652 (0.665)\tDT 0.238 (0.251)\tloss 0.175 (0.142)\tKL loss 0.177 (0.147)\tAcc@1 95.312 (95.388)\n",
      "Train epoch 92, total time 65.10, accuracy:95.39\n",
      "Test: [0/20]\tTime 0.276 (0.276)\tLoss 0.9549 (0.9549)\tKL loss 0.931 (0.931)\tAcc@1 80.469 (80.469)\n",
      "Test: [10/20]\tTime 0.278 (0.275)\tLoss 0.9697 (0.9347)\tKL loss 0.946 (0.912)\tAcc@1 78.711 (81.499)\n",
      " * Acc@1 81.670\n",
      "Train: [93][10/98]\tBT 0.658 (0.664)\tDT 0.243 (0.251)\tloss 0.182 (0.151)\tKL loss 0.187 (0.155)\tAcc@1 93.945 (95.117)\n",
      "Train: [93][20/98]\tBT 0.665 (0.661)\tDT 0.251 (0.248)\tloss 0.156 (0.154)\tKL loss 0.171 (0.157)\tAcc@1 94.922 (95.156)\n",
      "Train: [93][30/98]\tBT 0.656 (0.660)\tDT 0.240 (0.247)\tloss 0.107 (0.152)\tKL loss 0.109 (0.154)\tAcc@1 96.094 (95.202)\n",
      "Train: [93][40/98]\tBT 0.664 (0.660)\tDT 0.252 (0.247)\tloss 0.127 (0.151)\tKL loss 0.143 (0.154)\tAcc@1 95.703 (95.142)\n",
      "Train: [93][50/98]\tBT 0.648 (0.662)\tDT 0.238 (0.249)\tloss 0.158 (0.157)\tKL loss 0.165 (0.160)\tAcc@1 94.141 (95.035)\n",
      "Train: [93][60/98]\tBT 0.655 (0.661)\tDT 0.243 (0.248)\tloss 0.210 (0.155)\tKL loss 0.203 (0.158)\tAcc@1 93.945 (95.049)\n",
      "Train: [93][70/98]\tBT 0.657 (0.665)\tDT 0.244 (0.251)\tloss 0.104 (0.150)\tKL loss 0.107 (0.153)\tAcc@1 96.680 (95.181)\n",
      "Train: [93][80/98]\tBT 0.663 (0.664)\tDT 0.253 (0.251)\tloss 0.137 (0.148)\tKL loss 0.156 (0.152)\tAcc@1 94.922 (95.205)\n",
      "Train: [93][90/98]\tBT 0.668 (0.664)\tDT 0.257 (0.251)\tloss 0.195 (0.148)\tKL loss 0.191 (0.152)\tAcc@1 93.555 (95.219)\n",
      "Train epoch 93, total time 65.08, accuracy:95.24\n",
      "Test: [0/20]\tTime 0.271 (0.271)\tLoss 0.9596 (0.9596)\tKL loss 0.935 (0.935)\tAcc@1 80.469 (80.469)\n",
      "Test: [10/20]\tTime 0.272 (0.276)\tLoss 0.9709 (0.9339)\tKL loss 0.947 (0.912)\tAcc@1 78.711 (81.534)\n",
      " * Acc@1 81.690\n",
      "Train: [94][10/98]\tBT 0.664 (0.661)\tDT 0.249 (0.248)\tloss 0.154 (0.136)\tKL loss 0.156 (0.142)\tAcc@1 95.508 (95.566)\n",
      "Train: [94][20/98]\tBT 0.665 (0.661)\tDT 0.250 (0.248)\tloss 0.151 (0.137)\tKL loss 0.160 (0.143)\tAcc@1 95.508 (95.635)\n",
      "Train: [94][30/98]\tBT 0.664 (0.663)\tDT 0.254 (0.250)\tloss 0.146 (0.136)\tKL loss 0.146 (0.143)\tAcc@1 95.117 (95.671)\n",
      "Train: [94][40/98]\tBT 0.661 (0.665)\tDT 0.247 (0.251)\tloss 0.126 (0.137)\tKL loss 0.130 (0.143)\tAcc@1 95.898 (95.605)\n",
      "Train: [94][50/98]\tBT 0.663 (0.666)\tDT 0.247 (0.252)\tloss 0.149 (0.136)\tKL loss 0.156 (0.142)\tAcc@1 94.531 (95.633)\n",
      "Train: [94][60/98]\tBT 0.669 (0.669)\tDT 0.256 (0.255)\tloss 0.091 (0.136)\tKL loss 0.093 (0.143)\tAcc@1 96.875 (95.566)\n",
      "Train: [94][70/98]\tBT 0.656 (0.668)\tDT 0.244 (0.255)\tloss 0.218 (0.140)\tKL loss 0.184 (0.146)\tAcc@1 94.922 (95.460)\n",
      "Train: [94][80/98]\tBT 0.659 (0.668)\tDT 0.245 (0.254)\tloss 0.083 (0.141)\tKL loss 0.086 (0.147)\tAcc@1 97.070 (95.437)\n",
      "Train: [94][90/98]\tBT 0.654 (0.667)\tDT 0.242 (0.254)\tloss 0.149 (0.141)\tKL loss 0.155 (0.147)\tAcc@1 95.312 (95.401)\n",
      "Train epoch 94, total time 65.32, accuracy:95.43\n",
      "Test: [0/20]\tTime 0.281 (0.281)\tLoss 0.9587 (0.9587)\tKL loss 0.935 (0.935)\tAcc@1 80.664 (80.664)\n",
      "Test: [10/20]\tTime 0.275 (0.274)\tLoss 0.9787 (0.9337)\tKL loss 0.954 (0.912)\tAcc@1 78.711 (81.623)\n",
      " * Acc@1 81.750\n",
      "Train: [95][10/98]\tBT 0.659 (0.661)\tDT 0.246 (0.249)\tloss 0.157 (0.147)\tKL loss 0.149 (0.151)\tAcc@1 95.703 (95.449)\n",
      "Train: [95][20/98]\tBT 0.658 (0.664)\tDT 0.240 (0.251)\tloss 0.145 (0.144)\tKL loss 0.157 (0.151)\tAcc@1 94.727 (95.264)\n",
      "Train: [95][30/98]\tBT 0.667 (0.664)\tDT 0.254 (0.250)\tloss 0.167 (0.141)\tKL loss 0.172 (0.147)\tAcc@1 95.312 (95.306)\n",
      "Train: [95][40/98]\tBT 0.655 (0.668)\tDT 0.241 (0.254)\tloss 0.159 (0.142)\tKL loss 0.158 (0.146)\tAcc@1 94.922 (95.278)\n",
      "Train: [95][50/98]\tBT 0.654 (0.667)\tDT 0.239 (0.253)\tloss 0.152 (0.139)\tKL loss 0.155 (0.144)\tAcc@1 95.117 (95.309)\n",
      "Train: [95][60/98]\tBT 0.659 (0.666)\tDT 0.246 (0.252)\tloss 0.157 (0.142)\tKL loss 0.155 (0.146)\tAcc@1 96.094 (95.303)\n",
      "Train: [95][70/98]\tBT 0.662 (0.666)\tDT 0.246 (0.252)\tloss 0.096 (0.141)\tKL loss 0.101 (0.145)\tAcc@1 97.070 (95.335)\n",
      "Train: [95][80/98]\tBT 0.653 (0.665)\tDT 0.241 (0.251)\tloss 0.129 (0.144)\tKL loss 0.135 (0.148)\tAcc@1 95.508 (95.242)\n",
      "Train: [95][90/98]\tBT 0.659 (0.665)\tDT 0.244 (0.251)\tloss 0.179 (0.145)\tKL loss 0.172 (0.150)\tAcc@1 94.141 (95.206)\n",
      "Train epoch 95, total time 65.20, accuracy:95.21\n",
      "Test: [0/20]\tTime 0.283 (0.283)\tLoss 0.9584 (0.9584)\tKL loss 0.935 (0.935)\tAcc@1 81.055 (81.055)\n",
      "Test: [10/20]\tTime 0.282 (0.277)\tLoss 0.9675 (0.9324)\tKL loss 0.944 (0.910)\tAcc@1 79.297 (81.836)\n",
      " * Acc@1 81.880\n",
      "Train: [96][10/98]\tBT 0.667 (0.660)\tDT 0.250 (0.247)\tloss 0.112 (0.129)\tKL loss 0.112 (0.137)\tAcc@1 95.703 (96.074)\n",
      "Train: [96][20/98]\tBT 0.664 (0.660)\tDT 0.248 (0.247)\tloss 0.159 (0.133)\tKL loss 0.167 (0.141)\tAcc@1 95.508 (95.820)\n",
      "Train: [96][30/98]\tBT 0.656 (0.673)\tDT 0.245 (0.258)\tloss 0.109 (0.134)\tKL loss 0.114 (0.141)\tAcc@1 96.094 (95.827)\n",
      "Train: [96][40/98]\tBT 0.661 (0.671)\tDT 0.248 (0.256)\tloss 0.140 (0.136)\tKL loss 0.139 (0.143)\tAcc@1 95.312 (95.654)\n",
      "Train: [96][50/98]\tBT 0.660 (0.669)\tDT 0.249 (0.255)\tloss 0.124 (0.136)\tKL loss 0.135 (0.144)\tAcc@1 95.117 (95.645)\n",
      "Train: [96][60/98]\tBT 0.664 (0.668)\tDT 0.248 (0.254)\tloss 0.115 (0.135)\tKL loss 0.119 (0.144)\tAcc@1 96.094 (95.615)\n",
      "Train: [96][70/98]\tBT 0.671 (0.667)\tDT 0.259 (0.253)\tloss 0.148 (0.137)\tKL loss 0.156 (0.146)\tAcc@1 95.117 (95.564)\n",
      "Train: [96][80/98]\tBT 0.656 (0.666)\tDT 0.241 (0.252)\tloss 0.135 (0.136)\tKL loss 0.131 (0.144)\tAcc@1 95.117 (95.571)\n",
      "Train: [96][90/98]\tBT 0.673 (0.666)\tDT 0.258 (0.252)\tloss 0.100 (0.136)\tKL loss 0.106 (0.143)\tAcc@1 96.289 (95.601)\n",
      "Train epoch 96, total time 65.29, accuracy:95.57\n",
      "Test: [0/20]\tTime 0.277 (0.277)\tLoss 0.9556 (0.9556)\tKL loss 0.932 (0.932)\tAcc@1 81.250 (81.250)\n",
      "Test: [10/20]\tTime 0.276 (0.277)\tLoss 0.9682 (0.9296)\tKL loss 0.944 (0.908)\tAcc@1 79.492 (81.783)\n",
      " * Acc@1 81.870\n",
      "Train: [97][10/98]\tBT 0.660 (0.685)\tDT 0.248 (0.269)\tloss 0.157 (0.143)\tKL loss 0.160 (0.147)\tAcc@1 94.727 (95.312)\n",
      "Train: [97][20/98]\tBT 0.661 (0.673)\tDT 0.248 (0.258)\tloss 0.116 (0.140)\tKL loss 0.117 (0.146)\tAcc@1 95.898 (95.391)\n",
      "Train: [97][30/98]\tBT 0.650 (0.670)\tDT 0.238 (0.256)\tloss 0.200 (0.146)\tKL loss 0.213 (0.152)\tAcc@1 93.164 (95.156)\n",
      "Train: [97][40/98]\tBT 0.648 (0.667)\tDT 0.234 (0.253)\tloss 0.178 (0.142)\tKL loss 0.169 (0.147)\tAcc@1 96.289 (95.386)\n",
      "Train: [97][50/98]\tBT 0.658 (0.666)\tDT 0.245 (0.252)\tloss 0.162 (0.142)\tKL loss 0.165 (0.146)\tAcc@1 94.922 (95.367)\n",
      "Train: [97][60/98]\tBT 0.667 (0.665)\tDT 0.252 (0.251)\tloss 0.120 (0.142)\tKL loss 0.123 (0.147)\tAcc@1 94.922 (95.391)\n",
      "Train: [97][70/98]\tBT 0.667 (0.665)\tDT 0.254 (0.251)\tloss 0.139 (0.143)\tKL loss 0.144 (0.148)\tAcc@1 95.898 (95.343)\n",
      "Train: [97][80/98]\tBT 0.664 (0.665)\tDT 0.252 (0.252)\tloss 0.121 (0.143)\tKL loss 0.119 (0.148)\tAcc@1 95.312 (95.332)\n",
      "Train: [97][90/98]\tBT 0.663 (0.665)\tDT 0.248 (0.251)\tloss 0.145 (0.142)\tKL loss 0.150 (0.146)\tAcc@1 95.703 (95.369)\n",
      "Train epoch 97, total time 65.46, accuracy:95.35\n",
      "Test: [0/20]\tTime 0.280 (0.280)\tLoss 0.9553 (0.9553)\tKL loss 0.932 (0.932)\tAcc@1 80.859 (80.859)\n",
      "Test: [10/20]\tTime 0.283 (0.279)\tLoss 0.9747 (0.9311)\tKL loss 0.950 (0.909)\tAcc@1 79.102 (81.747)\n",
      " * Acc@1 81.810\n",
      "Train: [98][10/98]\tBT 0.661 (0.659)\tDT 0.247 (0.246)\tloss 0.099 (0.133)\tKL loss 0.118 (0.142)\tAcc@1 95.898 (95.469)\n",
      "Train: [98][20/98]\tBT 0.674 (0.663)\tDT 0.259 (0.250)\tloss 0.140 (0.130)\tKL loss 0.146 (0.139)\tAcc@1 96.094 (95.615)\n",
      "Train: [98][30/98]\tBT 0.647 (0.664)\tDT 0.234 (0.250)\tloss 0.147 (0.133)\tKL loss 0.141 (0.141)\tAcc@1 96.094 (95.553)\n",
      "Train: [98][40/98]\tBT 0.658 (0.663)\tDT 0.247 (0.250)\tloss 0.143 (0.138)\tKL loss 0.144 (0.146)\tAcc@1 95.312 (95.435)\n",
      "Train: [98][50/98]\tBT 0.658 (0.662)\tDT 0.244 (0.249)\tloss 0.118 (0.138)\tKL loss 0.119 (0.146)\tAcc@1 96.875 (95.355)\n",
      "Train: [98][60/98]\tBT 0.669 (0.663)\tDT 0.251 (0.250)\tloss 0.157 (0.137)\tKL loss 0.157 (0.144)\tAcc@1 94.922 (95.384)\n",
      "Train: [98][70/98]\tBT 0.667 (0.664)\tDT 0.252 (0.250)\tloss 0.148 (0.137)\tKL loss 0.156 (0.144)\tAcc@1 95.117 (95.399)\n",
      "Train: [98][80/98]\tBT 0.665 (0.667)\tDT 0.255 (0.253)\tloss 0.147 (0.137)\tKL loss 0.156 (0.144)\tAcc@1 94.336 (95.386)\n",
      "Train: [98][90/98]\tBT 0.658 (0.667)\tDT 0.243 (0.253)\tloss 0.103 (0.137)\tKL loss 0.115 (0.144)\tAcc@1 96.875 (95.380)\n",
      "Train epoch 98, total time 65.31, accuracy:95.36\n",
      "Test: [0/20]\tTime 0.270 (0.270)\tLoss 0.9525 (0.9525)\tKL loss 0.928 (0.928)\tAcc@1 80.469 (80.469)\n",
      "Test: [10/20]\tTime 0.287 (0.288)\tLoss 0.9673 (0.9308)\tKL loss 0.944 (0.909)\tAcc@1 79.102 (81.516)\n",
      " * Acc@1 81.670\n",
      "Train: [99][10/98]\tBT 0.671 (0.658)\tDT 0.258 (0.245)\tloss 0.145 (0.138)\tKL loss 0.148 (0.145)\tAcc@1 95.703 (95.293)\n",
      "Train: [99][20/98]\tBT 0.663 (0.660)\tDT 0.249 (0.247)\tloss 0.126 (0.135)\tKL loss 0.131 (0.143)\tAcc@1 95.898 (95.518)\n",
      "Train: [99][30/98]\tBT 0.645 (0.660)\tDT 0.236 (0.247)\tloss 0.121 (0.135)\tKL loss 0.131 (0.142)\tAcc@1 96.680 (95.430)\n",
      "Train: [99][40/98]\tBT 0.667 (0.661)\tDT 0.249 (0.248)\tloss 0.140 (0.136)\tKL loss 0.159 (0.143)\tAcc@1 94.141 (95.469)\n",
      "Train: [99][50/98]\tBT 0.657 (0.661)\tDT 0.244 (0.248)\tloss 0.150 (0.138)\tKL loss 0.152 (0.145)\tAcc@1 94.922 (95.402)\n",
      "Train: [99][60/98]\tBT 0.656 (0.661)\tDT 0.241 (0.248)\tloss 0.130 (0.140)\tKL loss 0.137 (0.146)\tAcc@1 95.312 (95.348)\n",
      "Train: [99][70/98]\tBT 0.655 (0.665)\tDT 0.240 (0.251)\tloss 0.170 (0.142)\tKL loss 0.181 (0.149)\tAcc@1 94.531 (95.282)\n",
      "Train: [99][80/98]\tBT 0.652 (0.664)\tDT 0.240 (0.250)\tloss 0.142 (0.143)\tKL loss 0.138 (0.149)\tAcc@1 94.531 (95.303)\n",
      "Train: [99][90/98]\tBT 0.653 (0.664)\tDT 0.242 (0.250)\tloss 0.145 (0.141)\tKL loss 0.151 (0.147)\tAcc@1 94.336 (95.349)\n",
      "Train epoch 99, total time 65.05, accuracy:95.33\n",
      "Test: [0/20]\tTime 0.274 (0.274)\tLoss 0.9530 (0.9530)\tKL loss 0.929 (0.929)\tAcc@1 80.273 (80.273)\n",
      "Test: [10/20]\tTime 0.278 (0.284)\tLoss 0.9623 (0.9296)\tKL loss 0.939 (0.907)\tAcc@1 79.297 (81.658)\n",
      " * Acc@1 81.820\n",
      "Train: [100][10/98]\tBT 0.658 (0.668)\tDT 0.245 (0.255)\tloss 0.118 (0.136)\tKL loss 0.139 (0.138)\tAcc@1 96.289 (95.566)\n",
      "Train: [100][20/98]\tBT 0.673 (0.665)\tDT 0.255 (0.252)\tloss 0.112 (0.135)\tKL loss 0.115 (0.139)\tAcc@1 96.094 (95.439)\n",
      "Train: [100][30/98]\tBT 0.662 (0.666)\tDT 0.248 (0.252)\tloss 0.163 (0.136)\tKL loss 0.163 (0.141)\tAcc@1 94.336 (95.449)\n",
      "Train: [100][40/98]\tBT 0.659 (0.665)\tDT 0.246 (0.251)\tloss 0.207 (0.138)\tKL loss 0.217 (0.143)\tAcc@1 93.945 (95.391)\n",
      "Train: [100][50/98]\tBT 0.672 (0.668)\tDT 0.255 (0.254)\tloss 0.193 (0.138)\tKL loss 0.195 (0.143)\tAcc@1 93.945 (95.391)\n",
      "Train: [100][60/98]\tBT 0.656 (0.667)\tDT 0.241 (0.253)\tloss 0.119 (0.139)\tKL loss 0.128 (0.144)\tAcc@1 95.508 (95.358)\n",
      "Train: [100][70/98]\tBT 0.668 (0.666)\tDT 0.254 (0.253)\tloss 0.151 (0.138)\tKL loss 0.160 (0.143)\tAcc@1 93.750 (95.371)\n",
      "Train: [100][80/98]\tBT 0.675 (0.666)\tDT 0.263 (0.253)\tloss 0.172 (0.137)\tKL loss 0.176 (0.142)\tAcc@1 93.555 (95.364)\n",
      "Train: [100][90/98]\tBT 0.660 (0.666)\tDT 0.246 (0.252)\tloss 0.156 (0.138)\tKL loss 0.164 (0.143)\tAcc@1 94.531 (95.378)\n",
      "Train epoch 100, total time 65.25, accuracy:95.36\n",
      "Test: [0/20]\tTime 0.274 (0.274)\tLoss 0.9530 (0.9530)\tKL loss 0.929 (0.929)\tAcc@1 80.469 (80.469)\n",
      "Test: [10/20]\tTime 0.269 (0.276)\tLoss 0.9697 (0.9298)\tKL loss 0.945 (0.907)\tAcc@1 79.297 (81.729)\n",
      " * Acc@1 81.840\n",
      "best accuracy: 81.94\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "opt = parse_option()\n",
    "\n",
    "print('Loading saved model')\n",
    "resnet_orig_model = ResNet18(num_classes=10)\n",
    "resnet_orig_model.load_state_dict(torch.load('cifar resnet.pt'))\n",
    "resnet_orig_model.to(device)\n",
    "original_model = nn.Sequential(resnet_orig_model, nn.Softmax(dim=1))\n",
    "\n",
    "image_imputer = ImageImputer(width=32, height=32, superpixel_size=2)\n",
    "\n",
    "# build data loader\n",
    "train_loader, val_loader = set_loader(opt, original_model,image_imputer.num_players )\n",
    "\n",
    "# build model and criterion\n",
    "mask_layer_model, supcon_resnet_model, classifier, criterion = set_model(opt)\n",
    "\n",
    "# build optimizer\n",
    "optimizer = set_optimizer(opt, classifier)\n",
    "\n",
    "# training routine\n",
    "for epoch in range(1, opt.epochs + 1):\n",
    "    adjust_learning_rate(opt, optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    time1 = time.time()\n",
    "    loss, acc = train(train_loader, mask_layer_model, supcon_resnet_model, original_model, classifier, criterion,\n",
    "                        optimizer, epoch, opt, image_imputer)\n",
    "    time2 = time.time()\n",
    "    print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(\n",
    "        epoch, time2 - time1, acc))\n",
    "\n",
    "    # eval for one epoch\n",
    "    loss, val_acc = validate(val_loader, mask_layer_model, supcon_resnet_model, classifier, criterion, opt, image_imputer)\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "\n",
    "print('best accuracy: {:.2f}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b3f461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskLayer2dSCL()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MaskLayer2dSCL(value=0,append=0,include_second_coalition= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1fc9b8638362cbdc7141d1e74cba8a8c09a8baa0cc5883fc38fa28281f96844d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
